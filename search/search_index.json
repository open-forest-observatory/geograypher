{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Geograypher: Multiview Semantic Reasoning with Geospatial Data","text":"<p>Geograypher is a tool developed by the Open Forest Observatory to help answer ecological questions using images from aerial surveys. Land managers and research ecologists are increasingly using small uncrewed aerial systems (sUAS or \"drones\") to survey large regions with overlapping high-resolution aerial images. This data can be used to generate predictions of some attribute, for example identifying the location of individual trees or deliniating the boundaries of different classes of vegetation. A common workflow is to take the raw images from the drone survey and generate a stiched top-down \"orthomosaic\" using photogrammetry software. Then, this orthomosaic is used as input to a machine learning model that predicts the attribute in question.</p> <p>Geograypher was developed to support an alternative workflow where machine learning predictions are generated on individual images and then these predictions are projected into geospatial coordinates. This is beneficial because there are artifacts and errors in the orthomosaic because it is synthesized from many individual images. Additionally, there are multiple raw images of a given location which can each be used to make independent predictions. While geograypher primarily supports this \"multiview\" workflow, it also supports more conventional orthomosaic workflows which can be used as baseline or alternative approach.</p>"},{"location":"software_design/","title":"Software design","text":"<p>This page is a work in progress. Some main topics I'd like to cover are the following:</p> <ul> <li>What is in scope for geograyper. Mapping content between images, meshes, and geospatial coordinates is the main focus. Geograypher is designed to be used in workflows that use machine learning, but geograypher is explicitly designed to not perform machine learning itself.</li> <li>Software architecture: Geograypher is primarily an object oriented library.</li> <li>Main mathematical/algorithmic concepts. Geograypher leverages concepts from multiple fields such as geometric computer vision, graphics, and geospatial informatics.</li> <li>Roadmap: what features do we want to implement in the future. Primarily around object detection. And supporting more types of data and/or pluggable modules for rendering.</li> </ul>"},{"location":"API_reference/entrypoints/","title":"Entrypoints","text":"<p>Entrypoints are functions can be directly called to perform a high-level task. Each of the entrypoints listed below can be called as a command line script. These scripts accept arguments that are directly passed to these functions.</p>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.render_labels.render_labels","title":"<code>render_labels(mesh_file, cameras_file, image_folder, texture, render_savefolder, mesh_CRS, original_image_folder=None, subset_images_savefolder=None, texture_column_name=None, DTM_file=None, ground_height_threshold=None, render_ground_class=False, textured_mesh_savefile=None, ROI=None, mesh_ROI_buffer_radius_meters=50, cameras_ROI_buffer_radius_meters=150, IDs_to_labels=None, render_image_scale=1, mesh_downsample=1, n_cameras_per_chunk=None, cast_to_uint8=True, save_as_npy=False, vis=False, mesh_vis_file=None, labels_vis_folder=None)</code>","text":"<p>Renders image-based labels using geospatial ground truth data</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the MetaShape-exported .xml cameras file</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to create the mesh</p> required <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>See TexturedPhotogrammetryMesh.load_texture</p> required <code>render_savefolder</code> <code>PATH_TYPE</code> <p>Where to save the rendered labels</p> required <code>mesh_CRS</code> <code>CRS</code> <p>(pyproj.CRS): The vertex coordinates of the input mesh should be interpreteted in this coordinate references system to georeference them. Since meshes are not commonly used for geospatial tasks, there isn't a common standard for encoding this information in the mesh.</p> required <code>original_image_folder</code> <code>Union[PATH_TYPE, None]</code> <p>Where the images were when photogrammetry was run. Metashape saves imagenames with an absolute path which can cause issues. If this argument is provided, this path is removed from the start of each image file name, which allows the camera set to be used with a moved folder of images specified by <code>image_folder</code>. Defaults to None.</p> <code>None</code> <code>subset_images_savefolder</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the subset of images for which labels are generated. Defaults to None.</p> <code>None</code> <code>texture_column_name</code> <code>Union[str, None]</code> <p>Column to use in vector file for texture information\". Defaults to None.</p> <code>None</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a DTM file to use for ground thresholding. Defaults to None.</p> <code>None</code> <code>ground_height_threshold</code> <code>Union[float, None]</code> <p>Set points under this height to ground. Only applicable if DTM_file is provided. Defaults to None.</p> <code>None</code> <code>render_ground_class</code> <code>bool</code> <p>Should the ground class be included in the renders or deleted.. Defaults to False.</p> <code>False</code> <code>textured_mesh_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the textured and subsetted mesh, if needed in the future. Defaults to None.</p> <code>None</code> <code>ROI</code> <code>Union[PATH_TYPE, GeoDataFrame, MultiPolygon, None]</code> <p>The region of interest to render labels for. Defaults to None.</p> <code>None</code> <code>mesh_ROI_buffer_radius_meters</code> <code>float</code> <p>The distance in meters to include around the ROI for the mesh. Defaults to 50.</p> <code>50</code> <code>cameras_ROI_buffer_radius_meters</code> <code>float</code> <p>The distance in meters to include around the ROI for the cameras. Defaults to 150.</p> <code>150</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping between the integer labels and string values for the classes. Defaults to None.</p> <code>None</code> <code>render_image_scale</code> <code>float</code> <p>Downsample the images to this fraction of the size for increased performance but lower quality. Defaults to 1.</p> <code>1</code> <code>mesh_downsample</code> <code>float</code> <p>Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.</p> <code>1</code> <code>n_cameras_per_chunk</code> <code>Union[int, None]</code> <p>If set, break the camera set and mesh into chunks so there are approximately this many cameras per chunk. This is useful for large meshes that are otherwise very slow. Defaults to None.</p> <code>None</code> <code>cast_to_uint8</code> <code>bool</code> <p>If True, cast the rendered labels to uint8. If False, it is cast to uint16 or uint32, if not saved as .npy. Defaults to True.</p> <code>True</code> <code>save_as_npy</code> <code>bool</code> <p>If True, save the rendered labels are float64 and saved as numpy arrays. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/entrypoints/render_labels.py</code> <pre><code>def render_labels(\n    mesh_file: PATH_TYPE,\n    cameras_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    texture: typing.Union[PATH_TYPE, np.ndarray, None],\n    render_savefolder: PATH_TYPE,\n    mesh_CRS: pyproj.CRS,\n    original_image_folder: typing.Union[PATH_TYPE, None] = None,\n    subset_images_savefolder: typing.Union[PATH_TYPE, None] = None,\n    texture_column_name: typing.Union[str, None] = None,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    ground_height_threshold: typing.Union[float, None] = None,\n    render_ground_class: bool = False,\n    textured_mesh_savefile: typing.Union[PATH_TYPE, None] = None,\n    ROI: typing.Union[PATH_TYPE, gpd.GeoDataFrame, shapely.MultiPolygon, None] = None,\n    mesh_ROI_buffer_radius_meters: float = 50,\n    cameras_ROI_buffer_radius_meters: float = 150,\n    IDs_to_labels: typing.Union[dict, None] = None,\n    render_image_scale: float = 1,\n    mesh_downsample: float = 1,\n    n_cameras_per_chunk: typing.Union[int, None] = None,\n    cast_to_uint8: bool = True,\n    save_as_npy: bool = False,\n    vis: bool = False,\n    mesh_vis_file: typing.Union[PATH_TYPE, None] = None,\n    labels_vis_folder: typing.Union[PATH_TYPE, None] = None,\n):\n    \"\"\"Renders image-based labels using geospatial ground truth data\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        cameras_file (PATH_TYPE):\n            Path to the MetaShape-exported .xml cameras file\n        image_folder (PATH_TYPE):\n            Path to the folder of images used to create the mesh\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]):\n            See TexturedPhotogrammetryMesh.load_texture\n        render_savefolder (PATH_TYPE):\n            Where to save the rendered labels\n        mesh_CRS: (pyproj.CRS):\n            The vertex coordinates of the input mesh should be interpreteted in this coordinate\n            references system to georeference them. Since meshes are not commonly used for\n            geospatial tasks, there isn't a common standard for encoding this information in the mesh.\n        original_image_folder (typing.Union[PATH_TYPE, None], optional):\n            Where the images were when photogrammetry was run. Metashape saves imagenames with an\n            absolute path which can cause issues. If this argument is provided, this path is removed\n            from the start of each image file name, which allows the camera set to be used with a\n            moved folder of images specified by `image_folder`. Defaults to None.\n        subset_images_savefolder (typing.Union[PATH_TYPE, None], optional):\n            Where to save the subset of images for which labels are generated. Defaults to None.\n        texture_column_name (typing.Union[str, None], optional):\n            Column to use in vector file for texture information\". Defaults to None.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a DTM file to use for ground thresholding. Defaults to None.\n        ground_height_threshold (typing.Union[float, None], optional):\n            Set points under this height to ground. Only applicable if DTM_file is provided. Defaults to None.\n        render_ground_class (bool, optional):\n            Should the ground class be included in the renders or deleted.. Defaults to False.\n        textured_mesh_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the textured and subsetted mesh, if needed in the future. Defaults to None.\n        ROI (typing.Union[PATH_TYPE, gpd.GeoDataFrame, shapely.MultiPolygon, None], optional):\n            The region of interest to render labels for. Defaults to None.\n        mesh_ROI_buffer_radius_meters (float, optional):\n            The distance in meters to include around the ROI for the mesh. Defaults to 50.\n        cameras_ROI_buffer_radius_meters (float, optional):\n            The distance in meters to include around the ROI for the cameras. Defaults to 150.\n        IDs_to_labels (typing.Union[None, dict], optional):\n            Mapping between the integer labels and string values for the classes. Defaults to None.\n        render_image_scale (float, optional):\n            Downsample the images to this fraction of the size for increased performance but lower quality. Defaults to 1.\n        mesh_downsample (float, optional):\n            Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.\n        n_cameras_per_chunk (typing.Union[int, None]):\n            If set, break the camera set and mesh into chunks so there are approximately this many\n            cameras per chunk. This is useful for large meshes that are otherwise very slow.\n            Defaults to None.\n        cast_to_uint8 (bool, optional):\n            If True, cast the rendered labels to uint8. If False, it is cast to uint16 or uint32, if not saved as .npy.\n            Defaults to True.\n        save_as_npy (bool, optional):\n            If True, save the rendered labels are float64 and saved as numpy arrays. Defaults to False.\n        mesh_vis (typing.Union[PATH_TYPE, None])\n            Path to save the visualized mesh instead of showing it interactively. Only applicable if vis=True. Defaults to None.\n        labels_vis (typing.Union[PATH_TYPE, None])\n            Defaults to None.\n    \"\"\"\n    ## Determine the ROI\n    # If the ROI is unset and the texture is a geodataframe, set the ROI to that\n    if ROI is None and isinstance(texture, gpd.GeoDataFrame):\n        ROI = texture\n    elif ROI is None and isinstance(texture, (str, Path)):\n        try:\n            ROI = gpd.read_file(texture)\n        except fiona.errors.DriverError:\n            pass\n\n    ## Create the camera set\n    # This is done first because it's often faster than mesh operations which\n    # makes it a good place to check for failures\n    camera_set = MetashapeCameraSet(\n        cameras_file, image_folder, original_image_folder=original_image_folder\n    )\n\n    if ROI is not None:\n        # Extract cameras near the training data\n        camera_set = camera_set.get_subset_ROI(\n            ROI=ROI, buffer_radius=cameras_ROI_buffer_radius_meters, is_geospatial=True\n        )\n    # If requested, save out the images corresponding to this subset of cameras.\n    # This is useful for model training.\n    if subset_images_savefolder is not None:\n        camera_set.save_images(subset_images_savefolder)\n\n    # Determine how many chunks, if any, the mesh cameras should be split into\n    n_render_chunks = (\n        None\n        if n_cameras_per_chunk is None\n        else int(ceil(len(camera_set) / n_cameras_per_chunk))\n    )\n    # Select whether to use a class that renders by chunks or not\n    MeshClass = (\n        TexturedPhotogrammetryMesh\n        if n_render_chunks is None\n        else TexturedPhotogrammetryMeshChunked\n    )\n\n    ## Create the textured mesh\n    mesh = MeshClass(\n        mesh_file,\n        input_CRS=mesh_CRS,\n        downsample_target=mesh_downsample,\n        texture=texture,\n        texture_column_name=texture_column_name,\n        ROI=ROI,\n        ROI_buffer_meters=mesh_ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n    )\n\n    ## Set the ground class if applicable\n    if DTM_file is not None and ground_height_threshold is not None:\n        # The ground ID will be set to the next value if None, or np.nan if np.nan\n        ground_ID = None if render_ground_class else np.nan\n        mesh.label_ground_class(\n            DTM_file=DTM_file,\n            height_above_ground_threshold=ground_height_threshold,\n            only_label_existing_labels=True,\n            ground_class_name=\"GROUND\",\n            ground_ID=ground_ID,\n            set_mesh_texture=True,\n        )\n\n    # Save the textured and subsetted mesh, if applicable\n    if textured_mesh_savefile is not None:\n        mesh.save_mesh(textured_mesh_savefile)\n\n    # Show the cameras and mesh if requested\n    if vis or mesh_vis_file is not None:\n        mesh.vis(camera_set=camera_set, screenshot_filename=mesh_vis_file)\n\n    # Include n_render_chunks as an optional keyword argument, if provided. This is only applicable\n    # if this mesh is a TexturedPhotogrammetryMeshChunked object\n    render_kwargs = {} if n_render_chunks is None else {\"n_clusters\": n_render_chunks}\n    # Render the labels and save them. This is the slow step.\n    mesh.save_renders(\n        camera_set=camera_set,\n        render_image_scale=render_image_scale,\n        save_native_resolution=True,\n        output_folder=render_savefolder,\n        make_composites=False,\n        cast_to_uint8=cast_to_uint8,\n        save_as_npy=save_as_npy,\n        **render_kwargs,\n    )\n\n    if vis or labels_vis_folder is not None:\n        # Show some examples of the rendered labels side-by-side with the real images\n        show_segmentation_labels(\n            label_folder=render_savefolder,\n            image_folder=image_folder,\n            savefolder=labels_vis_folder,\n            label_suffix=\".tif\",\n            num_show=10,\n        )\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.aggregate_images.aggregate_images","title":"<code>aggregate_images(mesh_file, cameras_file, image_folder, label_folder, mesh_CRS, original_image_folder=None, subset_images_folder=None, filename_regex=None, take_every_nth_camera=100, DTM_file=None, height_above_ground_threshold=2.0, ROI=None, ROI_buffer_radius_meters=50, IDs_to_labels=None, mesh_downsample=1.0, n_aggregation_clusters=None, n_cameras_per_aggregation_cluster=None, aggregate_image_scale=1.0, aggregated_face_values_savefile=None, predicted_face_classes_savefile=None, top_down_vector_projection_savefile=None, vis=False)</code>","text":"<p>Aggregate labels from multiple viewpoints onto the surface of the mesh</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the MetaShape-exported .xml cameras file</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to create the mesh</p> required <code>filename_regex</code> <code>str</code> <p>Use only images with paths matching this regex</p> <code>None</code> <code>label_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of labels to be aggregated onto the mesh. Must be in the same structure as the images</p> required <code>mesh_CRS</code> <code>CRS</code> <p>The CRS to interpret the mesh in.</p> required <code>original_image_folder</code> <code>Union[PATH_TYPE, None]</code> <p>Where the images were when photogrammetry was run. Metashape saves imagenames with an absolute path which can cause issues. If this argument is provided, this path is removed from the start of each image file name, which allows the camera set to be used with a moved folder of images specified by <code>image_folder</code>. Defaults to None.</p> <code>None</code> <code>subset_images_folder</code> <code>Union[PATH_TYPE, None]</code> <p>Use only images from this subset. Defaults to None.</p> <code>None</code> <code>take_every_nth_camera</code> <code>Union[int, None]</code> <p>Downsample the camera set to only every nth camera if set. Defaults to None.</p> <code>100</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a digital terrain model file to remove ground points. Defaults to None.</p> <code>None</code> <code>height_above_ground_threshold</code> <code>float</code> <p>Height in meters above the DTM to consider ground. Only used if DTM_file is set. Defaults to 2.0.</p> <code>2.0</code> <code>ROI</code> <code>Union[PATH_TYPE, None]</code> <p>Geofile region of interest to crop the mesh to. Defaults to None.</p> <code>None</code> <code>ROI_buffer_radius_meters</code> <code>float</code> <p>Keep points within this distance of the provided ROI object, if unset, everything will be kept. Defaults to 50.</p> <code>50</code> <code>IDs_to_labels</code> <code>Union[dict, None]</code> <p>Maps from integer IDs to human-readable class name labels. Defaults to None.</p> <code>None</code> <code>mesh_downsample</code> <code>float</code> <p>Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.0.</p> <code>1.0</code> <code>n_aggregation_clusters</code> <code>Union[int, None]</code> <p>If set, aggregate with this many clusters. Defaults to None.</p> <code>None</code> <code>n_cameras_per_aggregation_cluster</code> <code>Union[int, None]</code> <p>If set, and n_aggregation_clusters is not, use to compute a number of clusters such that each cluster has this many cameras. Defaults to None.</p> <code>None</code> <code>aggregate_image_scale</code> <code>float</code> <p>Downsample the labels before aggregation for faster runtime but lower quality. Defaults to 1.0.</p> <code>1.0</code> <code>aggregated_face_values_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the aggregated image values as a numpy array. Defaults to None.</p> <code>None</code> <code>predicted_face_classes_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the most common label per face texture as a numpy array. Defaults to None.</p> <code>None</code> <code>top_down_vector_projection_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to export the predicted map. Defaults to None.</p> <code>None</code> <code>vis</code> <code>bool</code> <p>Show the mesh model and predicted results. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/entrypoints/aggregate_images.py</code> <pre><code>def aggregate_images(\n    mesh_file: PATH_TYPE,\n    cameras_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    label_folder: PATH_TYPE,\n    mesh_CRS: pyproj.CRS,\n    original_image_folder: typing.Union[PATH_TYPE, None] = None,\n    subset_images_folder: typing.Union[PATH_TYPE, None] = None,\n    filename_regex: typing.Optional[str] = None,\n    take_every_nth_camera: typing.Union[int, None] = 100,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    height_above_ground_threshold: float = 2.0,\n    ROI: typing.Union[PATH_TYPE, None] = None,\n    ROI_buffer_radius_meters: float = 50,\n    IDs_to_labels: typing.Union[dict, str, None] = None,\n    mesh_downsample: float = 1.0,\n    n_aggregation_clusters: typing.Union[int, None] = None,\n    n_cameras_per_aggregation_cluster: typing.Union[int, None] = None,\n    aggregate_image_scale: float = 1.0,\n    aggregated_face_values_savefile: typing.Union[PATH_TYPE, None] = None,\n    predicted_face_classes_savefile: typing.Union[PATH_TYPE, None] = None,\n    top_down_vector_projection_savefile: typing.Union[PATH_TYPE, None] = None,\n    vis: bool = False,\n):\n    \"\"\"Aggregate labels from multiple viewpoints onto the surface of the mesh\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        cameras_file (PATH_TYPE):\n            Path to the MetaShape-exported .xml cameras file\n        image_folder (PATH_TYPE):\n            Path to the folder of images used to create the mesh\n        filename_regex (str, optional):\n            Use only images with paths matching this regex\n        label_folder (PATH_TYPE):\n            Path to the folder of labels to be aggregated onto the mesh. Must be in the same\n            structure as the images\n        mesh_CRS (pyproj.CRS):\n            The CRS to interpret the mesh in.\n        original_image_folder (typing.Union[PATH_TYPE, None], optional):\n            Where the images were when photogrammetry was run. Metashape saves imagenames with an\n            absolute path which can cause issues. If this argument is provided, this path is removed\n            from the start of each image file name, which allows the camera set to be used with a\n            moved folder of images specified by `image_folder`. Defaults to None.\n        subset_images_folder (typing.Union[PATH_TYPE, None], optional):\n            Use only images from this subset. Defaults to None.\n        take_every_nth_camera (typing.Union[int, None], optional):\n            Downsample the camera set to only every nth camera if set. Defaults to None.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a digital terrain model file to remove ground points. Defaults to None.\n        height_above_ground_threshold (float, optional):\n            Height in meters above the DTM to consider ground. Only used if DTM_file is set.\n            Defaults to 2.0.\n        ROI (typing.Union[PATH_TYPE, None], optional):\n            Geofile region of interest to crop the mesh to. Defaults to None.\n        ROI_buffer_radius_meters (float, optional):\n            Keep points within this distance of the provided ROI object, if unset, everything will\n            be kept. Defaults to 50.\n        IDs_to_labels (typing.Union[dict, None], optional):\n            Maps from integer IDs to human-readable class name labels. Defaults to None.\n        mesh_downsample (float, optional):\n            Downsample the mesh to this fraction of vertices for increased performance but lower\n            quality. Defaults to 1.0.\n        n_aggregation_clusters (typing.Union[int, None]):\n            If set, aggregate with this many clusters. Defaults to None.\n        n_cameras_per_aggregation_cluster (typing.Union[int, None]):\n            If set, and n_aggregation_clusters is not, use to compute a number of clusters such that\n            each cluster has this many cameras. Defaults to None.\n        aggregate_image_scale (float, optional):\n            Downsample the labels before aggregation for faster runtime but lower quality. Defaults\n            to 1.0.\n        aggregated_face_values_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the aggregated image values as a numpy array. Defaults to None.\n        predicted_face_classes_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the most common label per face texture as a numpy array. Defaults to None.\n        top_down_vector_projection_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to export the predicted map. Defaults to None.\n        vis (bool, optional):\n            Show the mesh model and predicted results. Defaults to False.\n    \"\"\"\n\n    if isinstance(IDs_to_labels, str):\n        IDs_to_labels = {\n            int(k): v for k, v in json.load(open(IDs_to_labels, \"r\")).items()\n        }\n\n    ## Create the camera set\n    # Do the camera operations first because they are fast and good initial error checking\n    camera_set = MetashapeCameraSet(\n        cameras_file,\n        image_folder,\n        original_image_folder=original_image_folder,\n        validate_images=True,\n    )\n\n    # If the ROI is not None, subset to cameras within a buffer distance of the ROI\n    # TODO let get_subset_ROI accept a None ROI and return the full camera set\n    if subset_images_folder is not None:\n        camera_set = camera_set.get_cameras_in_folder(subset_images_folder)\n\n    # Subset based on regex if requested\n    if filename_regex is not None:\n        camera_set = camera_set.get_cameras_matching_filename_regex(\n            filename_regex=filename_regex\n        )\n\n    # If you only want to take every nth camera, helpful for initial testing\n    if take_every_nth_camera is not None:\n        camera_set = camera_set.get_subset_cameras(\n            range(0, len(camera_set), take_every_nth_camera)\n        )\n\n    if ROI is not None and ROI_buffer_radius_meters is not None:\n        # Extract cameras near the training data\n        camera_set = camera_set.get_subset_ROI(\n            ROI=ROI, buffer_radius=ROI_buffer_radius_meters\n        )\n\n    # If the number of aggregation clusters is not set but the number of cameras per cluster is,\n    # then compute it\n    if n_aggregation_clusters is None and n_cameras_per_aggregation_cluster is not None:\n        n_aggregation_clusters = int(\n            math.ceil(len(camera_set) / n_cameras_per_aggregation_cluster)\n        )\n\n    # Choose whether to use a mesh class that aggregates by clusters of cameras and chunks of the mesh\n    MeshClass = (\n        TexturedPhotogrammetryMesh\n        if n_aggregation_clusters is None\n        else TexturedPhotogrammetryMeshChunked\n    )\n    ## Create the mesh\n    mesh = MeshClass(\n        mesh_file,\n        input_CRS=mesh_CRS,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n        downsample_target=mesh_downsample,\n    )\n\n    # Show the mesh if requested\n    if vis:\n        mesh.vis(camera_set=camera_set)\n\n    # Create a segmentor object to load in the predictions\n    segmentor = LookUpSegmentor(\n        base_folder=image_folder,\n        lookup_folder=label_folder,\n        num_classes=np.max(list(mesh.get_IDs_to_labels().keys())) + 1,\n    )\n    # Create a camera set that returns the segmented images instead of the original ones\n    segmentor_camera_set = SegmentorPhotogrammetryCameraSet(\n        camera_set, segmentor=segmentor\n    )\n\n    # Create the potentially-empty dict of kwargs to match what this class expects\n    n_clusters_kwargs = (\n        {} if n_aggregation_clusters is None else {\"n_clusters\": n_aggregation_clusters}\n    )\n\n    ## Perform aggregation, this is the slow step\n    aggregated_face_labels, _ = mesh.aggregate_projected_images(\n        segmentor_camera_set,\n        aggregate_img_scale=aggregate_image_scale,\n        **n_clusters_kwargs,\n    )\n\n    # If requested, save this data\n    if aggregated_face_values_savefile is not None:\n        ensure_containing_folder(aggregated_face_values_savefile)\n        np.save(aggregated_face_values_savefile, aggregated_face_labels)\n\n    # Find the index of the most common class per face, with faces with no predictions set to nan\n    predicted_face_classes = find_argmax_nonzero_value(\n        aggregated_face_labels, keepdims=True\n    )\n\n    # If requested, label the ground faces\n    if DTM_file is not None and height_above_ground_threshold is not None:\n        predicted_face_classes = mesh.label_ground_class(\n            labels=predicted_face_classes,\n            height_above_ground_threshold=height_above_ground_threshold,\n            DTM_file=DTM_file,\n            ground_ID=np.nan,\n            set_mesh_texture=False,\n        )\n\n    if predicted_face_classes_savefile is not None:\n        ensure_containing_folder(predicted_face_classes_savefile)\n        np.save(predicted_face_classes_savefile, predicted_face_classes)\n\n    if vis:\n        # Show the mesh with predicted classes\n        mesh.vis(vis_scalars=predicted_face_classes)\n\n    # If the vector file should be exported\n    if top_down_vector_projection_savefile is not None:\n        # Compute the label names\n        if IDs_to_labels is not None:\n            # This ensures that any missing keys are replaced with None so proper indexing is retained\n            label_names = [\n                IDs_to_labels.get(i, None)\n                for i in range(max(list(IDs_to_labels.keys())) + 1)\n            ]\n        else:\n            label_names = None\n        # Export the 2D top down projection\n        mesh.export_face_labels_vector(\n            face_labels=np.squeeze(predicted_face_classes),\n            export_file=top_down_vector_projection_savefile,\n            vis=vis,\n            label_names=label_names,\n        )\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.project_detections.project_detections","title":"<code>project_detections(mesh_filename, mesh_CRS, cameras_filename, project_to_mesh=False, convert_to_geospatial=False, image_folder=None, detections_folder=None, projections_to_mesh_filename=None, projections_to_geospatial_savefilename=None, default_focal_length=None, image_shape=None, segmentor_kwargs={}, vis_mesh=False, vis_geodata=False)</code>","text":"<p>Project per-image detections to geospatial coordinates</p> <p>Parameters:</p> Name Type Description Default <code>mesh_filename</code> <code>PATH_TYPE</code> <p>Path to mesh file, in local coordinates from Metashape</p> required <code>mesh_CRS</code> <code>CRS</code> <p>The CRS to interpret the mesh in</p> required <code>cameras_filename</code> <code>PATH_TYPE</code> <p>Path to cameras file. This also contains local-to-global coordinate transform to convert the mesh to geospatial units.</p> required <code>project_to_mesh</code> <code>bool</code> <p>Execute the projection to mesh step. Defaults to False.</p> <code>False</code> <code>convert_to_geospatial</code> <code>bool</code> <p>Execute the conversion to geospatial step. Defaults to False.</p> <code>False</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to generate the detections. TODO, see if this can be removed since none of this information is actually used. Defaults to None.</p> <code>None</code> <code>detections_folder</code> <code>PATH_TYPE</code> <p>Folder of detections in the DeepForest format, one per image. Defaults to None.</p> <code>None</code> <code>projections_to_mesh_filename</code> <code>PATH_TYPE</code> <p>Where to save and/or load from the data for the detections projected to the mesh faces. Defaults to None.</p> <code>None</code> <code>projections_to_geospatial_savefilename</code> <code>PATH_TYPE</code> <p>Where to export the geospatial detections. Defaults to None.</p> <code>None</code> <code>default_focal_length</code> <code>float</code> <p>Since the focal length is not provided in many cameras files, it can be specified. The units are in pixels. TODO, figure out where this information can be reliably obtained from. Defaults to None.</p> <code>None</code> <code>segmentor_kwargs</code> <code>dict</code> <p>Dict of keyword arguments to pass to the segmentor. Defaults to {}.</p> <code>{}</code> <code>vis_mesh</code> <code>bool</code> <p>Show the mesh with detections projected onto it. Defaults to False.</p> <code>False</code> <code>vis_geodata</code> <code>bool</code> <p>Show the geospatial projection. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If convert_to_geospatial but no projections to mesh are available</p> <code>FileNotFoundError</code> <p>If the projections_to_mesh_filename is set and needed but not present</p> Source code in <code>geograypher/entrypoints/project_detections.py</code> <pre><code>def project_detections(\n    mesh_filename: PATH_TYPE,\n    mesh_CRS: pyproj.CRS,\n    cameras_filename: PATH_TYPE,\n    project_to_mesh: bool = False,\n    convert_to_geospatial: bool = False,\n    image_folder: PATH_TYPE = None,\n    detections_folder: PATH_TYPE = None,\n    projections_to_mesh_filename: PATH_TYPE = None,\n    projections_to_geospatial_savefilename: PATH_TYPE = None,\n    default_focal_length: float = None,\n    image_shape: tuple = None,\n    segmentor_kwargs: dict = {},\n    vis_mesh: bool = False,\n    vis_geodata: bool = False,\n):\n    \"\"\"Project per-image detections to geospatial coordinates\n\n    Args:\n        mesh_filename (PATH_TYPE):\n            Path to mesh file, in local coordinates from Metashape\n        mesh_CRS (pyproj.CRS):\n            The CRS to interpret the mesh in\n        cameras_filename (PATH_TYPE):\n            Path to cameras file. This also contains local-to-global coordinate transform to convert\n            the mesh to geospatial units.\n        project_to_mesh (bool, optional):\n            Execute the projection to mesh step. Defaults to False.\n        convert_to_geospatial (bool, optional):\n            Execute the conversion to geospatial step. Defaults to False.\n        image_folder (PATH_TYPE, optional):\n            Path to the folder of images used to generate the detections. TODO, see if this can be\n            removed since none of this information is actually used. Defaults to None.\n        detections_folder (PATH_TYPE, optional):\n            Folder of detections in the DeepForest format, one per image. Defaults to None.\n        projections_to_mesh_filename (PATH_TYPE, optional):\n            Where to save and/or load from the data for the detections projected to the mesh faces.\n            Defaults to None.\n        projections_to_geospatial_savefilename (PATH_TYPE, optional):\n            Where to export the geospatial detections. Defaults to None.\n        default_focal_length (float, optional):\n            Since the focal length is not provided in many cameras files, it can be specified.\n            The units are in pixels. TODO, figure out where this information can be reliably obtained\n            from. Defaults to None.\n        segmentor_kwargs (dict, optional):\n            Dict of keyword arguments to pass to the segmentor. Defaults to {}.\n        vis_mesh (bool, optional):\n            Show the mesh with detections projected onto it. Defaults to False.\n        vis_geodata (bool, optional):\n            Show the geospatial projection. Defaults to False.\n\n    Raises:\n        ValueError: If convert_to_geospatial but no projections to mesh are available\n        FileNotFoundError: If the projections_to_mesh_filename is set and needed but not present\n    \"\"\"\n    # Create the mesh object, which will be used for either workflow\n    mesh = TexturedPhotogrammetryMeshIndexPredictions(mesh_filename, input_CRS=mesh_CRS)\n\n    # Project per-image detections to the mesh\n    if project_to_mesh:\n        # Create a camera set associated with the images that have detections\n        camera_set = MetashapeCameraSet(\n            cameras_filename,\n            image_folder,\n            default_sensor_params={\"f\": default_focal_length, \"cx\": 0, \"cy\": 0},\n        )\n        # Infer the image shape from the first image in the folder\n        if image_shape is None:\n            image_filename_list = sorted(list(Path(image_folder).glob(\"*.*\")))\n            if len(image_filename_list) &gt; 0:\n                first_file = image_filename_list[0]\n                logging.info(f\"loading image shape from {first_file}\")\n                first_image = imread(first_file)\n                image_shape = first_image.shape[:2]\n            else:\n                raise ValueError(\n                    f\"No image_shape provided and folder of images {image_folder} was empty\"\n                )\n        # Create an object that looks up the detections from a folder of CSVs or one individual one.\n        # Using this, it can generate \"predictions\" for a given image.\n        detections_predictor = TabularRectangleSegmentor(\n            detection_file_or_folder=detections_folder,\n            image_folder=image_folder,\n            image_shape=image_shape,\n            **segmentor_kwargs,\n        )\n\n        # If a file is provided for the projections, save the detection info alongside it\n        if projections_to_mesh_filename is not None:\n            # Export the per-image detection information as one standardized file\n            detection_info_file = Path(\n                projections_to_mesh_filename.parent,\n                projections_to_mesh_filename.stem + \"_detection_info.csv\",\n            )\n            logging.info(f\"Saving detection info to {detection_info_file}\")\n            detections_predictor.save_detection_data(detection_info_file)\n\n        # Wrap the camera set so that it returns the detections rather than the original images\n        detections_camera_set = SegmentorPhotogrammetryCameraSet(\n            camera_set, segmentor=detections_predictor\n        )\n        # Project the detections to the mesh\n        aggregated_prejected_images_returns = mesh.aggregate_projected_images(\n            cameras=detections_camera_set, n_classes=detections_predictor.num_classes\n        )\n        # Get the summed (not averaged) projections\n        aggregated_projections = aggregated_prejected_images_returns[1][\n            \"summed_projections\"\n        ]\n\n        if projections_to_mesh_filename is not None:\n            # Export the per-face texture to an npz file, since it's a sparse array\n            ensure_containing_folder(projections_to_mesh_filename)\n            save_npz(projections_to_mesh_filename, aggregated_projections)\n\n        if vis_mesh:\n            # Determine which detection is predicted for each face, if any. In cases where multiple\n            # detections project to the same face, the one with the lower index will be reported\n            detection_ID_per_face = np.argmax(aggregated_projections, axis=1).astype(\n                float\n            )\n            # Mask out locations for which there are no predictions\n            detection_ID_per_face[np.sum(aggregated_projections, axis=1) == 0] = np.nan\n            # Show the mesh\n            mesh.vis(vis_scalars=detection_ID_per_face)\n\n    # Convert per-face projections to geospatial ones\n    if convert_to_geospatial:\n        # Determine if the mesh texture was computed in the last step or otherwise if it can be loaded\n        if not project_to_mesh:\n            if projections_to_mesh_filename is None:\n                raise ValueError(\"No projections_to_mesh_savefilename provided\")\n            elif os.path.isfile(projections_to_mesh_filename):\n                aggregated_projections = load_npz(projections_to_mesh_filename)\n                detection_info_file = Path(\n                    projections_to_mesh_filename.parent,\n                    projections_to_mesh_filename.stem + \"_detection_info.csv\",\n                )\n                detection_info = pd.read_csv(detection_info_file)\n            else:\n                raise FileNotFoundError(\n                    f\"projections_to_mesh_filename {projections_to_mesh_filename} not found\"\n                )\n        else:\n            detection_info = detections_predictor.get_all_detections()\n\n        # Convert the per-face labels to geospatial coordinates. Optionally vis and/or export\n        mesh.export_face_labels_vector(\n            face_labels=aggregated_projections,\n            export_file=projections_to_geospatial_savefilename,\n            vis=vis_geodata,\n        )\n\n        projected_geo_data = gpd.read_file(projections_to_geospatial_savefilename)\n        # Merge the two dataframes so the left df's \"class_ID\" field aligns with the right df's\n        # \"instance_ID\". This will add back the original data assocaited with each per-image detection\n        # to the projected data.\n        # Add the \"_right\" suffix to any of the original fields that share a name with the ones in the\n        # projected data\n        merged = projected_geo_data.merge(\n            detection_info,\n            left_on=CLASS_ID_KEY,\n            right_on=INSTANCE_ID_KEY,\n            suffixes=(None, \"_right\"),\n        )\n        # Drop the columns that are just an integer ID, except for \"instance_ID\"\n        # TODO determine why \"Unnamed: 0\" appears\n        merged.drop(columns=[CLASS_ID_KEY, \"Unnamed: 0\"], inplace=True)\n\n        # Save the data back out with the updated information\n        merged.to_file(projections_to_geospatial_savefilename)\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.label_polygons.label_polygons","title":"<code>label_polygons(mesh_file, input_CRS, aggregated_face_values_file, geospatial_polygons_to_label, geospatial_polygons_labeled_savefile, mesh_downsample=1.0, DTM_file=None, height_above_ground_threshold=2.0, ground_voting_weight=0.01, ROI=None, ROI_buffer_radius_meters=50, n_polygons_per_cluster=1000, IDs_to_labels=None, vis_mesh=False)</code>","text":"<p>Label each polygon with the most commonly predicted class as computed by the weighted sum of 3D face areas</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>input_CRS</code> <code>CRS</code> <p>The CRS to interpret the mesh in.</p> required <code>aggregated_face_values_file</code> <code>PATH_TYPE</code> <p>Path to a (n_faces, n_classes) numpy array containing the frequency of each class prediction for each face</p> required <code>geospatial_polygons_to_label</code> <code>Union[PATH_TYPE, None]</code> <p>Each polygon/multipolygon will be labeled independently. Defaults to None.</p> required <code>geospatial_polygons_labeled_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the labeled results.</p> required <code>mesh_downsample</code> <code>float</code> <p>Fraction to downsample mesh. Should match what was used to generate the aggregated_face_values_file. Defaults to 1.0.</p> <code>1.0</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a digital terrain model file to remove ground points. Defaults to None.</p> <code>None</code> <code>height_above_ground_threshold</code> <code>float</code> <p>Height in meters above the DTM to consider ground. Only used if DTM_file is set. Defaults to 2.0.</p> <code>2.0</code> <code>ground_voting_weight</code> <code>float</code> <p>Faces identified as ground are given this weight during voting. Defaults to 0.01.</p> <code>0.01</code> <code>ROI</code> <code>Union[PATH_TYPE, None]</code> <p>Geofile region of interest to crop the mesh to. Should match what was used to generate aggregated_face_values_file. Defaults to None.</p> <code>None</code> <code>ROI_buffer_radius_meters</code> <code>float</code> <p>Keep points within this distance of the provided ROI object, if unset, everything will be kept. Should match what was used to generate aggregated_face_values_file. Defaults to 50.</p> <code>50</code> <code>n_polygons_per_cluster</code> <code>int</code> <p>The number of polygons to use in each cluster, when computing labeling by chunks. Defaults to 1000.</p> <code>1000</code> <code>IDs_to_labels</code> <code>Union[dict, None]</code> <p>Mapping from integer IDs to human readable labels. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/entrypoints/label_polygons.py</code> <pre><code>def label_polygons(\n    mesh_file: PATH_TYPE,\n    input_CRS: pyproj.CRS,\n    aggregated_face_values_file: PATH_TYPE,\n    geospatial_polygons_to_label: typing.Union[PATH_TYPE, None],\n    geospatial_polygons_labeled_savefile: typing.Union[PATH_TYPE, None],\n    mesh_downsample: float = 1.0,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    height_above_ground_threshold: float = 2.0,\n    ground_voting_weight: float = 0.01,\n    ROI: typing.Union[PATH_TYPE, None] = None,\n    ROI_buffer_radius_meters: float = 50,\n    n_polygons_per_cluster: int = 1000,\n    IDs_to_labels: typing.Union[dict, None] = None,\n    vis_mesh: bool = False,\n):\n    \"\"\"\n    Label each polygon with the most commonly predicted class as computed by the weighted sum of 3D\n    face areas\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        input_CRS (pyproj.CRS):\n            The CRS to interpret the mesh in.\n        aggregated_face_values_file (PATH_TYPE):\n            Path to a (n_faces, n_classes) numpy array containing the frequency of each class\n            prediction for each face\n        geospatial_polygons_to_label (typing.Union[PATH_TYPE, None], optional):\n            Each polygon/multipolygon will be labeled independently. Defaults to None.\n        geospatial_polygons_labeled_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the labeled results.\n        mesh_downsample (float, optional):\n            Fraction to downsample mesh. Should match what was used to generate the\n            aggregated_face_values_file. Defaults to 1.0.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a digital terrain model file to remove ground points. Defaults to None.\n        height_above_ground_threshold (float, optional):\n            Height in meters above the DTM to consider ground. Only used if DTM_file is set.\n            Defaults to 2.0.\n        ground_voting_weight (float, optional):\n            Faces identified as ground are given this weight during voting. Defaults to 0.01.\n        ROI (typing.Union[PATH_TYPE, None], optional):\n            Geofile region of interest to crop the mesh to. Should match what was used to generate\n            aggregated_face_values_file. Defaults to None.\n        ROI_buffer_radius_meters (float, optional):\n            Keep points within this distance of the provided ROI object, if unset, everything will\n            be kept. Should match what was used to generate aggregated_face_values_file. Defaults to 50.\n        n_polygons_per_cluster (int, optional):\n            The number of polygons to use in each cluster, when computing labeling by chunks.\n            Defaults to 1000.\n        IDs_to_labels (typing.Union[dict, None], optional):\n            Mapping from integer IDs to human readable labels. Defaults to None.\n    \"\"\"\n    # Load this first because it's quick\n    aggregated_face_values = np.load(aggregated_face_values_file)\n    predicted_face_classes = np.argmax(aggregated_face_values, axis=1).astype(float)\n    no_preds_mask = np.all(np.logical_not(np.isfinite(aggregated_face_values)), axis=1)\n    predicted_face_classes[no_preds_mask] = np.nan\n\n    ## Create the mesh\n    mesh = TexturedPhotogrammetryMeshChunked(\n        mesh_file,\n        input_CRS=input_CRS,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n        downsample_target=mesh_downsample,\n    )\n\n    if vis_mesh:\n        mesh.vis(vis_scalars=predicted_face_classes)\n\n    # Extract which vertices are labeled as ground\n    # TODO check that the types are correct here\n    ground_mask_verts = mesh.get_height_above_ground(\n        DTM_file=DTM_file,\n        threshold=height_above_ground_threshold,\n    )\n    # Convert that vertex labels into face labels\n    ground_mask_faces = mesh.vert_to_face_texture(ground_mask_verts)\n\n    # Ground points get a weighting of ground_voting_weight, others get 1\n    ground_weighting = 1 - (\n        (1 - ground_voting_weight) * ground_mask_faces.astype(float)\n    )\n    if vis_mesh:\n        ground_masked_predicted_face_classes = predicted_face_classes.copy()\n        ground_masked_predicted_face_classes[ground_mask_faces.astype(bool)] = np.nan\n        mesh.vis(vis_scalars=ground_masked_predicted_face_classes)\n\n    # Perform per-polygon labeling\n    polygon_labels = mesh.label_polygons(\n        face_labels=predicted_face_classes,\n        polygons=geospatial_polygons_to_label,\n        face_weighting=ground_weighting,\n        n_polygons_per_cluster=n_polygons_per_cluster,\n    )\n\n    # Save out the predicted classes into a copy of the original file\n    geospatial_polygons = gpd.read_file(geospatial_polygons_to_label)\n    geospatial_polygons[PRED_CLASS_ID_KEY] = polygon_labels\n    ensure_containing_folder(geospatial_polygons_labeled_savefile)\n    geospatial_polygons.to_file(geospatial_polygons_labeled_savefile)\n</code></pre>"},{"location":"API_reference/cameras/","title":"Cameras","text":"<ul> <li> <p>Cameras Docstrings</p> </li> <li> <p>Derived Cameras Docstrings</p> </li> <li> <p>Segmentor Docstrings</p> </li> </ul>"},{"location":"API_reference/cameras/cameras/","title":"Cameras Docstrings","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera","title":"<code>PhotogrammetryCamera</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCamera:\n    def __init__(\n        self,\n        image_filename: PATH_TYPE,\n        cam_to_world_transform: np.ndarray,\n        f: float,\n        cx: float,\n        cy: float,\n        image_width: int,\n        image_height: int,\n        distortion_params: Dict[str, float] = {},\n        lon_lat: Union[None, Tuple[float, float]] = None,\n        local_to_epsg_4978_transform: Union[np.array, None] = None,\n    ):\n        \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n        Args:\n            image_filename (PATH_TYPE): The image used for reconstruction\n            transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n            f (float): Focal length in pixels\n            cx (float): Principle point x (pixels) from center\n            cy (float): Principle point y (pixels) from center\n            image_width (int): Input image width pixels\n            image_height (int): Input image height pixels\n            distortion_params (dict, optional): Distortion parameters, currently unused\n            lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n        \"\"\"\n        self.image_filename = image_filename\n        self.cam_to_world_transform = cam_to_world_transform\n        self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n        self.f = f\n        self.cx = cx\n        self.cy = cy\n        self.image_width = image_width\n        self.image_height = image_height\n        self.distortion_params = distortion_params\n        self._local_to_epsg_4978_transform = local_to_epsg_4978_transform\n\n        if lon_lat is None:\n            self.lon_lat = (None, None)\n        else:\n            self.lon_lat = lon_lat\n\n        self.image_size = (image_height, image_width)\n        self.image = None\n        self.cache_image = (\n            False  # Only set to true if you can hold all images in memory\n        )\n\n    def get_camera_hash(self, include_image_hash: bool = False):\n        \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n        Args:\n            include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n        Returns:\n            int: A hash value representing the current state of the camera\n        \"\"\"\n        # Geometric information of hash\n        transform_hash = self.cam_to_world_transform.tolist()\n        camera_settings = {\n            \"transform\": transform_hash,\n            \"f\": self.f,\n            \"cx\": self.cx,\n            \"cy\": self.cy,\n            \"image_width\": self.image_width,\n            \"image_height\": self.image_height,\n            \"distortion_params\": self.distortion_params,\n            \"lon_lat\": self.lon_lat,\n        }\n\n        # Include the image associated with the hash if specified\n        if include_image_hash:\n            camera_settings[\"image_filename\"] = str(self.image_filename)\n\n        camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n        hasher = hashlib.sha256()\n        hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n        return hasher.hexdigest()\n\n    def get_camera_properties(self):\n        \"\"\"Returns the properties about a camera.\n\n        Returns:\n            dict: A dictionary containing the focal length, principal point coordinates,\n                image height, image width, distortion parameters, and world_to_cam_transform.\n        \"\"\"\n        camera_properties = {\n            \"focal_length\": self.f,\n            \"principal_point_x\": self.cx,\n            \"principal_point_y\": self.cy,\n            \"image_height\": self.image_height,\n            \"image_width\": self.image_width,\n            \"distortion_params\": self.distortion_params,\n            \"world_to_cam_transform\": self.world_to_cam_transform,\n        }\n        return camera_properties\n\n    def get_image(self, image_scale: float = 1.0) -&gt; np.ndarray:\n        # Check if the image is cached\n        if self.image is None:\n            image = imread(self.image_filename)\n            if image.dtype == np.uint8:\n                image = image / 255.0\n\n            # Avoid unneccesary read if we have memory\n            if self.cache_image:\n                self.image = image\n        else:\n            image = self.image\n\n        # Resizing is never cached, consider revisiting\n        if image_scale != 1.0:\n            image = resize(\n                image,\n                (int(image.shape[0] * image_scale), int(image.shape[1] * image_scale)),\n            )\n\n        return image\n\n    def get_image_filename(self):\n        return self.image_filename\n\n    def get_image_size(self, image_scale=1.0):\n        \"\"\"Return image size, potentially scaled\n\n        Args:\n            image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n        Returns:\n            tuple[int]: (h, w) in pixels\n        \"\"\"\n        # We should never have to deal with other cases if the reported size is accurate\n        if self.image_size is not None:\n            pass\n        elif self.image is not None:\n            self.image_size = self.image.shape[:2]\n        else:\n            image = self.get_image()\n            self.image_size = image.shape[:2]\n\n        return (\n            int(self.image_size[0] * image_scale),\n            int(self.image_size[1] * image_scale),\n        )\n\n    def get_lon_lat(self, negate_easting=True):\n        \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n        if None in self.lon_lat:\n            self.lon_lat = get_GPS_exif(self.image_filename)\n\n            if negate_easting:\n                self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n        return self.lon_lat\n\n    def get_camera_location(\n        self, get_z_coordinate: bool = False, as_CRS: Optional[pyproj.CRS] = None\n    ):\n        \"\"\"Returns a tuple of camera coordinates from the camera-to-world transformation matrix.\n        Args:\n            get_z_coordinate (bool):\n                Flag that user can set if they want z-coordinates. Defaults to False.\n            as_CRS (Optional[pyproj.CRS]):\n                If given, return the points in the given CRS. If not given,\n                return points in the default frame (Metashape local)\n        Returns:\n            Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n        \"\"\"\n\n        if as_CRS is None:\n            point = self.cam_to_world_transform[0:3, 3]\n        else:\n            transformer = pyproj.Transformer.from_crs(\n                EARTH_CENTERED_EARTH_FIXED_CRS, as_CRS\n            )\n            cam_in_ECEF = (\n                self._local_to_epsg_4978_transform\n                @ self.cam_to_world_transform\n                @ np.array([[0, 0, 0, 1]]).T\n            )\n            point = transformer.transform(\n                xx=cam_in_ECEF[0, 0],\n                yy=cam_in_ECEF[1, 0],\n                zz=cam_in_ECEF[2, 0],\n            )\n        return tuple(point) if get_z_coordinate else tuple(point[:2])\n\n    def get_camera_view_angle(self, in_deg: bool = True) -&gt; tuple:\n        \"\"\"Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result\n\n        Args:\n            in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n        Returns:\n            tuple: (pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter\n        \"\"\"\n        # This is the origin, a point at one unit along the principal axis, a point one unit\n        # up (-Y), and a point one unit right (+X)\n        points_in_camera_frame = np.array(\n            [[0, 0, 0, 1], [0, 0, 1, 1], [0, -1, 0, 1], [1, 0, 0, 1]]\n        ).T\n\n        # Transform the points first into the world frame and then into the earth-centered,\n        # earth-fixed frame\n        points_in_ECEF = (\n            self._local_to_epsg_4978_transform\n            @ self.cam_to_world_transform\n            @ points_in_camera_frame\n        )\n        # Remove the homogenous coordinate and transpose\n        points_in_ECEF = points_in_ECEF[:-1].T\n        # Convert to shapely points\n        points_in_ECEF = [Point(*point) for point in points_in_ECEF]\n        # Convert to a dataframe\n        points_in_ECEF = gpd.GeoDataFrame(\n            geometry=points_in_ECEF, crs=EARTH_CENTERED_EARTH_FIXED_CRS\n        )\n\n        # Convert to lat lon\n        points_in_lat_lon = points_in_ECEF.to_crs(LAT_LON_CRS)\n        # Convert to a local projected CRS\n        points_in_projected_CRS = ensure_projected_CRS(points_in_lat_lon)\n        # Extract the geometry\n        points_in_projected_CRS = np.array(\n            [[p.x, p.y, p.z] for p in points_in_projected_CRS.geometry]\n        )\n\n        # Compute three vectors starting at the camera origin\n        view_vector = points_in_projected_CRS[1] - points_in_projected_CRS[0]\n        up_vector = points_in_projected_CRS[2] - points_in_projected_CRS[0]\n        right_vector = points_in_projected_CRS[3] - points_in_projected_CRS[0]\n\n        # The nadir vector points straight down\n        NADIR_VEC = np.array([0, 0, -1])\n\n        # For pitch, project the view vector onto the plane defined by the up vector and the nadir\n        pitch_projection_view_vec = projection_onto_plane(\n            view_vector, up_vector, NADIR_VEC\n        )\n        # For yaw, project the view vector onto the plane defined by the right vector and the nadir\n        yaw_projection_view_vec = projection_onto_plane(\n            view_vector, right_vector, NADIR_VEC\n        )\n\n        # Find the angle between these projected vectors and the nadir vector\n        pitch_angle = angle_between(pitch_projection_view_vec, NADIR_VEC)\n        yaw_angle = angle_between(yaw_projection_view_vec, NADIR_VEC)\n\n        # Return in degrees if requested\n        if in_deg:\n            return (np.rad2deg(pitch_angle), np.rad2deg(yaw_angle))\n        # Return in radians\n        return (pitch_angle, yaw_angle)\n\n    def get_local_to_epsg_4978_transform(self) -&gt; np.ndarray:\n        \"\"\"\n        Return the 4x4 homogenous transform mapping from the local coordinates used for\n        photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by\n        EPSG:4978 (https://epsg.io/4978).\n\n        Returns:\n            np.ndarray:\n                The transform in the form:\n                   [R | t]\n                   [0 | 1]\n                When a homogenous vector is multiplied on the right of this matrix, it is\n                transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse\n                of this matrix can be used to map from EPSG:4879 to local coordinates.\n        \"\"\"\n        return self._local_to_epsg_4978_transform\n\n    def check_projected_in_image(\n        self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n    ):\n        \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n        Args:\n            homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n            image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n        Returns:\n            np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n            np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n        \"\"\"\n        img_width, image_height = image_size\n\n        # Divide by the z coord to project onto the image plane\n        image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n        # Transpose for convenience, (n_points, 3)\n        image_space_points = image_space_points.T\n\n        # We only want to consider points in front of the camera. Simple projection cannot tell\n        # if a point is on the same ray behind the camera\n        in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n        # Check that the point is projected within the image and is in front of the camera\n        # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n        valid_points_bool = (\n            (image_space_points[:, 0] &gt; 0)\n            * (image_space_points[:, 1] &gt; 0)\n            * (image_space_points[:, 0] &lt; img_width)\n            * (image_space_points[:, 1] &lt; image_height)\n            * in_front_of_cam\n        )\n\n        # Extract the points that are valid\n        valid_image_space_points = image_space_points[valid_points_bool, :].to(\n            torch.int\n        )\n        # Return the boolean array\n        valid_points_bool = valid_points_bool.cpu().numpy()\n        valid_image_space_points = valid_image_space_points.cpu().numpy\n        return valid_points_bool, valid_image_space_points\n\n    def extract_colors(\n        self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n    ):\n        \"\"\"_summary_\n\n        Args:\n            valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n            valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n            img (np.ndarray): (h, w, n_channels) image to query from\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # Set up the data arrays\n        colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n        mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n        # Set the entries which are valid to false, meaning a valid entry in the masked array\n        # TODO see if I can use valid_bool directly instead\n        valid_inds = np.where(valid_bool)[0]\n        mask[valid_inds, :] = False\n\n        # Extract coordinates\n        i_locs = valid_locs[:, 1]\n        j_locs = valid_locs[:, 0]\n        # Index based on the coordinates\n        valid_color_samples = img[i_locs, j_locs, :]\n        # Insert the valid samples into the array at the valid locations\n        colors_per_vertex[valid_inds, :] = valid_color_samples\n        # Convert to a masked array\n        masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n        return masked_color_per_vertex\n\n    def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n        \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # [R|t] matrix\n        transform_3x4_world_to_cam = torch.Tensor(\n            self.world_to_cam_transform[:3, :]\n        ).to(device)\n        K = torch.Tensor(\n            [\n                [self.f, 0, self.image_width / 2.0 + self.cx],\n                [0, self.f, self.image_width + self.cy],\n                [0, 0, 1],\n            ],\n            device=device,\n        )\n        # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n        camera_matrix = K @ transform_3x4_world_to_cam\n\n        # Add the extra dimension of ones for matrix multiplication\n        homogenous_mesh_verts = torch.concatenate(\n            (\n                torch.Tensor(mesh_verts).to(device),\n                torch.ones((mesh_verts.shape[0], 1)).to(device),\n            ),\n            axis=1,\n        ).T\n\n        # TODO review terminology\n        homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n        # Determine what points project onto the image and at what locations\n        valid_bool, valid_locs = self.check_projected_in_image(\n            projected_verts=homogenous_camera_points,\n            image_size=(self.image_width, self.image_height),\n        )\n        # Extract corresponding colors from the image\n        colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n        return colors_per_vertex\n\n    def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n        \"\"\"\n        Get a pyvista camera at the location specified by photogrammetry.\n        Note that there is no principle point and only the vertical field of view is set\n\n        Args:\n            focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n        Returns:\n            pv.Camera: The pyvista camera from that viewpoint.\n        \"\"\"\n        # Instantiate a new camera\n        camera = pv.Camera()\n        # Get the position as the translational part of the transform\n        camera_position = self.cam_to_world_transform[:3, 3]\n        # Get the look point by transforming a ray along the camera's Z axis into world\n        # coordinates and then adding this to the location\n        camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n            (0, 0, focal_dist)\n        )\n        # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n        camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n        # Compute the vertical field of view\n        vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n        # Set the values\n        camera.focal_point = camera_look\n        camera.position = camera_position\n        camera.up = camera_up\n        camera.view_angle = vertical_FOV_angle\n\n        return camera\n\n    def get_vis_mesh(self, frustum_scale: float = 0.1) -&gt; pv.PolyData:\n        \"\"\"Get this camera as a mesh representation.\n\n        Args:\n            frustum_scale (float, optional): Size of cameras in world units.\n\n        Returns (PolyData): blue mesh of the camera as a frustum with a\n            red face indicating the image top.\n        \"\"\"\n\n        scaled_halfwidth = self.image_width / (self.f * 2)\n        scaled_halfheight = self.image_height / (self.f * 2)\n\n        scaled_cx = self.cx / self.f\n        scaled_cy = self.cy / self.f\n\n        right = scaled_cx + scaled_halfwidth\n        left = scaled_cx - scaled_halfwidth\n        top = scaled_cy + scaled_halfheight\n        bottom = scaled_cy - scaled_halfheight\n\n        vertices = (\n            np.array(\n                [\n                    [0, 0, 0],\n                    [right, top, 1],\n                    [right, bottom, 1],\n                    [left, bottom, 1],\n                    [left, top, 1],\n                ]\n            ).T\n            * frustum_scale\n        )\n        # Make the coordinates homogenous\n        vertices = np.vstack((vertices, np.ones((1, 5))))\n\n        # Project the vertices into the world cordinates\n        projected_vertices = self.cam_to_world_transform @ vertices\n\n        # Deal with the case where there is a scale transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        ## mesh faces\n        faces = np.hstack(\n            [\n                [3, 0, 1, 2],  # side\n                [3, 0, 2, 3],  # bottom\n                [3, 0, 3, 4],  # side\n                [3, 0, 4, 1],  # top\n                [3, 1, 2, 3],  # endcap triangle #1\n                [3, 3, 4, 1],  # endcap triangle #2\n            ]\n        )\n        # All blue except the top (-Y) surface is red\n        face_colors = np.array(\n            [\n                [0, 0, 255],\n                [255, 0, 0],\n                [0, 0, 255],\n                [0, 0, 255],\n                [0, 0, 255],\n                [0, 0, 255],\n            ]\n        ).astype(np.uint8)\n\n        # Create a mesh for the camera frustum\n        frustum = pv.PolyData(projected_vertices[:3].T, faces)\n        # Unsure exactly what's going on here, but it's required for it to be valid\n        frustum.triangulate()\n\n        # Assign the face colors to the mesh\n        frustum[\"RGB\"] = pv.pyvista_ndarray(face_colors)\n\n        return frustum\n\n    def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n        \"\"\"\n        Visualize the camera as a frustum, at the appropriate translation and\n        rotation and with the given focal length and aspect ratio.\n\n        Args:\n            plotter (pv.Plotter): The plotter to add the visualization to\n            frustum_scale (float, optional): The length of the frustum in world units.\n        \"\"\"\n\n        mesh = self.get_vis_mesh(frustum_scale)\n\n        # Show the mesh with the given face colors\n        plotter.add_mesh(\n            mesh,\n            scalars=\"RGB\",\n            rgb=True,\n        )\n\n    def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n        \"\"\"Compute rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n        Returns:\n            np.array: The projected vertices, TODO\n        \"\"\"\n        # Transform from i, j to x, y\n        pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n        # Cast a ray from the center of the mesh for vis\n        principal_point = np.array(\n            [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n        )\n        centered_pixel_coords = pixel_coords_xy - principal_point\n        scaled_pixel_coords = centered_pixel_coords / self.f\n\n        n_points = len(scaled_pixel_coords)\n\n        if n_points == 0:\n            return\n\n        def scale_vector(point, line_length):\n            \"\"\"\n            Helper function to normalize the direction vector and scale it so\n            the final vector magnitude is line_length.\n            \"\"\"\n            # Expand the (x, y) homogenous coordinates to (x, y, 1) and normalize\n            homogeneous = np.hstack([point, 1])\n            norm = np.linalg.norm(homogeneous)\n            return (homogeneous / norm) * line_length\n\n        line_verts = [\n            np.array(\n                [\n                    [0, 0, 0, 1],\n                    np.hstack([scale_vector(point, line_length), 1]),\n                ]\n            )\n            for point in scaled_pixel_coords\n        ]\n        line_verts = np.concatenate(line_verts, axis=0).T\n\n        projected_vertices = self.cam_to_world_transform @ line_verts\n\n        # Handle scale in transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        projected_vertices = projected_vertices[:3, :].T\n\n        return projected_vertices\n\n    def vis_rays(\n        self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n    ):\n        \"\"\"Show rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            plotter (pv.Plotter): Plotter to use.\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n        \"\"\"\n        # If there are no detections, just skip it\n        if len(pixel_coords_ij) == 0:\n            return\n\n        projected_vertices = self.cast_rays(\n            pixel_coords_ij=pixel_coords_ij, line_length=line_length\n        )\n        n_points = int(projected_vertices.shape[0] / 2)\n\n        lines = np.vstack(\n            (\n                np.full(n_points, fill_value=2),\n                np.arange(0, 2 * n_points, 2),\n                np.arange(0, 2 * n_points, 2) + 1,\n            )\n        ).T\n\n        mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n        plotter.add_mesh(mesh)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera-functions","title":"Functions","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.__init__","title":"<code>__init__(image_filename, cam_to_world_transform, f, cx, cy, image_width, image_height, distortion_params={}, lon_lat=None, local_to_epsg_4978_transform=None)</code>","text":"<p>Represents the information about one camera location/image as determined by photogrammetry</p> <p>Parameters:</p> Name Type Description Default <code>image_filename</code> <code>PATH_TYPE</code> <p>The image used for reconstruction</p> required <code>transform</code> <code>ndarray</code> <p>A 4x4 transform representing the camera-to-world transform</p> required <code>f</code> <code>float</code> <p>Focal length in pixels</p> required <code>cx</code> <code>float</code> <p>Principle point x (pixels) from center</p> required <code>cy</code> <code>float</code> <p>Principle point y (pixels) from center</p> required <code>image_width</code> <code>int</code> <p>Input image width pixels</p> required <code>image_height</code> <code>int</code> <p>Input image height pixels</p> required <code>distortion_params</code> <code>dict</code> <p>Distortion parameters, currently unused</p> <code>{}</code> <code>lon_lat</code> <code>Union[None, Tuple[float, float]]</code> <p>Location, defaults to None</p> <code>None</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    image_filename: PATH_TYPE,\n    cam_to_world_transform: np.ndarray,\n    f: float,\n    cx: float,\n    cy: float,\n    image_width: int,\n    image_height: int,\n    distortion_params: Dict[str, float] = {},\n    lon_lat: Union[None, Tuple[float, float]] = None,\n    local_to_epsg_4978_transform: Union[np.array, None] = None,\n):\n    \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n    Args:\n        image_filename (PATH_TYPE): The image used for reconstruction\n        transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n        f (float): Focal length in pixels\n        cx (float): Principle point x (pixels) from center\n        cy (float): Principle point y (pixels) from center\n        image_width (int): Input image width pixels\n        image_height (int): Input image height pixels\n        distortion_params (dict, optional): Distortion parameters, currently unused\n        lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n    \"\"\"\n    self.image_filename = image_filename\n    self.cam_to_world_transform = cam_to_world_transform\n    self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n    self.f = f\n    self.cx = cx\n    self.cy = cy\n    self.image_width = image_width\n    self.image_height = image_height\n    self.distortion_params = distortion_params\n    self._local_to_epsg_4978_transform = local_to_epsg_4978_transform\n\n    if lon_lat is None:\n        self.lon_lat = (None, None)\n    else:\n        self.lon_lat = lon_lat\n\n    self.image_size = (image_height, image_width)\n    self.image = None\n    self.cache_image = (\n        False  # Only set to true if you can hold all images in memory\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.cast_rays","title":"<code>cast_rays(pixel_coords_ij, line_length=10)</code>","text":"<p>Compute rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> <p>Returns:</p> Type Description <p>np.array: The projected vertices, TODO</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n    \"\"\"Compute rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n    Returns:\n        np.array: The projected vertices, TODO\n    \"\"\"\n    # Transform from i, j to x, y\n    pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n    # Cast a ray from the center of the mesh for vis\n    principal_point = np.array(\n        [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n    )\n    centered_pixel_coords = pixel_coords_xy - principal_point\n    scaled_pixel_coords = centered_pixel_coords / self.f\n\n    n_points = len(scaled_pixel_coords)\n\n    if n_points == 0:\n        return\n\n    def scale_vector(point, line_length):\n        \"\"\"\n        Helper function to normalize the direction vector and scale it so\n        the final vector magnitude is line_length.\n        \"\"\"\n        # Expand the (x, y) homogenous coordinates to (x, y, 1) and normalize\n        homogeneous = np.hstack([point, 1])\n        norm = np.linalg.norm(homogeneous)\n        return (homogeneous / norm) * line_length\n\n    line_verts = [\n        np.array(\n            [\n                [0, 0, 0, 1],\n                np.hstack([scale_vector(point, line_length), 1]),\n            ]\n        )\n        for point in scaled_pixel_coords\n    ]\n    line_verts = np.concatenate(line_verts, axis=0).T\n\n    projected_vertices = self.cam_to_world_transform @ line_verts\n\n    # Handle scale in transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    projected_vertices = projected_vertices[:3, :].T\n\n    return projected_vertices\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.check_projected_in_image","title":"<code>check_projected_in_image(homogenous_image_coords, image_size)</code>","text":"<p>Check if projected points are within the bound of the image and in front of camera</p> <p>Parameters:</p> Name Type Description Default <code>homogenous_image_coords</code> <code>ndarray</code> <p>The points after the application of K[R|t]. (3, n_points)</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>The size of the image (width, height) in pixels</p> required <p>Returns:</p> Type Description <p>np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)</p> <p>np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def check_projected_in_image(\n    self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n):\n    \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n    Args:\n        homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n        image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n    Returns:\n        np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n        np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n    \"\"\"\n    img_width, image_height = image_size\n\n    # Divide by the z coord to project onto the image plane\n    image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n    # Transpose for convenience, (n_points, 3)\n    image_space_points = image_space_points.T\n\n    # We only want to consider points in front of the camera. Simple projection cannot tell\n    # if a point is on the same ray behind the camera\n    in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n    # Check that the point is projected within the image and is in front of the camera\n    # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n    valid_points_bool = (\n        (image_space_points[:, 0] &gt; 0)\n        * (image_space_points[:, 1] &gt; 0)\n        * (image_space_points[:, 0] &lt; img_width)\n        * (image_space_points[:, 1] &lt; image_height)\n        * in_front_of_cam\n    )\n\n    # Extract the points that are valid\n    valid_image_space_points = image_space_points[valid_points_bool, :].to(\n        torch.int\n    )\n    # Return the boolean array\n    valid_points_bool = valid_points_bool.cpu().numpy()\n    valid_image_space_points = valid_image_space_points.cpu().numpy\n    return valid_points_bool, valid_image_space_points\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.extract_colors","title":"<code>extract_colors(valid_bool, valid_locs, img)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>valid_bool</code> <code>ndarray</code> <p>(n_points,) boolean array cooresponding to valid points</p> required <code>valid_locs</code> <code>ndarray</code> <p>(n_valid, 2) float array of image-space locations (x,y)</p> required <code>img</code> <code>ndarray</code> <p>(h, w, n_channels) image to query from</p> required <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def extract_colors(\n    self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n):\n    \"\"\"_summary_\n\n    Args:\n        valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n        valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n        img (np.ndarray): (h, w, n_channels) image to query from\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # Set up the data arrays\n    colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n    mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n    # Set the entries which are valid to false, meaning a valid entry in the masked array\n    # TODO see if I can use valid_bool directly instead\n    valid_inds = np.where(valid_bool)[0]\n    mask[valid_inds, :] = False\n\n    # Extract coordinates\n    i_locs = valid_locs[:, 1]\n    j_locs = valid_locs[:, 0]\n    # Index based on the coordinates\n    valid_color_samples = img[i_locs, j_locs, :]\n    # Insert the valid samples into the array at the valid locations\n    colors_per_vertex[valid_inds, :] = valid_color_samples\n    # Convert to a masked array\n    masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n    return masked_color_per_vertex\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_hash","title":"<code>get_camera_hash(include_image_hash=False)</code>","text":"<p>Generates a hash value for the camera's geometry and optionally includes the image</p> <p>Parameters:</p> Name Type Description Default <code>include_image_hash</code> <code>bool</code> <p>Whether to include the image filename in the hash computation. Defaults to false.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <p>A hash value representing the current state of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_hash(self, include_image_hash: bool = False):\n    \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n    Args:\n        include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n    Returns:\n        int: A hash value representing the current state of the camera\n    \"\"\"\n    # Geometric information of hash\n    transform_hash = self.cam_to_world_transform.tolist()\n    camera_settings = {\n        \"transform\": transform_hash,\n        \"f\": self.f,\n        \"cx\": self.cx,\n        \"cy\": self.cy,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"distortion_params\": self.distortion_params,\n        \"lon_lat\": self.lon_lat,\n    }\n\n    # Include the image associated with the hash if specified\n    if include_image_hash:\n        camera_settings[\"image_filename\"] = str(self.image_filename)\n\n    camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n    hasher = hashlib.sha256()\n    hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n    return hasher.hexdigest()\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_location","title":"<code>get_camera_location(get_z_coordinate=False, as_CRS=None)</code>","text":"<p>Returns a tuple of camera coordinates from the camera-to-world transformation matrix. Args:     get_z_coordinate (bool):         Flag that user can set if they want z-coordinates. Defaults to False.     as_CRS (Optional[pyproj.CRS]):         If given, return the points in the given CRS. If not given,         return points in the default frame (Metashape local) Returns:     Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_location(\n    self, get_z_coordinate: bool = False, as_CRS: Optional[pyproj.CRS] = None\n):\n    \"\"\"Returns a tuple of camera coordinates from the camera-to-world transformation matrix.\n    Args:\n        get_z_coordinate (bool):\n            Flag that user can set if they want z-coordinates. Defaults to False.\n        as_CRS (Optional[pyproj.CRS]):\n            If given, return the points in the given CRS. If not given,\n            return points in the default frame (Metashape local)\n    Returns:\n        Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n    \"\"\"\n\n    if as_CRS is None:\n        point = self.cam_to_world_transform[0:3, 3]\n    else:\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_CRS, as_CRS\n        )\n        cam_in_ECEF = (\n            self._local_to_epsg_4978_transform\n            @ self.cam_to_world_transform\n            @ np.array([[0, 0, 0, 1]]).T\n        )\n        point = transformer.transform(\n            xx=cam_in_ECEF[0, 0],\n            yy=cam_in_ECEF[1, 0],\n            zz=cam_in_ECEF[2, 0],\n        )\n    return tuple(point) if get_z_coordinate else tuple(point[:2])\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_properties","title":"<code>get_camera_properties()</code>","text":"<p>Returns the properties about a camera.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the focal length, principal point coordinates, image height, image width, distortion parameters, and world_to_cam_transform.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_properties(self):\n    \"\"\"Returns the properties about a camera.\n\n    Returns:\n        dict: A dictionary containing the focal length, principal point coordinates,\n            image height, image width, distortion parameters, and world_to_cam_transform.\n    \"\"\"\n    camera_properties = {\n        \"focal_length\": self.f,\n        \"principal_point_x\": self.cx,\n        \"principal_point_y\": self.cy,\n        \"image_height\": self.image_height,\n        \"image_width\": self.image_width,\n        \"distortion_params\": self.distortion_params,\n        \"world_to_cam_transform\": self.world_to_cam_transform,\n    }\n    return camera_properties\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_view_angle","title":"<code>get_camera_view_angle(in_deg=True)</code>","text":"<p>Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result</p> <p>Parameters:</p> Name Type Description Default <code>in_deg</code> <code>bool</code> <p>Return the angles in degrees rather than radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_view_angle(self, in_deg: bool = True) -&gt; tuple:\n    \"\"\"Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result\n\n    Args:\n        in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n    Returns:\n        tuple: (pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter\n    \"\"\"\n    # This is the origin, a point at one unit along the principal axis, a point one unit\n    # up (-Y), and a point one unit right (+X)\n    points_in_camera_frame = np.array(\n        [[0, 0, 0, 1], [0, 0, 1, 1], [0, -1, 0, 1], [1, 0, 0, 1]]\n    ).T\n\n    # Transform the points first into the world frame and then into the earth-centered,\n    # earth-fixed frame\n    points_in_ECEF = (\n        self._local_to_epsg_4978_transform\n        @ self.cam_to_world_transform\n        @ points_in_camera_frame\n    )\n    # Remove the homogenous coordinate and transpose\n    points_in_ECEF = points_in_ECEF[:-1].T\n    # Convert to shapely points\n    points_in_ECEF = [Point(*point) for point in points_in_ECEF]\n    # Convert to a dataframe\n    points_in_ECEF = gpd.GeoDataFrame(\n        geometry=points_in_ECEF, crs=EARTH_CENTERED_EARTH_FIXED_CRS\n    )\n\n    # Convert to lat lon\n    points_in_lat_lon = points_in_ECEF.to_crs(LAT_LON_CRS)\n    # Convert to a local projected CRS\n    points_in_projected_CRS = ensure_projected_CRS(points_in_lat_lon)\n    # Extract the geometry\n    points_in_projected_CRS = np.array(\n        [[p.x, p.y, p.z] for p in points_in_projected_CRS.geometry]\n    )\n\n    # Compute three vectors starting at the camera origin\n    view_vector = points_in_projected_CRS[1] - points_in_projected_CRS[0]\n    up_vector = points_in_projected_CRS[2] - points_in_projected_CRS[0]\n    right_vector = points_in_projected_CRS[3] - points_in_projected_CRS[0]\n\n    # The nadir vector points straight down\n    NADIR_VEC = np.array([0, 0, -1])\n\n    # For pitch, project the view vector onto the plane defined by the up vector and the nadir\n    pitch_projection_view_vec = projection_onto_plane(\n        view_vector, up_vector, NADIR_VEC\n    )\n    # For yaw, project the view vector onto the plane defined by the right vector and the nadir\n    yaw_projection_view_vec = projection_onto_plane(\n        view_vector, right_vector, NADIR_VEC\n    )\n\n    # Find the angle between these projected vectors and the nadir vector\n    pitch_angle = angle_between(pitch_projection_view_vec, NADIR_VEC)\n    yaw_angle = angle_between(yaw_projection_view_vec, NADIR_VEC)\n\n    # Return in degrees if requested\n    if in_deg:\n        return (np.rad2deg(pitch_angle), np.rad2deg(yaw_angle))\n    # Return in radians\n    return (pitch_angle, yaw_angle)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_image_size","title":"<code>get_image_size(image_scale=1.0)</code>","text":"<p>Return image size, potentially scaled</p> <p>Parameters:</p> Name Type Description Default <code>image_scale</code> <code>float</code> <p>How much to scale by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>tuple[int]: (h, w) in pixels</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_image_size(self, image_scale=1.0):\n    \"\"\"Return image size, potentially scaled\n\n    Args:\n        image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n    Returns:\n        tuple[int]: (h, w) in pixels\n    \"\"\"\n    # We should never have to deal with other cases if the reported size is accurate\n    if self.image_size is not None:\n        pass\n    elif self.image is not None:\n        self.image_size = self.image.shape[:2]\n    else:\n        image = self.get_image()\n        self.image_size = image.shape[:2]\n\n    return (\n        int(self.image_size[0] * image_scale),\n        int(self.image_size[1] * image_scale),\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_local_to_epsg_4978_transform","title":"<code>get_local_to_epsg_4978_transform()</code>","text":"<p>Return the 4x4 homogenous transform mapping from the local coordinates used for photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by EPSG:4978 (https://epsg.io/4978).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The transform in the form:    [R | t]    [0 | 1] When a homogenous vector is multiplied on the right of this matrix, it is transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse of this matrix can be used to map from EPSG:4879 to local coordinates.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_local_to_epsg_4978_transform(self) -&gt; np.ndarray:\n    \"\"\"\n    Return the 4x4 homogenous transform mapping from the local coordinates used for\n    photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by\n    EPSG:4978 (https://epsg.io/4978).\n\n    Returns:\n        np.ndarray:\n            The transform in the form:\n               [R | t]\n               [0 | 1]\n            When a homogenous vector is multiplied on the right of this matrix, it is\n            transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse\n            of this matrix can be used to map from EPSG:4879 to local coordinates.\n    \"\"\"\n    return self._local_to_epsg_4978_transform\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_lon_lat","title":"<code>get_lon_lat(negate_easting=True)</code>","text":"<p>Return the lon, lat tuple, reading from exif metadata if neccessary</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat(self, negate_easting=True):\n    \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n    if None in self.lon_lat:\n        self.lon_lat = get_GPS_exif(self.image_filename)\n\n        if negate_easting:\n            self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n    return self.lon_lat\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_pyvista_camera","title":"<code>get_pyvista_camera(focal_dist=10)</code>","text":"<p>Get a pyvista camera at the location specified by photogrammetry. Note that there is no principle point and only the vertical field of view is set</p> <p>Parameters:</p> Name Type Description Default <code>focal_dist</code> <code>float</code> <p>How far away from the camera the center point should be. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Camera</code> <p>pv.Camera: The pyvista camera from that viewpoint.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n    \"\"\"\n    Get a pyvista camera at the location specified by photogrammetry.\n    Note that there is no principle point and only the vertical field of view is set\n\n    Args:\n        focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n    Returns:\n        pv.Camera: The pyvista camera from that viewpoint.\n    \"\"\"\n    # Instantiate a new camera\n    camera = pv.Camera()\n    # Get the position as the translational part of the transform\n    camera_position = self.cam_to_world_transform[:3, 3]\n    # Get the look point by transforming a ray along the camera's Z axis into world\n    # coordinates and then adding this to the location\n    camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n        (0, 0, focal_dist)\n    )\n    # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n    camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n    # Compute the vertical field of view\n    vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n    # Set the values\n    camera.focal_point = camera_look\n    camera.position = camera_position\n    camera.up = camera_up\n    camera.view_angle = vertical_FOV_angle\n\n    return camera\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_vis_mesh","title":"<code>get_vis_mesh(frustum_scale=0.1)</code>","text":"<p>Get this camera as a mesh representation.</p> <p>Parameters:</p> Name Type Description Default <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units.</p> <code>0.1</code> <p>Returns (PolyData): blue mesh of the camera as a frustum with a     red face indicating the image top.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_vis_mesh(self, frustum_scale: float = 0.1) -&gt; pv.PolyData:\n    \"\"\"Get this camera as a mesh representation.\n\n    Args:\n        frustum_scale (float, optional): Size of cameras in world units.\n\n    Returns (PolyData): blue mesh of the camera as a frustum with a\n        red face indicating the image top.\n    \"\"\"\n\n    scaled_halfwidth = self.image_width / (self.f * 2)\n    scaled_halfheight = self.image_height / (self.f * 2)\n\n    scaled_cx = self.cx / self.f\n    scaled_cy = self.cy / self.f\n\n    right = scaled_cx + scaled_halfwidth\n    left = scaled_cx - scaled_halfwidth\n    top = scaled_cy + scaled_halfheight\n    bottom = scaled_cy - scaled_halfheight\n\n    vertices = (\n        np.array(\n            [\n                [0, 0, 0],\n                [right, top, 1],\n                [right, bottom, 1],\n                [left, bottom, 1],\n                [left, top, 1],\n            ]\n        ).T\n        * frustum_scale\n    )\n    # Make the coordinates homogenous\n    vertices = np.vstack((vertices, np.ones((1, 5))))\n\n    # Project the vertices into the world cordinates\n    projected_vertices = self.cam_to_world_transform @ vertices\n\n    # Deal with the case where there is a scale transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    ## mesh faces\n    faces = np.hstack(\n        [\n            [3, 0, 1, 2],  # side\n            [3, 0, 2, 3],  # bottom\n            [3, 0, 3, 4],  # side\n            [3, 0, 4, 1],  # top\n            [3, 1, 2, 3],  # endcap triangle #1\n            [3, 3, 4, 1],  # endcap triangle #2\n        ]\n    )\n    # All blue except the top (-Y) surface is red\n    face_colors = np.array(\n        [\n            [0, 0, 255],\n            [255, 0, 0],\n            [0, 0, 255],\n            [0, 0, 255],\n            [0, 0, 255],\n            [0, 0, 255],\n        ]\n    ).astype(np.uint8)\n\n    # Create a mesh for the camera frustum\n    frustum = pv.PolyData(projected_vertices[:3].T, faces)\n    # Unsure exactly what's going on here, but it's required for it to be valid\n    frustum.triangulate()\n\n    # Assign the face colors to the mesh\n    frustum[\"RGB\"] = pv.pyvista_ndarray(face_colors)\n\n    return frustum\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.project_mesh_verts","title":"<code>project_mesh_verts(mesh_verts, img, device)</code>","text":"<p>Get a color per vertex using only projective geometry, without considering occlusion or distortion</p> <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n    \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # [R|t] matrix\n    transform_3x4_world_to_cam = torch.Tensor(\n        self.world_to_cam_transform[:3, :]\n    ).to(device)\n    K = torch.Tensor(\n        [\n            [self.f, 0, self.image_width / 2.0 + self.cx],\n            [0, self.f, self.image_width + self.cy],\n            [0, 0, 1],\n        ],\n        device=device,\n    )\n    # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n    camera_matrix = K @ transform_3x4_world_to_cam\n\n    # Add the extra dimension of ones for matrix multiplication\n    homogenous_mesh_verts = torch.concatenate(\n        (\n            torch.Tensor(mesh_verts).to(device),\n            torch.ones((mesh_verts.shape[0], 1)).to(device),\n        ),\n        axis=1,\n    ).T\n\n    # TODO review terminology\n    homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n    # Determine what points project onto the image and at what locations\n    valid_bool, valid_locs = self.check_projected_in_image(\n        projected_verts=homogenous_camera_points,\n        image_size=(self.image_width, self.image_height),\n    )\n    # Extract corresponding colors from the image\n    colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n    return colors_per_vertex\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis","title":"<code>vis(plotter=None, frustum_scale=0.1)</code>","text":"<p>Visualize the camera as a frustum, at the appropriate translation and rotation and with the given focal length and aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>The plotter to add the visualization to</p> <code>None</code> <code>frustum_scale</code> <code>float</code> <p>The length of the frustum in world units.</p> <code>0.1</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n    \"\"\"\n    Visualize the camera as a frustum, at the appropriate translation and\n    rotation and with the given focal length and aspect ratio.\n\n    Args:\n        plotter (pv.Plotter): The plotter to add the visualization to\n        frustum_scale (float, optional): The length of the frustum in world units.\n    \"\"\"\n\n    mesh = self.get_vis_mesh(frustum_scale)\n\n    # Show the mesh with the given face colors\n    plotter.add_mesh(\n        mesh,\n        scalars=\"RGB\",\n        rgb=True,\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis_rays","title":"<code>vis_rays(pixel_coords_ij, plotter, line_length=10)</code>","text":"<p>Show rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>plotter</code> <code>Plotter</code> <p>Plotter to use.</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis_rays(\n    self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n):\n    \"\"\"Show rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        plotter (pv.Plotter): Plotter to use.\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n    \"\"\"\n    # If there are no detections, just skip it\n    if len(pixel_coords_ij) == 0:\n        return\n\n    projected_vertices = self.cast_rays(\n        pixel_coords_ij=pixel_coords_ij, line_length=line_length\n    )\n    n_points = int(projected_vertices.shape[0] / 2)\n\n    lines = np.vstack(\n        (\n            np.full(n_points, fill_value=2),\n            np.arange(0, 2 * n_points, 2),\n            np.arange(0, 2 * n_points, 2) + 1,\n        )\n    ).T\n\n    mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n    plotter.add_mesh(mesh)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet","title":"<code>PhotogrammetryCameraSet</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCameraSet:\n    def __init__(\n        self,\n        cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n        cam_to_world_transforms: Optional[List[np.ndarray]] = None,\n        intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n            0: EXAMPLE_INTRINSICS\n        },\n        image_filenames: Optional[List[PATH_TYPE]] = None,\n        lon_lats: Optional[List[Union[None, Tuple[float, float]]]] = None,\n        image_folder: Optional[PATH_TYPE] = None,\n        sensor_IDs: Optional[List[int]] = None,\n        validate_images: bool = False,\n        local_to_epsg_4978_transform: np.ndarray = np.eye(4),\n    ):\n        \"\"\"Create a camera set, representing multiple cameras in a common global coordinate frame.\n\n        Args:\n            cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n            intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n            image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n            lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n            image_folder (PATH_TYPE): The top level folder of the images\n            sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n            validate_images (bool, optional): Should the existance of the images be checked.\n                Any image_filenames that do not exist will be dropped, leaving a CameraSet only\n                containing existing images. Defaults to False.\n            local_to_epsg_4978_transform (np.ndarray):\n                A 4x4 transform mapping coordinates from the local frame of the camera set into the\n                global earth-centered, earth-fixed coordinate frame EPSG:4978.\n\n        Raises:\n            ValueError: If the number of sensor IDs is different than the number of transforms.\n        \"\"\"\n        # Record the values\n        self._local_to_epsg_4978_transform = local_to_epsg_4978_transform\n        # Save parameters used for caching distortion products\n        self._maps_ideal_to_warped = {}\n        self._maps_warped_to_ideal = {}\n\n        # Create an object using the supplied cameras\n        if cameras is not None:\n            if isinstance(cameras, PhotogrammetryCamera):\n                self.image_folder = Path(cameras.image_filename).parent\n                cameras = [cameras]\n            else:\n                self.image_folder = Path(\n                    os.path.commonpath([str(cam.image_filename) for cam in cameras])\n                )\n            self.cameras = cameras\n            return\n\n        # Standardization\n        n_transforms = len(cam_to_world_transforms)\n\n        # Create list of Nones for image filenames if not set\n        if image_filenames is None:\n            image_filenames = [None] * n_transforms\n\n        if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n            # Create a list of the only index if not set\n            sensor_IDs = [\n                list(intrinsic_params_per_sensor_type.keys())[0]\n            ] * n_transforms\n        elif len(sensor_IDs) != n_transforms:\n            raise ValueError(\n                f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n            )\n\n        # If lon lats is None, set it to a list of Nones per transform\n        if lon_lats is None:\n            lon_lats = [None] * n_transforms\n\n        if image_folder is None:\n            # TODO set it to the least common ancestor of all filenames\n            pass\n\n        # Record values for the future\n        self.cam_to_world_transforms = cam_to_world_transforms\n        self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n        self.image_filenames = image_filenames\n        self.lon_lats = lon_lats\n        self.sensor_IDs = sensor_IDs\n        self.image_folder = image_folder\n\n        if validate_images:\n            missing_images, invalid_images = self.find_missing_images()\n            if len(missing_images) &gt; 0:\n                print(f\"Deleting {len(missing_images)} missing images\")\n                valid_images = np.where(np.logical_not(invalid_images))[0]\n                self.image_filenames = np.array(self.image_filenames)[\n                    valid_images\n                ].tolist()\n                # Avoid calling .tolist() because this will recursively set all elements to lists\n                # when this should be a list of np.arrays\n                self.cam_to_world_transforms = [\n                    x for x in np.array(self.cam_to_world_transforms)[valid_images]\n                ]\n                self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n                self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n        self.cameras = []\n        for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n            self.image_filenames,\n            self.cam_to_world_transforms,\n            self.sensor_IDs,\n            self.lon_lats,\n        ):\n            sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n            # This means the sensor did not have enough parameters to be valid\n            if sensor_params is None:\n                continue\n\n            new_camera = PhotogrammetryCamera(\n                image_filename,\n                cam_to_world_transform,\n                lon_lat=lon_lat,\n                local_to_epsg_4978_transform=local_to_epsg_4978_transform,\n                **sensor_params,\n            )\n            self.cameras.append(new_camera)\n\n    def __len__(self):\n        return self.n_cameras()\n\n    def __getitem__(self, slice):\n        subset_cameras = self.cameras[slice]\n        if isinstance(subset_cameras, PhotogrammetryCamera):\n            # this is just one item indexed\n            return subset_cameras\n        # else, wrap the list of cameras in a CameraSet\n        return PhotogrammetryCameraSet(\n            subset_cameras,\n            local_to_epsg_4978_transform=self._local_to_epsg_4978_transform,\n        )\n\n    def get_image_folder(self):\n        return self.image_folder\n\n    def find_missing_images(self):\n        invalid_mask = []\n        for image_file in self.image_filenames:\n            if not image_file.is_file():\n                invalid_mask.append(True)\n            else:\n                invalid_mask.append(False)\n        invalid_images = np.array(self.image_filenames)[np.array(invalid_mask)].tolist()\n\n        return invalid_images, invalid_mask\n\n    def n_cameras(self) -&gt; int:\n        \"\"\"Return the number of cameras\"\"\"\n        return len(self.cameras)\n\n    def n_image_channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the image\"\"\"\n        return 3\n\n    def get_cameras_in_folder(self, folder: PATH_TYPE):\n        \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n        Args:\n            folder (PATH_TYPE): The folder location\n\n        Returns:\n            PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n        \"\"\"\n        # Get the inds where that camera is in the folder\n        imgs_in_folder_inds = [\n            i\n            for i in range(len(self.cameras))\n            if self.cameras[i].image_filename.is_relative_to(folder)\n        ]\n        # Return the PhotogrammetryCameraSet with those subset of cameras\n        subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n        return subset_cameras\n\n    def get_cameras_matching_filename_regex(\n        self, filename_regex: str\n    ) -&gt; \"PhotogrammetryCameraSet\":\n        \"\"\"Return the subset of cameras who's filenames match the provided regex\n\n        Args:\n            filename_regex (str): Regular expression passed to 're' engine\n\n        Returns:\n            PhotogrammetryCameraSet: Subset of cameras matching the regex\n        \"\"\"\n        # Compute boolean array for which ones match\n        imgs_matching_regex = [\n            bool(re.search(filename_regex, str(filename)))\n            for filename in self.image_filenames\n        ]\n        # Convert to integer indices within the set\n        imgs_matching_regex_inds = np.where(imgs_matching_regex)[0]\n\n        # Get the corresponding subset\n        subset_cameras = self.get_subset_cameras(imgs_matching_regex_inds)\n        return subset_cameras\n\n    def get_subset_cameras(self, inds: List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set[i] for i in inds]\n        return subset_camera_set\n\n    def get_image_by_index(self, index: int, image_scale: float = 1.0) -&gt; np.ndarray:\n        return self[index].get_image(image_scale=image_scale)\n\n    def get_camera_view_angles(self, in_deg: bool = True) -&gt; List[Tuple[float]]:\n        \"\"\"Compute the pitch and yaw off-nadir for all cameras in the set\n\n        Args:\n            in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n        Returns:\n            List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.\n        \"\"\"\n        return [\n            camera.get_camera_view_angle(in_deg=in_deg)\n            for camera in tqdm(self.cameras, desc=\"Computing view angles\")\n        ]\n\n    def get_image_filename(self, index: Union[int, None], absolute=True):\n        \"\"\"Get the image filename(s) based on the index\n\n        Args:\n            index (Union[int, None]):\n                Return the filename of the camera at this index, or all filenames if None.\n                #TODO update to support lists of integer indices as well\n            absolute (bool, optional):\n                Return the absolute filepath, as oposed to the path relative to the image folder.\n                Defaults to True.\n\n        Returns:\n            typing.Union[PATH_TYPE, list[PATH_TYPE]]:\n                If an integer index is provided, one path will be returned. If None, a list of paths\n                will be returned.\n        \"\"\"\n        if index is None:\n            return [\n                self.get_image_filename(i, absolute=absolute)\n                for i in range(len(self.cameras))\n            ]\n\n        filename = self.cameras[index].get_image_filename()\n        if absolute:\n            return Path(filename)\n        else:\n            return Path(filename).relative_to(self.get_image_folder())\n\n    def get_local_to_epsg_4978_transform(self):\n        \"\"\"\n        Return the 4x4 homogenous transform mapping from the local coordinates used for\n        photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by\n        EPSG:4978 (https://epsg.io/4978).\n\n        Returns:\n            np.ndarray:\n                The transform in the form:\n                   [R | t]\n                   [0 | 1]\n                When a homogenous vector is multiplied on the right of this matrix, it is\n                transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse\n                of this matrix can be used to map from EPSG:4879 to local coordinates.\n        \"\"\"\n        return self._local_to_epsg_4978_transform\n\n    def save_images(self, output_folder, copy=False, remove_folder: bool = True):\n        if remove_folder:\n            if os.path.isdir(output_folder):\n                print(f\"about to remove {output_folder}\")\n                shutil.rmtree(output_folder)\n\n        for i in tqdm(\n            range(len(self.cameras)),\n            f\"{'copying' if copy else 'linking'} images to {output_folder}\",\n        ):\n            output_file = Path(\n                output_folder, self.get_image_filename(i, absolute=False)\n            )\n            ensure_containing_folder(output_file)\n            src_file = self.get_image_filename(i, absolute=True)\n            if copy:\n                try:\n                    shutil.copy(src_file, output_file)\n                except FileNotFoundError:\n                    logging.warning(f\"Could not find {src_file}\")\n            else:\n                os.symlink(src_file, output_file)\n\n    def get_lon_lat_coords(self):\n        \"\"\"Returns a list of GPS coords for each camera\"\"\"\n        return [x.get_lon_lat() for x in self.cameras]\n\n    def get_camera_locations(self, **kwargs):\n        \"\"\"\n        Returns a list of camera locations for each camera.\n\n        Args:\n            **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n        Returns:\n            List[Tuple[float, float] or Tuple[float, float, float]]:\n                List of tuples containing the camera locations.\n        \"\"\"\n        return [cam.get_camera_location(**kwargs) for cam in self.cameras]\n\n    def distortion_key(\n        self, parameters: Dict[str, float], image_scale: float = 1.0\n    ) -&gt; str:\n        \"\"\"\n        Make a repeatable string key out of a distortion parameter dict\n        so that we can cache results by distortion parameters. We are\n        deciding that precision in the distortion parameters after 8\n        decimal points will be ignored in the caching process.\n\n        Arguments:\n            parameters (dict): Distortion parameters are assumed to be stored\n                \"name\": float(value)\n            image_scale (float, optional): If we want to warp a downsampled\n                version of an image, we need to track that mapping seperately\n                because downsampling affects the pixel to pixel mapping.\n                Include the image scale (0-1 float) along with the distortion\n                parameters to make a key.\n\n        Returns:\n            A repeatable string key based on the distortion values.\n        \"\"\"\n        keys = sorted(parameters.keys())\n        strings = [f\"{key}:{parameters[key]:.8f}\" for key in keys] + [\n            f\"image_scale:{image_scale:.8f}\"\n        ]\n        return \"|\".join(strings)\n\n    def make_distortion_map(\n        self,\n        camera: PhotogrammetryCamera,\n        inversion_downsample: int,\n        image_scale: float = 1.0,\n    ) -&gt; None:\n        \"\"\"\n        Cache a map connecting locations in one image to locations in another.\n        The basic construction is the pixel position of the map is the position\n        in the destination image, and the value of the map is the position in\n        the source image. Therefore if location [20, 30] has value [22.2, 28.4],\n        it means that the destination image pixel [20, 30] will be sampled from\n        the source image at pixel [22.2, 28.4] (the sampler can choose to snap\n        to the closest integer value or interpolate nearby pixels).\n\n        Arguments:\n            camera (PhotogrammetryCamera): Camera with parameters that\n                define the warp process. These include image size, principal\n                point, focal length, and distortion parameters.\n            inversion_downsample (int): Downsample the inverse map process in\n                order to reduce computation, at high res the resulting map\n                trafeoffs should be minimal.\n            image_scale: (float, optional) 0-1 fraction of the original image\n                size, used when warping/dewarping downsampled images. See\n                pix2face render_img_scale for an example.\n\n        Caches:\n            In self._maps_ideal_to_warped, stores a map of the structure\n                discussed above, keyed by self.distortion_key(params) so it can\n                be accessed for cameras using the same params.\n            In self._maps_warped_to_ideal, stores an inverted version\n        \"\"\"\n\n        # Sample over the ideal pixels, shape (H, W)\n        im_h, im_w = camera.image_size\n        if np.isclose(image_scale, 1.0):\n            h_range = np.arange(im_h)\n            w_range = np.arange(im_w)\n        else:\n            # In order to get a distortion map for a downsampled image, you\n            # need to run the ideal_to_warped equation over the same original\n            # range (0...im_h) because the radius of that range relates\n            # directly to the warping. However, do it in fewer steps (step\n            # size &gt; 1) to reflect the downsampling.\n            kwargs = {\"start\": 1 / (2 * image_scale), \"step\": 1 / image_scale}\n            h_range = np.arange(stop=im_h, **kwargs)[: int(im_h * image_scale)]\n            w_range = np.arange(stop=im_w, **kwargs)[: int(im_w * image_scale)]\n        rows, cols = np.meshgrid(h_range, w_range, indexing=\"ij\")\n\n        # Fill the (H, W) elements with the (i, j) distorted values at those locations\n        warp_cols, warp_rows = self.ideal_to_warped(camera, cols, rows)\n\n        # If dealing with downsampled images, now that we ran the warp equation,\n        # rescale the results to be within the new scale. If the original im_h\n        # is 1000 and image_scale is 0.5, we need to run ideal_to_warped on\n        # 0-1000 (for radius reasons) and then scale those results to 0-500.\n        if not np.isclose(image_scale, 1.0):\n            warp_cols *= image_scale\n            warp_rows *= image_scale\n\n        # Cache this mapping as (2, H, W)\n        dkey = self.distortion_key(camera.distortion_params, image_scale)\n        self._maps_ideal_to_warped[dkey] = np.stack([warp_rows, warp_cols], axis=0)\n        # Invert the warp map and cache as (2, H, W)\n        self._maps_warped_to_ideal[dkey] = inverse_map_interpolation(\n            self._maps_ideal_to_warped[dkey],\n            downsample=inversion_downsample,\n        )\n\n    def ideal_to_warped(\n        self, camera: PhotogrammetryCamera, xpix: np.ndarray, ypix: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Apply the given camera's distortion parameters to map pixels in an\n        ideal pinhole camera to pixel locations in the warped/distorted\n        image.\n\n        NOTE: Some standards apparently can define their distortion models\n        from warped to ideal, if we end up handling cameras like that we can\n        make the use-case more flexible.\n\n        Arguments:\n            camera (PhotogrammetryCamera): Camera with parameters that\n                define the warp process. These include image size, principal\n                point, focal length, and distortion parameters.\n            xpix (numpy array): Array of unknown shape, must match ypix\n            ypix (numpy array): Array of unknown shape, must match xpix\n\n        Returns:\n            Tuple of numpy arrays of warped x pixels and y pixels\n            [0] warped xpix\n            [1] warped ypix\n        \"\"\"\n        raise NotImplementedError(\n            f\"ideal_to_warped not implemented for {self.__class__}.\"\n        )\n\n    def warp_dewarp_image(\n        self,\n        camera: PhotogrammetryCamera,\n        input_image: np.ndarray,\n        fill_value: float = 0.0,\n        inversion_downsample: int = 8,\n        interpolation_order: int = 1,\n        warped_to_ideal: bool = True,\n        image_scale: float = 1.0,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Either apply a camera's distortion model to go from ideal\u2192warped or\n        undo the camera's distortion model to go from warped\u2192ideal, depending\n        on the warped_to_ideal flag.\n\n        Pixels in the output image that do not correspond to any input pixel\n        are set to the fill value.\n\n        Note that the warp map is cached, so the first call will take longer\n        and subsequent calls should be much faster.\n\n        Arguments:\n            camera (PhotogrammetryCamera): Camera with parameters that\n                define the warp process. These include image size, principal\n                point, focal length, and distortion parameters.\n            input_image (np.ndarray): (I, J, 3) Input image\n            fill_value (int, optional): Value to use for pixels in the\n                output image that are not mapped from the input. Defaults to 0.\n            inversion_downsample (int, optional): The distortion map creation\n                process is too heavyweight for really high-res images,\n                downsampling the inversion process gets a similar result with\n                less computation.\n            interpolation_order (int, optional):\n                The order of the interpolation. 0 is nearest neighbor and should be used for discrete\n                textures like pix2face masks. 1 can be used for data representing continious\n                quantities. Defaults to 1.\n            warped_to_ideal (bool, optional): If true, take in a warped image\n                and return an undistorted (dewarped/ideal) image. If false,\n                take in an undistorted image and return a warped image.\n            image_scale: (float, optional) 0-1 fraction of the original image\n                size, used when warping/dewarping downsampled images. See\n                pix2face render_img_scale for an example.\n\n        Returns:\n            np.ndarray: (I, J, 3) output image\n        \"\"\"\n\n        # Ensure that there is a cached map for these distortion parameters\n        dkey = self.distortion_key(camera.distortion_params, image_scale)\n        if dkey not in self._maps_ideal_to_warped:\n            self.make_distortion_map(camera, inversion_downsample, image_scale)\n\n        if warped_to_ideal:\n            inverse_map = self._maps_ideal_to_warped[dkey]\n        else:\n            inverse_map = self._maps_warped_to_ideal[dkey]\n\n        warped_image = flexible_inputs_warp(\n            input_image=input_image,\n            inverse_map=inverse_map,\n            interpolation_order=interpolation_order,\n            fill_value=fill_value,\n        )\n\n        return warped_image\n\n    def warp_dewarp_pixels(\n        self,\n        camera: PhotogrammetryCamera,\n        pixels: np.ndarray,\n        inversion_downsample: int = 8,\n        warped_to_ideal: bool = True,\n    ):\n        \"\"\"\n        Either apply a camera's distortion model to go from ideal pixels\u2192warped\n        or undo the camera's distortion model to go from warped pixels\u2192ideal,\n        depending on the warped_to_ideal flag.\n\n        Note that the warp map is cached, so the first call will take longer\n        and subsequent calls should be much faster.\n\n        Arguments:\n            camera (PhotogrammetryCamera): Camera with parameters that\n                define the warp process. These include image size, principal\n                point, focal length, and distortion parameters.\n            pixels (np.ndarray): (N, 2) Pixels locations in (i, j) format.\n            inversion_downsample (int, optional): The distortion map creation\n                process is too heavyweight for really high-res images,\n                downsampling the inversion process gets a similar result with\n                less computation.\n            warped_to_ideal (bool, optional): If true, take in a warped image\n                and return an undistorted (dewarped/ideal) image. If false,\n                take in an undistorted image and return a warped image.\n\n        Returns:\n            np.ndarray: (N, 2) warped/dewarped output pixel locations (i, j)\n                Note that the output is floating point (subpixel)\n        \"\"\"\n\n        # Ensure that there is a cached map for these distortion parameters\n        dkey = self.distortion_key(camera.distortion_params)\n        if dkey not in self._maps_ideal_to_warped:\n            self.make_distortion_map(camera, inversion_downsample)\n\n        # Get the right mapping array\n        if warped_to_ideal:\n            rowmap, colmap = self._maps_warped_to_ideal[dkey]\n        else:\n            rowmap, colmap = self._maps_ideal_to_warped[dkey]\n\n        # Look up the pixel locations\n        rows = rowmap[pixels[:, 0], pixels[:, 1]]\n        cols = colmap[pixels[:, 0], pixels[:, 1]]\n        return np.stack([rows, cols], axis=0).T\n\n    def get_subset_ROI(\n        self,\n        ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n        buffer_radius: float = 0,\n        is_geospatial: bool = None,\n    ):\n        \"\"\"Return cameras that are within a radius of the provided geometry\n\n        Args:\n            geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n                This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n                Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n            buffer_radius (float, optional):\n                Return points within this buffer of the geometry. Defaults to 0. Represents\n                meters if ROI is geospatial.\n            is_geospatial (bool, optional):\n                A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n                the flag is set if the provided geodata has a CRS.\n        Returns:\n            subset_camera_set (List[PhotogrammetryCamera]):\n                List of cameras that fall within the provided ROI\n        \"\"\"\n        # construct GeoDataFrame if not provided\n        if isinstance(ROI, (Polygon, MultiPolygon)):\n            # assume geodata is lat/lon if is_geospatial is True\n            if is_geospatial:\n                ROI = gpd.GeoDataFrame(crs=LAT_LON_CRS, geometry=[ROI])\n            else:\n                ROI = gpd.GeoDataFrame(geometry=[ROI])\n        elif not isinstance(ROI, gpd.GeoDataFrame):\n            # Read in the geofile\n            ROI = gpd.read_file(ROI)\n\n        if is_geospatial is None:\n            is_geospatial = ROI.crs is not None\n\n        if not is_geospatial:\n            # get internal coordinate system camera locations\n            image_locations = [Point(*x) for x in self.get_camera_locations()]\n            image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n        else:\n            # Make sure it's a geometric (meters-based) CRS\n            ROI = ensure_projected_CRS(ROI)\n            # Read the locations of all the points\n            image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n            # Create a dataframe, assuming inputs are lat lon\n            image_locations_df = gpd.GeoDataFrame(\n                geometry=image_locations, crs=LAT_LON_CRS\n            )\n            image_locations_df.to_crs(ROI.crs, inplace=True)\n\n        # Drop all the fields except the geometry for computational reasons\n        ROI = ROI[\"geometry\"]\n        # Buffer out by the requested distance\n        ROI = ROI.buffer(buffer_radius)\n        # Merge the potentially-individual polygons to one\n        # TODO Do experiments to see if this step should be before or after the buffer.\n        # Counterintuitively, it seems that after is faster\n        ROI = unary_union(ROI.tolist())\n\n        # Determine the binary mask for which cameras are within the ROI\n        cameras_in_ROI = image_locations_df.within(ROI).to_numpy()\n        # Convert to the integer indices\n        cameras_in_ROI = np.where(cameras_in_ROI)[0]\n        # Get the corresponding subset of cameras\n        subset_camera_set = self.get_subset_cameras(cameras_in_ROI)\n        return subset_camera_set\n\n    def triangulate_detections(\n        self,\n        detector: Union[RegionDetectionSegmentor, TabularRectangleSegmentor],\n        ray_length_meters: float = 1e3,\n        boundaries: Optional[Tuple[pv.PolyData, pv.PolyData]] = None,\n        limit_ray_length_meters: Optional[float] = None,\n        limit_angle_from_vert: Optional[float] = None,\n        similarity_threshold_meters: float = 0.1,\n        transform: Optional[Callable[[np.ndarray], np.ndarray]] = None,\n        louvain_resolution: float = 1.0,\n        out_dir: Optional[PATH_TYPE] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Take per-image detections and triangulate them to 3D locations.\n\n        Args:\n            detector (Union[RegionDetectionSegmentor, TabularRectangleSegmentor]):\n                Produces detections per image using the get_detection_centers method.\n            ray_length_meters (float, optional):\n                The length of the visualized rays in meters. Defaults to 1000.\n            boundaries (Optional[Tuple[pv.PolyData, pv.PolyData]])\n                Defaults to None.\n            limit_ray_length_meters (Optional[float])\n                Defaults to None.\n            limit_angle_from_vert (Optional[float])\n                Defaults to None.\n            similarity_threshold_meters (float, optional):\n                Consider rays a potential match if the distance between them is less than this\n                value. Defaults to 0.1.\n            transform (Optional[Callable[[np.ndarray], np.ndarray]]):\n                Defaults to None.\n            louvain_resolution (float, optional):\n                The resolution hyperparameter of the networkx.louvain_communities function.\n                Height value leads to more communities. Defaults to 1.0.\n            out_dir: Optional[PATH_TYPE]\n                Defaults to None.\n\n        Returns (np.ndarray):\n            (N unique objects, 3), the 3D locations of the identified objects.\n            If transform_to_epsg_4978 is None, then this is in the local coordinate\n            system of the mesh. If the transform is not None, (lat, lon, alt) is returned.\n        \"\"\"\n\n        # Enforce Path type\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n\n        def check_exists(file: Union[str, Path]):\n            \"\"\"Helper function to aid in caching and loading cached files.\"\"\"\n            if out_dir is None:\n                return False\n            if isinstance(file, str):\n                path = out_dir / file\n            else:\n                path = file\n            return path.is_file()\n\n        # Determine scale factor relating meters to internal coordinates\n        transform_to_epsg_4978 = self.get_local_to_epsg_4978_transform()\n        meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n        ray_length_local = ray_length_meters * meters_to_local_scale\n        similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n        if limit_ray_length_meters is None:\n            limit_ray_length_local = None\n        else:\n            limit_ray_length_local = limit_ray_length_meters * meters_to_local_scale\n\n        # Create line segments emanating from the cameras\n        if check_exists(\"line_segments.npz\"):\n            line_results = out_dir / \"line_segments.npz\"\n        else:\n            line_results = self.calc_line_segments(\n                detector=detector,\n                boundaries=boundaries,\n                ray_length_local=ray_length_local,\n                out_dir=out_dir,\n                limit_ray_length_local=limit_ray_length_local,\n                limit_angle_from_vert=limit_angle_from_vert,\n            )\n        # Load the results into memory if they were saved to file\n        if check_exists(line_results):\n            line_results = np.load(line_results)\n\n        # Turn line segments into graph distances, where \"close enough\"\n        # lines are connected nodes in the graph.\n        if check_exists(\"edge_weights.json\"):\n            weight_results = out_dir / \"edge_weights.json\"\n        else:\n            weight_results = calc_graph_weights(\n                starts=line_results[\"ray_starts\"],\n                ends=line_results[\"ray_ends\"],\n                ray_IDs=line_results[\"ray_IDs\"],\n                similarity_threshold=similarity_threshold_local,\n                out_dir=out_dir,\n                step=5000,\n                transform=transform,\n            )\n        # Load the results into memory if they were saved to file\n        if check_exists(weight_results):\n            weight_results = json.load(weight_results.open(\"r\"))\n\n        # Calculate community identities among the graph weights, where\n        # hopefully a preponderance of close line segments indicate a\n        # single object detected multiple times.\n        if check_exists(\"communities.npz\"):\n            community_results = out_dir / \"communities.npz\"\n        else:\n            community_results = calc_communities(\n                starts=line_results[\"ray_starts\"],\n                ends=line_results[\"ray_ends\"],\n                edge_weights=weight_results,\n                louvain_resolution=louvain_resolution,\n                out_dir=out_dir,\n                transform_to_epsg_4978=transform_to_epsg_4978,\n            )\n        # Load the results into memory if they were saved to file\n        if check_exists(community_results):\n            community_results = np.load(community_results)\n\n        # Return the 3D locations of the community points, preferentially\n        # in lat/lon form if it exists.\n        return community_results.get(\n            \"community_points_latlon\",\n            community_results[\"community_points\"],\n        )\n\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        add_orientation_cube: bool = False,\n        show: bool = False,\n        frustum_scale: float = None,\n        force_xvfb: bool = False,\n        interactive_jupyter: bool = False,\n    ):\n        \"\"\"Visualize all the cameras\n\n        Args:\n            plotter (pv.Plotter):\n                Plotter to add the cameras to. If None, will be created and then plotted\n            add_orientation_cube (bool, optional):\n                Add a cube to visualize the coordinate system. Defaults to False.\n            show (bool, optional):\n                Show the results instead of waiting for other content to be added\n            frustum_scale (float, optional):\n                Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n                between two cameras.\n            force_xvfb (bool, optional):\n                Force a headless rendering backend\n            interactive_jupyter (bool, optional):\n                Will allow you to interact with the visualization in your notebook if supported by\n                the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n                to False.\n\n        \"\"\"\n\n        if plotter is None:\n            plotter = pv.Plotter()\n            show = True\n\n        # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n        if frustum_scale is None:\n            if self.n_cameras() &gt;= 2:\n                camera_translation_matrices = np.array(\n                    [transform[:3, 3] for transform in self.cam_to_world_transforms]\n                )\n                distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n                max_distance = np.max(distances)\n                frustum_scale = (\n                    (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n                )\n            # else, set it to a default\n            else:\n                frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n        for camera in self.cameras:\n            camera.vis(plotter, frustum_scale=frustum_scale)\n        if add_orientation_cube:\n            # TODO Consider adding to a freestanding vis module\n            ocube = demos.orientation_cube()\n            plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n            plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n            plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n            plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n            plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n            plotter.show_axes()\n\n        if show:\n            if force_xvfb:\n                safe_start_xvfb()\n            plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n\n    def get_vis_mesh(self, frustum_scale: float = 0.1) -&gt; pv.PolyData:\n        \"\"\"Get all the cameras as a mesh representation.\n\n        Args:\n            frustum_scale (float, optional): Size of cameras in world units.\n\n        Returns: (PolyData) mesh representation of all cameras as frustums\n        \"\"\"\n        return pv.merge(\n            [\n                camera.get_vis_mesh(frustum_scale=frustum_scale)\n                for camera in self.cameras\n            ]\n        )\n\n    def calc_line_segments(\n        self,\n        detector: Union[RegionDetectionSegmentor, TabularRectangleSegmentor],\n        boundaries: Optional[Tuple[pv.PolyData, pv.PolyData]] = None,\n        ray_length_local: float = 1e3,\n        out_dir: Optional[PATH_TYPE] = None,\n        limit_ray_length_local: Optional[float] = None,\n        limit_angle_from_vert: Optional[float] = None,\n    ) -&gt; Union[Dict[str, np.ndarray], Path]:\n        \"\"\"\n        For each camera in the set, this method:\n        1. Gets detection centers from the provided detector\n        2. Projects rays from the camera through these detection points\n        3. Optionally filters rays by angle from vertical\n        4. Optionally clips rays to intersection with boundary surfaces\n        5. Returns resulting line segments or saves them to file\n\n        Args:\n            detector: Detector that provides detection centers for each image\n            boundaries: Optional tuple of (upper, lower) boundary surfaces as PyVista PolyData.\n                If provided, rays will be clipped to these surfaces.\n            ray_length_local: Length of the initial rays in local coordinate units. Default 1e3.\n            out_dir: Directory to save the output NPZ file containing ray data. If this is None,\n                the data will be returned as a dict. If this is a path, \"line_segments.npz\"\n                will be saved in that directory. Default is None.\n            limit_ray_length_local: Optional max ray length from origin to second boundary.\n                This is to mimic measuring from a camera (hypothetical ray source) to\n                the ground (assuming the boundaries are given as [ceiling, floor]).\n                Default is None, meaning no limit.\n            limit_angle_from_vert: Optional max angle (in radians) from vertical.\n                Default is None, meaning no limit.\n\n        Returns:\n            If out_dir is None, returns a dictionary with:\n                - \"ray_starts\": (N, 3) array of ray start points\n                - \"ray_ends\": (N, 3) array of ray end points\n                - \"ray_IDs\": (N,) array of camera indices that generated each ray\n            If out_dir is provided, saves this data to \"line_segments.npz\" in that directory\n        \"\"\"\n\n        # Record the lines corresponding to each detection and the associated image ID\n        all_line_segments = []\n        all_image_IDs = []\n\n        # Iterate over the cameras\n        for camera_ind in tqdm(\n            range(len(self.cameras)), desc=\"Building line segments per camera\"\n        ):\n            # Get the image filename\n            image_filename = str(self.get_image_filename(camera_ind))\n            # Get the centers of associated detection from the detector\n            # TODO, this only works with \"detectors\" that can look up the detections based on the\n            # filename alone. In the future we might want to support real detectors that actually\n            # use the image.\n            detection_centers_pixels = detector.get_detection_centers(image_filename)\n            # Project rays given the locations of the detections in pixel coordinates\n            if len(detection_centers_pixels) &gt; 0:\n                # Record the line segments, which will be ordered as alternating (start, end) rows\n                line_segments = self.cameras[camera_ind].cast_rays(\n                    pixel_coords_ij=detection_centers_pixels,\n                    line_length=ray_length_local,\n                )\n                all_line_segments.append(line_segments)\n                # Record which image ID generated each line\n                all_image_IDs.append(\n                    np.full(\n                        int(line_segments.shape[0] / 2),\n                        fill_value=camera_ind,\n                    )\n                )\n\n        if len(all_line_segments) &gt; 0:\n            # Concatenate the lists of arrays into a single array\n            all_line_segments = np.concatenate(all_line_segments, axis=0)\n            all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n            # Get the starts and ends, which are alternating rows\n            ray_starts = all_line_segments[0::2]\n            ray_ends = all_line_segments[1::2]\n            # Determine the direction\n            ray_directions = ray_ends - ray_starts\n            # Make the ray directions unit length\n            ray_directions = ray_directions / np.linalg.norm(\n                ray_directions, axis=1, keepdims=True\n            )\n\n            # Filter by angle from vertical if requested\n            if limit_angle_from_vert is not None:\n                # Angle from vertical (z-axis): arccos(|z component of unit vector|)\n                z_axis = ray_directions[:, 2]\n                angles = np.arccos(np.abs(z_axis))\n                keep_mask = angles &lt;= limit_angle_from_vert\n                ray_starts = ray_starts[keep_mask]\n                ray_ends = ray_ends[keep_mask]\n                ray_directions = ray_directions[keep_mask]\n                all_image_IDs = all_image_IDs[keep_mask]\n\n            if boundaries is not None:\n                print(\"Clipping all line segments to boundary surfaces\")\n                ray_starts, ray_ends, ray_directions, all_image_IDs = (\n                    clip_line_segments(\n                        boundaries=boundaries,\n                        origins=ray_starts,\n                        directions=ray_directions,\n                        image_indices=all_image_IDs,\n                        ray_limit=limit_ray_length_local,\n                    )\n                )\n\n        else:\n            ray_starts = np.empty((0, 3))\n            ray_ends = np.empty((0, 3))\n            all_image_IDs = np.empty((0,), dtype=int)\n\n        # Return or save to file\n        data = {\n            \"ray_starts\": ray_starts,\n            \"ray_ends\": ray_ends,\n            \"ray_IDs\": all_image_IDs,\n        }\n        if out_dir is None:\n            return data\n        else:\n            path = Path(out_dir) / \"line_segments.npz\"\n            np.savez(path, **data)\n            return path\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.__init__","title":"<code>__init__(cameras=None, cam_to_world_transforms=None, intrinsic_params_per_sensor_type={0: EXAMPLE_INTRINSICS}, image_filenames=None, lon_lats=None, image_folder=None, sensor_IDs=None, validate_images=False, local_to_epsg_4978_transform=np.eye(4))</code>","text":"<p>Create a camera set, representing multiple cameras in a common global coordinate frame.</p> <p>Parameters:</p> Name Type Description Default <code>cam_to_world_transforms</code> <code>List[ndarray]</code> <p>The list of 4x4 camera to world transforms</p> <code>None</code> <code>intrinsic_params_per_sensor</code> <code>Dict[int, Dict]</code> <p>A dictionary mapping from an int camera ID to the intrinsic parameters</p> required <code>image_filenames</code> <code>List[PATH_TYPE]</code> <p>The list of image filenames, ideally absolute paths</p> <code>None</code> <code>lon_lats</code> <code>Union[None, List[Union[None, Tuple[float, float]]]]</code> <p>A list of lon,lat tuples, or list of Nones, or None</p> <code>None</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>The top level folder of the images</p> <code>None</code> <code>sensor_IDs</code> <code>List[int]</code> <p>The list of sensor IDs, that index into the sensors_params_dict</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Should the existance of the images be checked. Any image_filenames that do not exist will be dropped, leaving a CameraSet only containing existing images. Defaults to False.</p> <code>False</code> <code>local_to_epsg_4978_transform</code> <code>ndarray</code> <p>A 4x4 transform mapping coordinates from the local frame of the camera set into the global earth-centered, earth-fixed coordinate frame EPSG:4978.</p> <code>eye(4)</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of sensor IDs is different than the number of transforms.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n    cam_to_world_transforms: Optional[List[np.ndarray]] = None,\n    intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n        0: EXAMPLE_INTRINSICS\n    },\n    image_filenames: Optional[List[PATH_TYPE]] = None,\n    lon_lats: Optional[List[Union[None, Tuple[float, float]]]] = None,\n    image_folder: Optional[PATH_TYPE] = None,\n    sensor_IDs: Optional[List[int]] = None,\n    validate_images: bool = False,\n    local_to_epsg_4978_transform: np.ndarray = np.eye(4),\n):\n    \"\"\"Create a camera set, representing multiple cameras in a common global coordinate frame.\n\n    Args:\n        cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n        intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n        image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n        lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n        image_folder (PATH_TYPE): The top level folder of the images\n        sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n        validate_images (bool, optional): Should the existance of the images be checked.\n            Any image_filenames that do not exist will be dropped, leaving a CameraSet only\n            containing existing images. Defaults to False.\n        local_to_epsg_4978_transform (np.ndarray):\n            A 4x4 transform mapping coordinates from the local frame of the camera set into the\n            global earth-centered, earth-fixed coordinate frame EPSG:4978.\n\n    Raises:\n        ValueError: If the number of sensor IDs is different than the number of transforms.\n    \"\"\"\n    # Record the values\n    self._local_to_epsg_4978_transform = local_to_epsg_4978_transform\n    # Save parameters used for caching distortion products\n    self._maps_ideal_to_warped = {}\n    self._maps_warped_to_ideal = {}\n\n    # Create an object using the supplied cameras\n    if cameras is not None:\n        if isinstance(cameras, PhotogrammetryCamera):\n            self.image_folder = Path(cameras.image_filename).parent\n            cameras = [cameras]\n        else:\n            self.image_folder = Path(\n                os.path.commonpath([str(cam.image_filename) for cam in cameras])\n            )\n        self.cameras = cameras\n        return\n\n    # Standardization\n    n_transforms = len(cam_to_world_transforms)\n\n    # Create list of Nones for image filenames if not set\n    if image_filenames is None:\n        image_filenames = [None] * n_transforms\n\n    if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n        # Create a list of the only index if not set\n        sensor_IDs = [\n            list(intrinsic_params_per_sensor_type.keys())[0]\n        ] * n_transforms\n    elif len(sensor_IDs) != n_transforms:\n        raise ValueError(\n            f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n        )\n\n    # If lon lats is None, set it to a list of Nones per transform\n    if lon_lats is None:\n        lon_lats = [None] * n_transforms\n\n    if image_folder is None:\n        # TODO set it to the least common ancestor of all filenames\n        pass\n\n    # Record values for the future\n    self.cam_to_world_transforms = cam_to_world_transforms\n    self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n    self.image_filenames = image_filenames\n    self.lon_lats = lon_lats\n    self.sensor_IDs = sensor_IDs\n    self.image_folder = image_folder\n\n    if validate_images:\n        missing_images, invalid_images = self.find_missing_images()\n        if len(missing_images) &gt; 0:\n            print(f\"Deleting {len(missing_images)} missing images\")\n            valid_images = np.where(np.logical_not(invalid_images))[0]\n            self.image_filenames = np.array(self.image_filenames)[\n                valid_images\n            ].tolist()\n            # Avoid calling .tolist() because this will recursively set all elements to lists\n            # when this should be a list of np.arrays\n            self.cam_to_world_transforms = [\n                x for x in np.array(self.cam_to_world_transforms)[valid_images]\n            ]\n            self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n            self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n    self.cameras = []\n    for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n        self.image_filenames,\n        self.cam_to_world_transforms,\n        self.sensor_IDs,\n        self.lon_lats,\n    ):\n        sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n        # This means the sensor did not have enough parameters to be valid\n        if sensor_params is None:\n            continue\n\n        new_camera = PhotogrammetryCamera(\n            image_filename,\n            cam_to_world_transform,\n            lon_lat=lon_lat,\n            local_to_epsg_4978_transform=local_to_epsg_4978_transform,\n            **sensor_params,\n        )\n        self.cameras.append(new_camera)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.calc_line_segments","title":"<code>calc_line_segments(detector, boundaries=None, ray_length_local=1000.0, out_dir=None, limit_ray_length_local=None, limit_angle_from_vert=None)</code>","text":"<p>For each camera in the set, this method: 1. Gets detection centers from the provided detector 2. Projects rays from the camera through these detection points 3. Optionally filters rays by angle from vertical 4. Optionally clips rays to intersection with boundary surfaces 5. Returns resulting line segments or saves them to file</p> <p>Parameters:</p> Name Type Description Default <code>detector</code> <code>Union[RegionDetectionSegmentor, TabularRectangleSegmentor]</code> <p>Detector that provides detection centers for each image</p> required <code>boundaries</code> <code>Optional[Tuple[PolyData, PolyData]]</code> <p>Optional tuple of (upper, lower) boundary surfaces as PyVista PolyData. If provided, rays will be clipped to these surfaces.</p> <code>None</code> <code>ray_length_local</code> <code>float</code> <p>Length of the initial rays in local coordinate units. Default 1e3.</p> <code>1000.0</code> <code>out_dir</code> <code>Optional[PATH_TYPE]</code> <p>Directory to save the output NPZ file containing ray data. If this is None, the data will be returned as a dict. If this is a path, \"line_segments.npz\" will be saved in that directory. Default is None.</p> <code>None</code> <code>limit_ray_length_local</code> <code>Optional[float]</code> <p>Optional max ray length from origin to second boundary. This is to mimic measuring from a camera (hypothetical ray source) to the ground (assuming the boundaries are given as [ceiling, floor]). Default is None, meaning no limit.</p> <code>None</code> <code>limit_angle_from_vert</code> <code>Optional[float]</code> <p>Optional max angle (in radians) from vertical. Default is None, meaning no limit.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Dict[str, ndarray], Path]</code> <p>If out_dir is None, returns a dictionary with: - \"ray_starts\": (N, 3) array of ray start points - \"ray_ends\": (N, 3) array of ray end points - \"ray_IDs\": (N,) array of camera indices that generated each ray</p> <code>Union[Dict[str, ndarray], Path]</code> <p>If out_dir is provided, saves this data to \"line_segments.npz\" in that directory</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def calc_line_segments(\n    self,\n    detector: Union[RegionDetectionSegmentor, TabularRectangleSegmentor],\n    boundaries: Optional[Tuple[pv.PolyData, pv.PolyData]] = None,\n    ray_length_local: float = 1e3,\n    out_dir: Optional[PATH_TYPE] = None,\n    limit_ray_length_local: Optional[float] = None,\n    limit_angle_from_vert: Optional[float] = None,\n) -&gt; Union[Dict[str, np.ndarray], Path]:\n    \"\"\"\n    For each camera in the set, this method:\n    1. Gets detection centers from the provided detector\n    2. Projects rays from the camera through these detection points\n    3. Optionally filters rays by angle from vertical\n    4. Optionally clips rays to intersection with boundary surfaces\n    5. Returns resulting line segments or saves them to file\n\n    Args:\n        detector: Detector that provides detection centers for each image\n        boundaries: Optional tuple of (upper, lower) boundary surfaces as PyVista PolyData.\n            If provided, rays will be clipped to these surfaces.\n        ray_length_local: Length of the initial rays in local coordinate units. Default 1e3.\n        out_dir: Directory to save the output NPZ file containing ray data. If this is None,\n            the data will be returned as a dict. If this is a path, \"line_segments.npz\"\n            will be saved in that directory. Default is None.\n        limit_ray_length_local: Optional max ray length from origin to second boundary.\n            This is to mimic measuring from a camera (hypothetical ray source) to\n            the ground (assuming the boundaries are given as [ceiling, floor]).\n            Default is None, meaning no limit.\n        limit_angle_from_vert: Optional max angle (in radians) from vertical.\n            Default is None, meaning no limit.\n\n    Returns:\n        If out_dir is None, returns a dictionary with:\n            - \"ray_starts\": (N, 3) array of ray start points\n            - \"ray_ends\": (N, 3) array of ray end points\n            - \"ray_IDs\": (N,) array of camera indices that generated each ray\n        If out_dir is provided, saves this data to \"line_segments.npz\" in that directory\n    \"\"\"\n\n    # Record the lines corresponding to each detection and the associated image ID\n    all_line_segments = []\n    all_image_IDs = []\n\n    # Iterate over the cameras\n    for camera_ind in tqdm(\n        range(len(self.cameras)), desc=\"Building line segments per camera\"\n    ):\n        # Get the image filename\n        image_filename = str(self.get_image_filename(camera_ind))\n        # Get the centers of associated detection from the detector\n        # TODO, this only works with \"detectors\" that can look up the detections based on the\n        # filename alone. In the future we might want to support real detectors that actually\n        # use the image.\n        detection_centers_pixels = detector.get_detection_centers(image_filename)\n        # Project rays given the locations of the detections in pixel coordinates\n        if len(detection_centers_pixels) &gt; 0:\n            # Record the line segments, which will be ordered as alternating (start, end) rows\n            line_segments = self.cameras[camera_ind].cast_rays(\n                pixel_coords_ij=detection_centers_pixels,\n                line_length=ray_length_local,\n            )\n            all_line_segments.append(line_segments)\n            # Record which image ID generated each line\n            all_image_IDs.append(\n                np.full(\n                    int(line_segments.shape[0] / 2),\n                    fill_value=camera_ind,\n                )\n            )\n\n    if len(all_line_segments) &gt; 0:\n        # Concatenate the lists of arrays into a single array\n        all_line_segments = np.concatenate(all_line_segments, axis=0)\n        all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n        # Get the starts and ends, which are alternating rows\n        ray_starts = all_line_segments[0::2]\n        ray_ends = all_line_segments[1::2]\n        # Determine the direction\n        ray_directions = ray_ends - ray_starts\n        # Make the ray directions unit length\n        ray_directions = ray_directions / np.linalg.norm(\n            ray_directions, axis=1, keepdims=True\n        )\n\n        # Filter by angle from vertical if requested\n        if limit_angle_from_vert is not None:\n            # Angle from vertical (z-axis): arccos(|z component of unit vector|)\n            z_axis = ray_directions[:, 2]\n            angles = np.arccos(np.abs(z_axis))\n            keep_mask = angles &lt;= limit_angle_from_vert\n            ray_starts = ray_starts[keep_mask]\n            ray_ends = ray_ends[keep_mask]\n            ray_directions = ray_directions[keep_mask]\n            all_image_IDs = all_image_IDs[keep_mask]\n\n        if boundaries is not None:\n            print(\"Clipping all line segments to boundary surfaces\")\n            ray_starts, ray_ends, ray_directions, all_image_IDs = (\n                clip_line_segments(\n                    boundaries=boundaries,\n                    origins=ray_starts,\n                    directions=ray_directions,\n                    image_indices=all_image_IDs,\n                    ray_limit=limit_ray_length_local,\n                )\n            )\n\n    else:\n        ray_starts = np.empty((0, 3))\n        ray_ends = np.empty((0, 3))\n        all_image_IDs = np.empty((0,), dtype=int)\n\n    # Return or save to file\n    data = {\n        \"ray_starts\": ray_starts,\n        \"ray_ends\": ray_ends,\n        \"ray_IDs\": all_image_IDs,\n    }\n    if out_dir is None:\n        return data\n    else:\n        path = Path(out_dir) / \"line_segments.npz\"\n        np.savez(path, **data)\n        return path\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.distortion_key","title":"<code>distortion_key(parameters, image_scale=1.0)</code>","text":"<p>Make a repeatable string key out of a distortion parameter dict so that we can cache results by distortion parameters. We are deciding that precision in the distortion parameters after 8 decimal points will be ignored in the caching process.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>dict</code> <p>Distortion parameters are assumed to be stored \"name\": float(value)</p> required <code>image_scale</code> <code>float</code> <p>If we want to warp a downsampled version of an image, we need to track that mapping seperately because downsampling affects the pixel to pixel mapping. Include the image scale (0-1 float) along with the distortion parameters to make a key.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>str</code> <p>A repeatable string key based on the distortion values.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def distortion_key(\n    self, parameters: Dict[str, float], image_scale: float = 1.0\n) -&gt; str:\n    \"\"\"\n    Make a repeatable string key out of a distortion parameter dict\n    so that we can cache results by distortion parameters. We are\n    deciding that precision in the distortion parameters after 8\n    decimal points will be ignored in the caching process.\n\n    Arguments:\n        parameters (dict): Distortion parameters are assumed to be stored\n            \"name\": float(value)\n        image_scale (float, optional): If we want to warp a downsampled\n            version of an image, we need to track that mapping seperately\n            because downsampling affects the pixel to pixel mapping.\n            Include the image scale (0-1 float) along with the distortion\n            parameters to make a key.\n\n    Returns:\n        A repeatable string key based on the distortion values.\n    \"\"\"\n    keys = sorted(parameters.keys())\n    strings = [f\"{key}:{parameters[key]:.8f}\" for key in keys] + [\n        f\"image_scale:{image_scale:.8f}\"\n    ]\n    return \"|\".join(strings)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_camera_locations","title":"<code>get_camera_locations(**kwargs)</code>","text":"<p>Returns a list of camera locations for each camera.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>List[Tuple[float, float] or Tuple[float, float, float]]: List of tuples containing the camera locations.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_locations(self, **kwargs):\n    \"\"\"\n    Returns a list of camera locations for each camera.\n\n    Args:\n        **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n    Returns:\n        List[Tuple[float, float] or Tuple[float, float, float]]:\n            List of tuples containing the camera locations.\n    \"\"\"\n    return [cam.get_camera_location(**kwargs) for cam in self.cameras]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_camera_view_angles","title":"<code>get_camera_view_angles(in_deg=True)</code>","text":"<p>Compute the pitch and yaw off-nadir for all cameras in the set</p> <p>Parameters:</p> Name Type Description Default <code>in_deg</code> <code>bool</code> <p>Return the angles in degrees rather than radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[float]]</code> <p>List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_view_angles(self, in_deg: bool = True) -&gt; List[Tuple[float]]:\n    \"\"\"Compute the pitch and yaw off-nadir for all cameras in the set\n\n    Args:\n        in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n    Returns:\n        List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.\n    \"\"\"\n    return [\n        camera.get_camera_view_angle(in_deg=in_deg)\n        for camera in tqdm(self.cameras, desc=\"Computing view angles\")\n    ]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_cameras_in_folder","title":"<code>get_cameras_in_folder(folder)</code>","text":"<p>Return the camera set with cameras corresponding to images in that folder</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PATH_TYPE</code> <p>The folder location</p> required <p>Returns:</p> Name Type Description <code>PhotogrammetryCameraSet</code> <p>A copy of the camera set with only the cameras from that folder</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_cameras_in_folder(self, folder: PATH_TYPE):\n    \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n    Args:\n        folder (PATH_TYPE): The folder location\n\n    Returns:\n        PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n    \"\"\"\n    # Get the inds where that camera is in the folder\n    imgs_in_folder_inds = [\n        i\n        for i in range(len(self.cameras))\n        if self.cameras[i].image_filename.is_relative_to(folder)\n    ]\n    # Return the PhotogrammetryCameraSet with those subset of cameras\n    subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n    return subset_cameras\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_cameras_matching_filename_regex","title":"<code>get_cameras_matching_filename_regex(filename_regex)</code>","text":"<p>Return the subset of cameras who's filenames match the provided regex</p> <p>Parameters:</p> Name Type Description Default <code>filename_regex</code> <code>str</code> <p>Regular expression passed to 're' engine</p> required <p>Returns:</p> Name Type Description <code>PhotogrammetryCameraSet</code> <code>PhotogrammetryCameraSet</code> <p>Subset of cameras matching the regex</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_cameras_matching_filename_regex(\n    self, filename_regex: str\n) -&gt; \"PhotogrammetryCameraSet\":\n    \"\"\"Return the subset of cameras who's filenames match the provided regex\n\n    Args:\n        filename_regex (str): Regular expression passed to 're' engine\n\n    Returns:\n        PhotogrammetryCameraSet: Subset of cameras matching the regex\n    \"\"\"\n    # Compute boolean array for which ones match\n    imgs_matching_regex = [\n        bool(re.search(filename_regex, str(filename)))\n        for filename in self.image_filenames\n    ]\n    # Convert to integer indices within the set\n    imgs_matching_regex_inds = np.where(imgs_matching_regex)[0]\n\n    # Get the corresponding subset\n    subset_cameras = self.get_subset_cameras(imgs_matching_regex_inds)\n    return subset_cameras\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_image_filename","title":"<code>get_image_filename(index, absolute=True)</code>","text":"<p>Get the image filename(s) based on the index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, None]</code> <p>Return the filename of the camera at this index, or all filenames if None.</p> required <code>absolute</code> <code>bool</code> <p>Return the absolute filepath, as oposed to the path relative to the image folder. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>typing.Union[PATH_TYPE, list[PATH_TYPE]]: If an integer index is provided, one path will be returned. If None, a list of paths will be returned.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_image_filename(self, index: Union[int, None], absolute=True):\n    \"\"\"Get the image filename(s) based on the index\n\n    Args:\n        index (Union[int, None]):\n            Return the filename of the camera at this index, or all filenames if None.\n            #TODO update to support lists of integer indices as well\n        absolute (bool, optional):\n            Return the absolute filepath, as oposed to the path relative to the image folder.\n            Defaults to True.\n\n    Returns:\n        typing.Union[PATH_TYPE, list[PATH_TYPE]]:\n            If an integer index is provided, one path will be returned. If None, a list of paths\n            will be returned.\n    \"\"\"\n    if index is None:\n        return [\n            self.get_image_filename(i, absolute=absolute)\n            for i in range(len(self.cameras))\n        ]\n\n    filename = self.cameras[index].get_image_filename()\n    if absolute:\n        return Path(filename)\n    else:\n        return Path(filename).relative_to(self.get_image_folder())\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_image_filename--todo-update-to-support-lists-of-integer-indices-as-well","title":"TODO update to support lists of integer indices as well","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_local_to_epsg_4978_transform","title":"<code>get_local_to_epsg_4978_transform()</code>","text":"<p>Return the 4x4 homogenous transform mapping from the local coordinates used for photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by EPSG:4978 (https://epsg.io/4978).</p> <p>Returns:</p> Type Description <p>np.ndarray: The transform in the form:    [R | t]    [0 | 1] When a homogenous vector is multiplied on the right of this matrix, it is transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse of this matrix can be used to map from EPSG:4879 to local coordinates.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_local_to_epsg_4978_transform(self):\n    \"\"\"\n    Return the 4x4 homogenous transform mapping from the local coordinates used for\n    photogrammetry to the earth-centered, earth-fixed coordinate reference system defined by\n    EPSG:4978 (https://epsg.io/4978).\n\n    Returns:\n        np.ndarray:\n            The transform in the form:\n               [R | t]\n               [0 | 1]\n            When a homogenous vector is multiplied on the right of this matrix, it is\n            transformed from the local coordinate frame to EPSG:4978. Conversely, the inverse\n            of this matrix can be used to map from EPSG:4879 to local coordinates.\n    \"\"\"\n    return self._local_to_epsg_4978_transform\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_lon_lat_coords","title":"<code>get_lon_lat_coords()</code>","text":"<p>Returns a list of GPS coords for each camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat_coords(self):\n    \"\"\"Returns a list of GPS coords for each camera\"\"\"\n    return [x.get_lon_lat() for x in self.cameras]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_subset_ROI","title":"<code>get_subset_ROI(ROI, buffer_radius=0, is_geospatial=None)</code>","text":"<p>Return cameras that are within a radius of the provided geometry</p> <p>Parameters:</p> Name Type Description Default <code>geodata</code> <code>Union[PATH_TYPE, GeoDataFrame, Polygon, MultiPolygon]</code> <p>This can be a Geopandas dataframe, path to a geofile readable by geopandas, or Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe</p> required <code>buffer_radius</code> <code>float</code> <p>Return points within this buffer of the geometry. Defaults to 0. Represents meters if ROI is geospatial.</p> <code>0</code> <code>is_geospatial</code> <code>bool</code> <p>A flag for user to indicate if ROI is geospatial or not; if no flag is provided, the flag is set if the provided geodata has a CRS.</p> <code>None</code> <p>Returns:     subset_camera_set (List[PhotogrammetryCamera]):         List of cameras that fall within the provided ROI</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_subset_ROI(\n    self,\n    ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n    buffer_radius: float = 0,\n    is_geospatial: bool = None,\n):\n    \"\"\"Return cameras that are within a radius of the provided geometry\n\n    Args:\n        geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n            This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n            Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n        buffer_radius (float, optional):\n            Return points within this buffer of the geometry. Defaults to 0. Represents\n            meters if ROI is geospatial.\n        is_geospatial (bool, optional):\n            A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n            the flag is set if the provided geodata has a CRS.\n    Returns:\n        subset_camera_set (List[PhotogrammetryCamera]):\n            List of cameras that fall within the provided ROI\n    \"\"\"\n    # construct GeoDataFrame if not provided\n    if isinstance(ROI, (Polygon, MultiPolygon)):\n        # assume geodata is lat/lon if is_geospatial is True\n        if is_geospatial:\n            ROI = gpd.GeoDataFrame(crs=LAT_LON_CRS, geometry=[ROI])\n        else:\n            ROI = gpd.GeoDataFrame(geometry=[ROI])\n    elif not isinstance(ROI, gpd.GeoDataFrame):\n        # Read in the geofile\n        ROI = gpd.read_file(ROI)\n\n    if is_geospatial is None:\n        is_geospatial = ROI.crs is not None\n\n    if not is_geospatial:\n        # get internal coordinate system camera locations\n        image_locations = [Point(*x) for x in self.get_camera_locations()]\n        image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n    else:\n        # Make sure it's a geometric (meters-based) CRS\n        ROI = ensure_projected_CRS(ROI)\n        # Read the locations of all the points\n        image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n        # Create a dataframe, assuming inputs are lat lon\n        image_locations_df = gpd.GeoDataFrame(\n            geometry=image_locations, crs=LAT_LON_CRS\n        )\n        image_locations_df.to_crs(ROI.crs, inplace=True)\n\n    # Drop all the fields except the geometry for computational reasons\n    ROI = ROI[\"geometry\"]\n    # Buffer out by the requested distance\n    ROI = ROI.buffer(buffer_radius)\n    # Merge the potentially-individual polygons to one\n    # TODO Do experiments to see if this step should be before or after the buffer.\n    # Counterintuitively, it seems that after is faster\n    ROI = unary_union(ROI.tolist())\n\n    # Determine the binary mask for which cameras are within the ROI\n    cameras_in_ROI = image_locations_df.within(ROI).to_numpy()\n    # Convert to the integer indices\n    cameras_in_ROI = np.where(cameras_in_ROI)[0]\n    # Get the corresponding subset of cameras\n    subset_camera_set = self.get_subset_cameras(cameras_in_ROI)\n    return subset_camera_set\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_vis_mesh","title":"<code>get_vis_mesh(frustum_scale=0.1)</code>","text":"<p>Get all the cameras as a mesh representation.</p> <p>Parameters:</p> Name Type Description Default <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units.</p> <code>0.1</code> <p>Returns: (PolyData) mesh representation of all cameras as frustums</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_vis_mesh(self, frustum_scale: float = 0.1) -&gt; pv.PolyData:\n    \"\"\"Get all the cameras as a mesh representation.\n\n    Args:\n        frustum_scale (float, optional): Size of cameras in world units.\n\n    Returns: (PolyData) mesh representation of all cameras as frustums\n    \"\"\"\n    return pv.merge(\n        [\n            camera.get_vis_mesh(frustum_scale=frustum_scale)\n            for camera in self.cameras\n        ]\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.ideal_to_warped","title":"<code>ideal_to_warped(camera, xpix, ypix)</code>","text":"<p>Apply the given camera's distortion parameters to map pixels in an ideal pinhole camera to pixel locations in the warped/distorted image.</p> <p>NOTE: Some standards apparently can define their distortion models from warped to ideal, if we end up handling cameras like that we can make the use-case more flexible.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>PhotogrammetryCamera</code> <p>Camera with parameters that define the warp process. These include image size, principal point, focal length, and distortion parameters.</p> required <code>xpix</code> <code>numpy array</code> <p>Array of unknown shape, must match ypix</p> required <code>ypix</code> <code>numpy array</code> <p>Array of unknown shape, must match xpix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Tuple of numpy arrays of warped x pixels and y pixels</p> <code>ndarray</code> <p>[0] warped xpix</p> <code>Tuple[ndarray, ndarray]</code> <p>[1] warped ypix</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def ideal_to_warped(\n    self, camera: PhotogrammetryCamera, xpix: np.ndarray, ypix: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Apply the given camera's distortion parameters to map pixels in an\n    ideal pinhole camera to pixel locations in the warped/distorted\n    image.\n\n    NOTE: Some standards apparently can define their distortion models\n    from warped to ideal, if we end up handling cameras like that we can\n    make the use-case more flexible.\n\n    Arguments:\n        camera (PhotogrammetryCamera): Camera with parameters that\n            define the warp process. These include image size, principal\n            point, focal length, and distortion parameters.\n        xpix (numpy array): Array of unknown shape, must match ypix\n        ypix (numpy array): Array of unknown shape, must match xpix\n\n    Returns:\n        Tuple of numpy arrays of warped x pixels and y pixels\n        [0] warped xpix\n        [1] warped ypix\n    \"\"\"\n    raise NotImplementedError(\n        f\"ideal_to_warped not implemented for {self.__class__}.\"\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.make_distortion_map","title":"<code>make_distortion_map(camera, inversion_downsample, image_scale=1.0)</code>","text":"<p>Cache a map connecting locations in one image to locations in another. The basic construction is the pixel position of the map is the position in the destination image, and the value of the map is the position in the source image. Therefore if location [20, 30] has value [22.2, 28.4], it means that the destination image pixel [20, 30] will be sampled from the source image at pixel [22.2, 28.4] (the sampler can choose to snap to the closest integer value or interpolate nearby pixels).</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>PhotogrammetryCamera</code> <p>Camera with parameters that define the warp process. These include image size, principal point, focal length, and distortion parameters.</p> required <code>inversion_downsample</code> <code>int</code> <p>Downsample the inverse map process in order to reduce computation, at high res the resulting map trafeoffs should be minimal.</p> required <code>image_scale</code> <code>float</code> <p>(float, optional) 0-1 fraction of the original image size, used when warping/dewarping downsampled images. See pix2face render_img_scale for an example.</p> <code>1.0</code> Caches <p>In self._maps_ideal_to_warped, stores a map of the structure     discussed above, keyed by self.distortion_key(params) so it can     be accessed for cameras using the same params. In self._maps_warped_to_ideal, stores an inverted version</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def make_distortion_map(\n    self,\n    camera: PhotogrammetryCamera,\n    inversion_downsample: int,\n    image_scale: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Cache a map connecting locations in one image to locations in another.\n    The basic construction is the pixel position of the map is the position\n    in the destination image, and the value of the map is the position in\n    the source image. Therefore if location [20, 30] has value [22.2, 28.4],\n    it means that the destination image pixel [20, 30] will be sampled from\n    the source image at pixel [22.2, 28.4] (the sampler can choose to snap\n    to the closest integer value or interpolate nearby pixels).\n\n    Arguments:\n        camera (PhotogrammetryCamera): Camera with parameters that\n            define the warp process. These include image size, principal\n            point, focal length, and distortion parameters.\n        inversion_downsample (int): Downsample the inverse map process in\n            order to reduce computation, at high res the resulting map\n            trafeoffs should be minimal.\n        image_scale: (float, optional) 0-1 fraction of the original image\n            size, used when warping/dewarping downsampled images. See\n            pix2face render_img_scale for an example.\n\n    Caches:\n        In self._maps_ideal_to_warped, stores a map of the structure\n            discussed above, keyed by self.distortion_key(params) so it can\n            be accessed for cameras using the same params.\n        In self._maps_warped_to_ideal, stores an inverted version\n    \"\"\"\n\n    # Sample over the ideal pixels, shape (H, W)\n    im_h, im_w = camera.image_size\n    if np.isclose(image_scale, 1.0):\n        h_range = np.arange(im_h)\n        w_range = np.arange(im_w)\n    else:\n        # In order to get a distortion map for a downsampled image, you\n        # need to run the ideal_to_warped equation over the same original\n        # range (0...im_h) because the radius of that range relates\n        # directly to the warping. However, do it in fewer steps (step\n        # size &gt; 1) to reflect the downsampling.\n        kwargs = {\"start\": 1 / (2 * image_scale), \"step\": 1 / image_scale}\n        h_range = np.arange(stop=im_h, **kwargs)[: int(im_h * image_scale)]\n        w_range = np.arange(stop=im_w, **kwargs)[: int(im_w * image_scale)]\n    rows, cols = np.meshgrid(h_range, w_range, indexing=\"ij\")\n\n    # Fill the (H, W) elements with the (i, j) distorted values at those locations\n    warp_cols, warp_rows = self.ideal_to_warped(camera, cols, rows)\n\n    # If dealing with downsampled images, now that we ran the warp equation,\n    # rescale the results to be within the new scale. If the original im_h\n    # is 1000 and image_scale is 0.5, we need to run ideal_to_warped on\n    # 0-1000 (for radius reasons) and then scale those results to 0-500.\n    if not np.isclose(image_scale, 1.0):\n        warp_cols *= image_scale\n        warp_rows *= image_scale\n\n    # Cache this mapping as (2, H, W)\n    dkey = self.distortion_key(camera.distortion_params, image_scale)\n    self._maps_ideal_to_warped[dkey] = np.stack([warp_rows, warp_cols], axis=0)\n    # Invert the warp map and cache as (2, H, W)\n    self._maps_warped_to_ideal[dkey] = inverse_map_interpolation(\n        self._maps_ideal_to_warped[dkey],\n        downsample=inversion_downsample,\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_cameras","title":"<code>n_cameras()</code>","text":"<p>Return the number of cameras</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_cameras(self) -&gt; int:\n    \"\"\"Return the number of cameras\"\"\"\n    return len(self.cameras)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_image_channels","title":"<code>n_image_channels()</code>","text":"<p>Return the number of channels in the image</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_image_channels(self) -&gt; int:\n    \"\"\"Return the number of channels in the image\"\"\"\n    return 3\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.triangulate_detections","title":"<code>triangulate_detections(detector, ray_length_meters=1000.0, boundaries=None, limit_ray_length_meters=None, limit_angle_from_vert=None, similarity_threshold_meters=0.1, transform=None, louvain_resolution=1.0, out_dir=None)</code>","text":"<p>Take per-image detections and triangulate them to 3D locations.</p> <p>Parameters:</p> Name Type Description Default <code>detector</code> <code>Union[RegionDetectionSegmentor, TabularRectangleSegmentor]</code> <p>Produces detections per image using the get_detection_centers method.</p> required <code>ray_length_meters</code> <code>float</code> <p>The length of the visualized rays in meters. Defaults to 1000.</p> <code>1000.0</code> <code>similarity_threshold_meters</code> <code>float</code> <p>Consider rays a potential match if the distance between them is less than this value. Defaults to 0.1.</p> <code>0.1</code> <code>transform</code> <code>Optional[Callable[[ndarray], ndarray]]</code> <p>Defaults to None.</p> <code>None</code> <code>louvain_resolution</code> <code>float</code> <p>The resolution hyperparameter of the networkx.louvain_communities function. Height value leads to more communities. Defaults to 1.0.</p> <code>1.0</code> <code>out_dir</code> <code>Optional[PATH_TYPE]</code> <p>Optional[PATH_TYPE] Defaults to None.</p> <code>None</code> <p>Returns (np.ndarray):     (N unique objects, 3), the 3D locations of the identified objects.     If transform_to_epsg_4978 is None, then this is in the local coordinate     system of the mesh. If the transform is not None, (lat, lon, alt) is returned.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def triangulate_detections(\n    self,\n    detector: Union[RegionDetectionSegmentor, TabularRectangleSegmentor],\n    ray_length_meters: float = 1e3,\n    boundaries: Optional[Tuple[pv.PolyData, pv.PolyData]] = None,\n    limit_ray_length_meters: Optional[float] = None,\n    limit_angle_from_vert: Optional[float] = None,\n    similarity_threshold_meters: float = 0.1,\n    transform: Optional[Callable[[np.ndarray], np.ndarray]] = None,\n    louvain_resolution: float = 1.0,\n    out_dir: Optional[PATH_TYPE] = None,\n) -&gt; np.ndarray:\n    \"\"\"Take per-image detections and triangulate them to 3D locations.\n\n    Args:\n        detector (Union[RegionDetectionSegmentor, TabularRectangleSegmentor]):\n            Produces detections per image using the get_detection_centers method.\n        ray_length_meters (float, optional):\n            The length of the visualized rays in meters. Defaults to 1000.\n        boundaries (Optional[Tuple[pv.PolyData, pv.PolyData]])\n            Defaults to None.\n        limit_ray_length_meters (Optional[float])\n            Defaults to None.\n        limit_angle_from_vert (Optional[float])\n            Defaults to None.\n        similarity_threshold_meters (float, optional):\n            Consider rays a potential match if the distance between them is less than this\n            value. Defaults to 0.1.\n        transform (Optional[Callable[[np.ndarray], np.ndarray]]):\n            Defaults to None.\n        louvain_resolution (float, optional):\n            The resolution hyperparameter of the networkx.louvain_communities function.\n            Height value leads to more communities. Defaults to 1.0.\n        out_dir: Optional[PATH_TYPE]\n            Defaults to None.\n\n    Returns (np.ndarray):\n        (N unique objects, 3), the 3D locations of the identified objects.\n        If transform_to_epsg_4978 is None, then this is in the local coordinate\n        system of the mesh. If the transform is not None, (lat, lon, alt) is returned.\n    \"\"\"\n\n    # Enforce Path type\n    if out_dir is not None:\n        out_dir = Path(out_dir)\n\n    def check_exists(file: Union[str, Path]):\n        \"\"\"Helper function to aid in caching and loading cached files.\"\"\"\n        if out_dir is None:\n            return False\n        if isinstance(file, str):\n            path = out_dir / file\n        else:\n            path = file\n        return path.is_file()\n\n    # Determine scale factor relating meters to internal coordinates\n    transform_to_epsg_4978 = self.get_local_to_epsg_4978_transform()\n    meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n    ray_length_local = ray_length_meters * meters_to_local_scale\n    similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n    if limit_ray_length_meters is None:\n        limit_ray_length_local = None\n    else:\n        limit_ray_length_local = limit_ray_length_meters * meters_to_local_scale\n\n    # Create line segments emanating from the cameras\n    if check_exists(\"line_segments.npz\"):\n        line_results = out_dir / \"line_segments.npz\"\n    else:\n        line_results = self.calc_line_segments(\n            detector=detector,\n            boundaries=boundaries,\n            ray_length_local=ray_length_local,\n            out_dir=out_dir,\n            limit_ray_length_local=limit_ray_length_local,\n            limit_angle_from_vert=limit_angle_from_vert,\n        )\n    # Load the results into memory if they were saved to file\n    if check_exists(line_results):\n        line_results = np.load(line_results)\n\n    # Turn line segments into graph distances, where \"close enough\"\n    # lines are connected nodes in the graph.\n    if check_exists(\"edge_weights.json\"):\n        weight_results = out_dir / \"edge_weights.json\"\n    else:\n        weight_results = calc_graph_weights(\n            starts=line_results[\"ray_starts\"],\n            ends=line_results[\"ray_ends\"],\n            ray_IDs=line_results[\"ray_IDs\"],\n            similarity_threshold=similarity_threshold_local,\n            out_dir=out_dir,\n            step=5000,\n            transform=transform,\n        )\n    # Load the results into memory if they were saved to file\n    if check_exists(weight_results):\n        weight_results = json.load(weight_results.open(\"r\"))\n\n    # Calculate community identities among the graph weights, where\n    # hopefully a preponderance of close line segments indicate a\n    # single object detected multiple times.\n    if check_exists(\"communities.npz\"):\n        community_results = out_dir / \"communities.npz\"\n    else:\n        community_results = calc_communities(\n            starts=line_results[\"ray_starts\"],\n            ends=line_results[\"ray_ends\"],\n            edge_weights=weight_results,\n            louvain_resolution=louvain_resolution,\n            out_dir=out_dir,\n            transform_to_epsg_4978=transform_to_epsg_4978,\n        )\n    # Load the results into memory if they were saved to file\n    if check_exists(community_results):\n        community_results = np.load(community_results)\n\n    # Return the 3D locations of the community points, preferentially\n    # in lat/lon form if it exists.\n    return community_results.get(\n        \"community_points_latlon\",\n        community_results[\"community_points\"],\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.vis","title":"<code>vis(plotter=None, add_orientation_cube=False, show=False, frustum_scale=None, force_xvfb=False, interactive_jupyter=False)</code>","text":"<p>Visualize all the cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to add the cameras to. If None, will be created and then plotted</p> <code>None</code> <code>add_orientation_cube</code> <code>bool</code> <p>Add a cube to visualize the coordinate system. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Show the results instead of waiting for other content to be added</p> <code>False</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. If None, will set to 1/120th of the maximum distance between two cameras.</p> <code>None</code> <code>force_xvfb</code> <code>bool</code> <p>Force a headless rendering backend</p> <code>False</code> <code>interactive_jupyter</code> <code>bool</code> <p>Will allow you to interact with the visualization in your notebook if supported by the notebook server. Otherwise will fail. Only applicable if <code>show=True</code>. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    add_orientation_cube: bool = False,\n    show: bool = False,\n    frustum_scale: float = None,\n    force_xvfb: bool = False,\n    interactive_jupyter: bool = False,\n):\n    \"\"\"Visualize all the cameras\n\n    Args:\n        plotter (pv.Plotter):\n            Plotter to add the cameras to. If None, will be created and then plotted\n        add_orientation_cube (bool, optional):\n            Add a cube to visualize the coordinate system. Defaults to False.\n        show (bool, optional):\n            Show the results instead of waiting for other content to be added\n        frustum_scale (float, optional):\n            Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n            between two cameras.\n        force_xvfb (bool, optional):\n            Force a headless rendering backend\n        interactive_jupyter (bool, optional):\n            Will allow you to interact with the visualization in your notebook if supported by\n            the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n            to False.\n\n    \"\"\"\n\n    if plotter is None:\n        plotter = pv.Plotter()\n        show = True\n\n    # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n    if frustum_scale is None:\n        if self.n_cameras() &gt;= 2:\n            camera_translation_matrices = np.array(\n                [transform[:3, 3] for transform in self.cam_to_world_transforms]\n            )\n            distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n            max_distance = np.max(distances)\n            frustum_scale = (\n                (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n            )\n        # else, set it to a default\n        else:\n            frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n    for camera in self.cameras:\n        camera.vis(plotter, frustum_scale=frustum_scale)\n    if add_orientation_cube:\n        # TODO Consider adding to a freestanding vis module\n        ocube = demos.orientation_cube()\n        plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n        plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n        plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n        plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n        plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n        plotter.show_axes()\n\n    if show:\n        if force_xvfb:\n            safe_start_xvfb()\n        plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.warp_dewarp_image","title":"<code>warp_dewarp_image(camera, input_image, fill_value=0.0, inversion_downsample=8, interpolation_order=1, warped_to_ideal=True, image_scale=1.0)</code>","text":"<p>Either apply a camera's distortion model to go from ideal\u2192warped or undo the camera's distortion model to go from warped\u2192ideal, depending on the warped_to_ideal flag.</p> <p>Pixels in the output image that do not correspond to any input pixel are set to the fill value.</p> <p>Note that the warp map is cached, so the first call will take longer and subsequent calls should be much faster.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>PhotogrammetryCamera</code> <p>Camera with parameters that define the warp process. These include image size, principal point, focal length, and distortion parameters.</p> required <code>input_image</code> <code>ndarray</code> <p>(I, J, 3) Input image</p> required <code>fill_value</code> <code>int</code> <p>Value to use for pixels in the output image that are not mapped from the input. Defaults to 0.</p> <code>0.0</code> <code>inversion_downsample</code> <code>int</code> <p>The distortion map creation process is too heavyweight for really high-res images, downsampling the inversion process gets a similar result with less computation.</p> <code>8</code> <code>interpolation_order</code> <code>int</code> <p>The order of the interpolation. 0 is nearest neighbor and should be used for discrete textures like pix2face masks. 1 can be used for data representing continious quantities. Defaults to 1.</p> <code>1</code> <code>warped_to_ideal</code> <code>bool</code> <p>If true, take in a warped image and return an undistorted (dewarped/ideal) image. If false, take in an undistorted image and return a warped image.</p> <code>True</code> <code>image_scale</code> <code>float</code> <p>(float, optional) 0-1 fraction of the original image size, used when warping/dewarping downsampled images. See pix2face render_img_scale for an example.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (I, J, 3) output image</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def warp_dewarp_image(\n    self,\n    camera: PhotogrammetryCamera,\n    input_image: np.ndarray,\n    fill_value: float = 0.0,\n    inversion_downsample: int = 8,\n    interpolation_order: int = 1,\n    warped_to_ideal: bool = True,\n    image_scale: float = 1.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Either apply a camera's distortion model to go from ideal\u2192warped or\n    undo the camera's distortion model to go from warped\u2192ideal, depending\n    on the warped_to_ideal flag.\n\n    Pixels in the output image that do not correspond to any input pixel\n    are set to the fill value.\n\n    Note that the warp map is cached, so the first call will take longer\n    and subsequent calls should be much faster.\n\n    Arguments:\n        camera (PhotogrammetryCamera): Camera with parameters that\n            define the warp process. These include image size, principal\n            point, focal length, and distortion parameters.\n        input_image (np.ndarray): (I, J, 3) Input image\n        fill_value (int, optional): Value to use for pixels in the\n            output image that are not mapped from the input. Defaults to 0.\n        inversion_downsample (int, optional): The distortion map creation\n            process is too heavyweight for really high-res images,\n            downsampling the inversion process gets a similar result with\n            less computation.\n        interpolation_order (int, optional):\n            The order of the interpolation. 0 is nearest neighbor and should be used for discrete\n            textures like pix2face masks. 1 can be used for data representing continious\n            quantities. Defaults to 1.\n        warped_to_ideal (bool, optional): If true, take in a warped image\n            and return an undistorted (dewarped/ideal) image. If false,\n            take in an undistorted image and return a warped image.\n        image_scale: (float, optional) 0-1 fraction of the original image\n            size, used when warping/dewarping downsampled images. See\n            pix2face render_img_scale for an example.\n\n    Returns:\n        np.ndarray: (I, J, 3) output image\n    \"\"\"\n\n    # Ensure that there is a cached map for these distortion parameters\n    dkey = self.distortion_key(camera.distortion_params, image_scale)\n    if dkey not in self._maps_ideal_to_warped:\n        self.make_distortion_map(camera, inversion_downsample, image_scale)\n\n    if warped_to_ideal:\n        inverse_map = self._maps_ideal_to_warped[dkey]\n    else:\n        inverse_map = self._maps_warped_to_ideal[dkey]\n\n    warped_image = flexible_inputs_warp(\n        input_image=input_image,\n        inverse_map=inverse_map,\n        interpolation_order=interpolation_order,\n        fill_value=fill_value,\n    )\n\n    return warped_image\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.warp_dewarp_pixels","title":"<code>warp_dewarp_pixels(camera, pixels, inversion_downsample=8, warped_to_ideal=True)</code>","text":"<p>Either apply a camera's distortion model to go from ideal pixels\u2192warped or undo the camera's distortion model to go from warped pixels\u2192ideal, depending on the warped_to_ideal flag.</p> <p>Note that the warp map is cached, so the first call will take longer and subsequent calls should be much faster.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>PhotogrammetryCamera</code> <p>Camera with parameters that define the warp process. These include image size, principal point, focal length, and distortion parameters.</p> required <code>pixels</code> <code>ndarray</code> <p>(N, 2) Pixels locations in (i, j) format.</p> required <code>inversion_downsample</code> <code>int</code> <p>The distortion map creation process is too heavyweight for really high-res images, downsampling the inversion process gets a similar result with less computation.</p> <code>8</code> <code>warped_to_ideal</code> <code>bool</code> <p>If true, take in a warped image and return an undistorted (dewarped/ideal) image. If false, take in an undistorted image and return a warped image.</p> <code>True</code> <p>Returns:</p> Type Description <p>np.ndarray: (N, 2) warped/dewarped output pixel locations (i, j) Note that the output is floating point (subpixel)</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def warp_dewarp_pixels(\n    self,\n    camera: PhotogrammetryCamera,\n    pixels: np.ndarray,\n    inversion_downsample: int = 8,\n    warped_to_ideal: bool = True,\n):\n    \"\"\"\n    Either apply a camera's distortion model to go from ideal pixels\u2192warped\n    or undo the camera's distortion model to go from warped pixels\u2192ideal,\n    depending on the warped_to_ideal flag.\n\n    Note that the warp map is cached, so the first call will take longer\n    and subsequent calls should be much faster.\n\n    Arguments:\n        camera (PhotogrammetryCamera): Camera with parameters that\n            define the warp process. These include image size, principal\n            point, focal length, and distortion parameters.\n        pixels (np.ndarray): (N, 2) Pixels locations in (i, j) format.\n        inversion_downsample (int, optional): The distortion map creation\n            process is too heavyweight for really high-res images,\n            downsampling the inversion process gets a similar result with\n            less computation.\n        warped_to_ideal (bool, optional): If true, take in a warped image\n            and return an undistorted (dewarped/ideal) image. If false,\n            take in an undistorted image and return a warped image.\n\n    Returns:\n        np.ndarray: (N, 2) warped/dewarped output pixel locations (i, j)\n            Note that the output is floating point (subpixel)\n    \"\"\"\n\n    # Ensure that there is a cached map for these distortion parameters\n    dkey = self.distortion_key(camera.distortion_params)\n    if dkey not in self._maps_ideal_to_warped:\n        self.make_distortion_map(camera, inversion_downsample)\n\n    # Get the right mapping array\n    if warped_to_ideal:\n        rowmap, colmap = self._maps_warped_to_ideal[dkey]\n    else:\n        rowmap, colmap = self._maps_ideal_to_warped[dkey]\n\n    # Look up the pixel locations\n    rows = rowmap[pixels[:, 0], pixels[:, 1]]\n    cols = colmap[pixels[:, 0], pixels[:, 1]]\n    return np.stack([rows, cols], axis=0).T\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/","title":"Derived Cameras Docstrings","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet","title":"<code>MetashapeCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class MetashapeCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        camera_file: PATH_TYPE,\n        image_folder: PATH_TYPE,\n        original_image_folder: typing.Optional[PATH_TYPE] = None,\n        validate_images: bool = False,\n        default_sensor_params: dict = {\"cx\": 0.0, \"cy\": 0.0},\n    ):\n        \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n        Args:\n            camera_file (PATH_TYPE):\n                Path to metashape .xml export\n            image_folder (PATH_TYPE):\n                Path to image folder root\n            original_image_folder (PATH_TYPE, optional):\n                Path to where the original images for photogrammetry were, which was not included\n                in the stored image zip files. This is removed from the absolute path recorded in\n                the camera file. Defaults to None.\n            validate_images (bool, optional): Should the existance of the images be checked.\n                Any image_filenames found in the camera_file that do not exist on disk will be\n                dropped, leaving a CameraSet only containing existing images. Defaults to False.\n            default_sensor_params (dict, optional):\n                Default parameters for the intrinsic parameters if not present. Defaults to zeros\n                \"cx\" and \"cy\".\n\n        Raises:\n            ValueError: If camera calibration does not contain the f, cx, and cy params\n        \"\"\"\n        # Load the xml file\n        # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n        tree = ET.parse(camera_file)\n        root = tree.getroot()\n        # first level\n        chunk = root.find(\"chunk\")\n        # second level\n        sensors = chunk.find(\"sensors\")\n        # Parse the sensors representation\n        sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n        # Set up the lists to populate\n        image_filenames = []\n        cam_to_world_transforms = []\n        sensor_IDs = []\n\n        cameras = chunk.find(\"cameras\")\n        # Iterate over metashape cameras and fill out required information\n        for cam_or_group in cameras:\n            if cam_or_group.tag == \"group\":\n                for cam in cam_or_group:\n                    update_lists(\n                        cam,\n                        image_folder,\n                        cam_to_world_transforms,\n                        image_filenames,\n                        sensor_IDs,\n                        original_image_folder=original_image_folder,\n                    )\n            else:\n                update_lists(\n                    cam_or_group,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                    original_image_folder=original_image_folder,\n                )\n\n        # Compute the lat lon using the transforms, because the reference values recorded in the file\n        # reflect the EXIF values, not the optimized ones\n\n        # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n        chunk_to_epsg4978 = parse_transform_metashape(camera_file=camera_file)\n\n        if chunk_to_epsg4978 is not None:\n            # Compute the location of each camera in ECEF\n            cam_locs_in_epsg4978 = []\n            for cam_to_world_transform in cam_to_world_transforms:\n                cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n                cam_locs_in_epsg4978.append(chunk_to_epsg4978 @ cam_loc_in_chunk)\n            cam_locs_in_epsg4978 = np.concatenate(cam_locs_in_epsg4978, axis=1)[:3].T\n            # Transform these points into lat-lon-alt\n            transformer = pyproj.Transformer.from_crs(\n                EARTH_CENTERED_EARTH_FIXED_CRS, LAT_LON_CRS\n            )\n            lat, lon, _ = transformer.transform(\n                xx=cam_locs_in_epsg4978[:, 0],\n                yy=cam_locs_in_epsg4978[:, 1],\n                zz=cam_locs_in_epsg4978[:, 2],\n            )\n            lon_lats = list(zip(lon, lat))\n        else:\n            # TODO consider trying to parse from the xml\n            lon_lats = None\n\n        # Actually construct the camera objects using the base class\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            lon_lats=lon_lats,\n            image_folder=image_folder,\n            sensor_IDs=sensor_IDs,\n            validate_images=validate_images,\n            local_to_epsg_4978_transform=chunk_to_epsg4978,\n        )\n\n    def ideal_to_warped(\n        self, camera: PhotogrammetryCamera, xpix: np.ndarray, ypix: np.ndarray\n    ) -&gt; typing.Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        For consistency, this should fully match the docstring from\n        cameras.PhotogrammetryCameraSet.ideal_to_warped()\n        \"\"\"\n\n        # Convert from x and y pixels to the homogeneous camera frame\n        # Note that for the math to work out, the cx and cy terms are neglected in this step and\n        # only applied at the very end\n        principal_x = camera.image_width / 2.0\n        principal_y = camera.image_height / 2.0\n        x = (xpix - principal_x) / camera.f\n        y = (ypix - principal_y) / camera.f\n\n        # Enforce that a strict subset of expected parameters are found\n        params = sorted(camera.distortion_params.keys())\n        if not set(params) &lt;= set([\"b1\", \"b2\", \"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"]):\n            raise ValueError(f\"Unexpected distortion params found: {params}\")\n        b1 = camera.distortion_params.get(\"b1\", 0)\n        b2 = camera.distortion_params.get(\"b2\", 0)\n        k1 = camera.distortion_params[\"k1\"]  # Enforce that the most basic is required\n        k2 = camera.distortion_params.get(\"k2\", 0)\n        k3 = camera.distortion_params.get(\"k3\", 0)\n        k4 = camera.distortion_params.get(\"k4\", 0)\n        p1 = camera.distortion_params.get(\"p1\", 0)\n        p2 = camera.distortion_params.get(\"p2\", 0)\n\n        # See page 246 of the manual (labeled page 240) \"Frame Cameras\" section\n        # for what these parameters mean\n        # https://www.agisoft.com/pdf/metashape-pro_2_2_en.pdf\n        r = np.sqrt(x**2 + y**2)\n        #  Distorted rays\n        xd = x * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6 + k4 * r**8) + (\n            p1 * (r**2 + 2 * x**2) + 2 * p2 * x * y\n        )\n        yd = y * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6 + k4 * r**8) + (\n            p2 * (r**2 + 2 * y**2) + 2 * p1 * x * y\n        )\n        # Pixels\n        xpix_warp = (\n            camera.image_width / 2.0 + camera.cx + xd * camera.f + xd * b1 + yd * b2\n        )\n        ypix_warp = camera.image_height / 2.0 + camera.cy + yd * camera.f\n        return xpix_warp, ypix_warp\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet.__init__","title":"<code>__init__(camera_file, image_folder, original_image_folder=None, validate_images=False, default_sensor_params={'cx': 0.0, 'cy': 0.0})</code>","text":"<p>Parse the information about the camera intrinsics and extrinsics</p> <p>Parameters:</p> Name Type Description Default <code>camera_file</code> <code>PATH_TYPE</code> <p>Path to metashape .xml export</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to image folder root</p> required <code>original_image_folder</code> <code>PATH_TYPE</code> <p>Path to where the original images for photogrammetry were, which was not included in the stored image zip files. This is removed from the absolute path recorded in the camera file. Defaults to None.</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Should the existance of the images be checked. Any image_filenames found in the camera_file that do not exist on disk will be dropped, leaving a CameraSet only containing existing images. Defaults to False.</p> <code>False</code> <code>default_sensor_params</code> <code>dict</code> <p>Default parameters for the intrinsic parameters if not present. Defaults to zeros \"cx\" and \"cy\".</p> <code>{'cx': 0.0, 'cy': 0.0}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If camera calibration does not contain the f, cx, and cy params</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    camera_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    original_image_folder: typing.Optional[PATH_TYPE] = None,\n    validate_images: bool = False,\n    default_sensor_params: dict = {\"cx\": 0.0, \"cy\": 0.0},\n):\n    \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n    Args:\n        camera_file (PATH_TYPE):\n            Path to metashape .xml export\n        image_folder (PATH_TYPE):\n            Path to image folder root\n        original_image_folder (PATH_TYPE, optional):\n            Path to where the original images for photogrammetry were, which was not included\n            in the stored image zip files. This is removed from the absolute path recorded in\n            the camera file. Defaults to None.\n        validate_images (bool, optional): Should the existance of the images be checked.\n            Any image_filenames found in the camera_file that do not exist on disk will be\n            dropped, leaving a CameraSet only containing existing images. Defaults to False.\n        default_sensor_params (dict, optional):\n            Default parameters for the intrinsic parameters if not present. Defaults to zeros\n            \"cx\" and \"cy\".\n\n    Raises:\n        ValueError: If camera calibration does not contain the f, cx, and cy params\n    \"\"\"\n    # Load the xml file\n    # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n    tree = ET.parse(camera_file)\n    root = tree.getroot()\n    # first level\n    chunk = root.find(\"chunk\")\n    # second level\n    sensors = chunk.find(\"sensors\")\n    # Parse the sensors representation\n    sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n    # Set up the lists to populate\n    image_filenames = []\n    cam_to_world_transforms = []\n    sensor_IDs = []\n\n    cameras = chunk.find(\"cameras\")\n    # Iterate over metashape cameras and fill out required information\n    for cam_or_group in cameras:\n        if cam_or_group.tag == \"group\":\n            for cam in cam_or_group:\n                update_lists(\n                    cam,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                    original_image_folder=original_image_folder,\n                )\n        else:\n            update_lists(\n                cam_or_group,\n                image_folder,\n                cam_to_world_transforms,\n                image_filenames,\n                sensor_IDs,\n                original_image_folder=original_image_folder,\n            )\n\n    # Compute the lat lon using the transforms, because the reference values recorded in the file\n    # reflect the EXIF values, not the optimized ones\n\n    # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n    chunk_to_epsg4978 = parse_transform_metashape(camera_file=camera_file)\n\n    if chunk_to_epsg4978 is not None:\n        # Compute the location of each camera in ECEF\n        cam_locs_in_epsg4978 = []\n        for cam_to_world_transform in cam_to_world_transforms:\n            cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n            cam_locs_in_epsg4978.append(chunk_to_epsg4978 @ cam_loc_in_chunk)\n        cam_locs_in_epsg4978 = np.concatenate(cam_locs_in_epsg4978, axis=1)[:3].T\n        # Transform these points into lat-lon-alt\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_CRS, LAT_LON_CRS\n        )\n        lat, lon, _ = transformer.transform(\n            xx=cam_locs_in_epsg4978[:, 0],\n            yy=cam_locs_in_epsg4978[:, 1],\n            zz=cam_locs_in_epsg4978[:, 2],\n        )\n        lon_lats = list(zip(lon, lat))\n    else:\n        # TODO consider trying to parse from the xml\n        lon_lats = None\n\n    # Actually construct the camera objects using the base class\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        lon_lats=lon_lats,\n        image_folder=image_folder,\n        sensor_IDs=sensor_IDs,\n        validate_images=validate_images,\n        local_to_epsg_4978_transform=chunk_to_epsg4978,\n    )\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet.ideal_to_warped","title":"<code>ideal_to_warped(camera, xpix, ypix)</code>","text":"<p>For consistency, this should fully match the docstring from cameras.PhotogrammetryCameraSet.ideal_to_warped()</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def ideal_to_warped(\n    self, camera: PhotogrammetryCamera, xpix: np.ndarray, ypix: np.ndarray\n) -&gt; typing.Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    For consistency, this should fully match the docstring from\n    cameras.PhotogrammetryCameraSet.ideal_to_warped()\n    \"\"\"\n\n    # Convert from x and y pixels to the homogeneous camera frame\n    # Note that for the math to work out, the cx and cy terms are neglected in this step and\n    # only applied at the very end\n    principal_x = camera.image_width / 2.0\n    principal_y = camera.image_height / 2.0\n    x = (xpix - principal_x) / camera.f\n    y = (ypix - principal_y) / camera.f\n\n    # Enforce that a strict subset of expected parameters are found\n    params = sorted(camera.distortion_params.keys())\n    if not set(params) &lt;= set([\"b1\", \"b2\", \"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"]):\n        raise ValueError(f\"Unexpected distortion params found: {params}\")\n    b1 = camera.distortion_params.get(\"b1\", 0)\n    b2 = camera.distortion_params.get(\"b2\", 0)\n    k1 = camera.distortion_params[\"k1\"]  # Enforce that the most basic is required\n    k2 = camera.distortion_params.get(\"k2\", 0)\n    k3 = camera.distortion_params.get(\"k3\", 0)\n    k4 = camera.distortion_params.get(\"k4\", 0)\n    p1 = camera.distortion_params.get(\"p1\", 0)\n    p2 = camera.distortion_params.get(\"p2\", 0)\n\n    # See page 246 of the manual (labeled page 240) \"Frame Cameras\" section\n    # for what these parameters mean\n    # https://www.agisoft.com/pdf/metashape-pro_2_2_en.pdf\n    r = np.sqrt(x**2 + y**2)\n    #  Distorted rays\n    xd = x * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6 + k4 * r**8) + (\n        p1 * (r**2 + 2 * x**2) + 2 * p2 * x * y\n    )\n    yd = y * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6 + k4 * r**8) + (\n        p2 * (r**2 + 2 * y**2) + 2 * p1 * x * y\n    )\n    # Pixels\n    xpix_warp = (\n        camera.image_width / 2.0 + camera.cx + xd * camera.f + xd * b1 + yd * b2\n    )\n    ypix_warp = camera.image_height / 2.0 + camera.cy + yd * camera.f\n    return xpix_warp, ypix_warp\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet","title":"<code>COLMAPCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class COLMAPCameraSet(PhotogrammetryCameraSet):\n\n    def __init__(\n        self,\n        cameras_file: PATH_TYPE,\n        images_file: PATH_TYPE,\n        image_folder: typing.Union[None, PATH_TYPE] = None,\n        validate_images: bool = False,\n    ):\n        \"\"\"\n        Create a camera set from the files exported by the open-source structure-from-motion\n        software COLMAP as defined here: https://colmap.github.io/format.html\n\n        Args:\n            cameras_file (PATH_TYPE):\n                Path to the file containing the camera models definitions\n            images_file (PATH_TYPE):\n                Path to the per-image information, including the pose and which camera model is used\n            image_folder (typing.Union[None, PATH_TYPE], optional):\n                Path to the folder of images used to generate the reconstruction. Defaults to None.\n            validate_images (bool, optional):\n                Ensure that the images described in images_file are present in image_folder.\n                Defaults to False.\n\n        Raises:\n            NotImplementedError: If the camera is not a Simple radial model\n        \"\"\"\n        # Parse the csv representation of the camera models\n        cameras_data = pd.read_csv(\n            cameras_file,\n            sep=\" \",\n            skiprows=[0, 1, 2],\n            header=None,\n            names=(\n                \"CAMERA_ID\",\n                \"MODEL\",\n                \"WIDTH\",\n                \"HEIGHT\",\n                \"PARAMS_F\",\n                \"PARAMS_CX\",\n                \"PARAMS_CY\",\n                \"PARAMS_RADIAL\",\n            ),\n        )\n        # Parse the csv of the per-image information\n        # Note that every image has first the useful information on one row and then unneeded\n        # keypoint information on the following row. Therefore, the keypoints are discarded.\n        images_data = pd.read_csv(\n            images_file,\n            sep=\" \",\n            skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n            header=None,\n            names=(\n                \"IMAGE_ID\",\n                \"QW\",\n                \"QX\",\n                \"QY\",\n                \"QZ\",\n                \"TX\",\n                \"TY\",\n                \"TZ\",\n                \"CAMERA_ID\",\n                \"NAME\",\n            ),\n            usecols=list(range(10)),\n        )\n\n        # TODO support more camera models\n        if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n            raise NotImplementedError(\"Not a supported camera model\")\n\n        # Parse the camera parameters, creating a dict for each distinct camera model\n        sensors_dict = {}\n        for _, row in cameras_data.iterrows():\n            # Note that the convention in this tool is for cx, cy to be defined from the center\n            # not the corner so it must be shifted\n            sensor_dict = {\n                \"image_width\": row[\"WIDTH\"],\n                \"image_height\": row[\"HEIGHT\"],\n                \"f\": row[\"PARAMS_F\"],\n                \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n                \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n                \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n            }\n            sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n        # Parse the per-image information\n        cam_to_world_transforms = []\n        sensor_IDs = []\n        image_filenames = []\n\n        for _, row in images_data.iterrows():\n            # Convert from the quaternion representation to the matrix one. Note that the W element\n            # is the first one in the COLMAP convention but the last one in scipy.\n            rot_mat = Rotation.from_quat(\n                (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n            ).as_matrix()\n            # Get the camera translation\n            translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n            # Create a 4x4 homogenous matrix representing the world_to_cam transform\n            world_to_cam = np.eye(4)\n            # Populate the sub-elements\n            world_to_cam[:3, :3] = rot_mat\n            world_to_cam[:3, 3] = translation_vec\n            # We need the cam to world transform. Since we're using a 4x4 representation, we can\n            # just invert the matrix\n            cam_to_world = np.linalg.inv(world_to_cam)\n            cam_to_world_transforms.append(cam_to_world)\n\n            # Record which camera model is used and the image filename\n            sensor_IDs.append(row[\"CAMERA_ID\"])\n            image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n        # Instantiate the camera set\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            sensor_IDs=sensor_IDs,\n            image_folder=image_folder,\n            validate_images=validate_images,\n        )\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet.__init__","title":"<code>__init__(cameras_file, images_file, image_folder=None, validate_images=False)</code>","text":"<p>Create a camera set from the files exported by the open-source structure-from-motion software COLMAP as defined here: https://colmap.github.io/format.html</p> <p>Parameters:</p> Name Type Description Default <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the file containing the camera models definitions</p> required <code>images_file</code> <code>PATH_TYPE</code> <p>Path to the per-image information, including the pose and which camera model is used</p> required <code>image_folder</code> <code>Union[None, PATH_TYPE]</code> <p>Path to the folder of images used to generate the reconstruction. Defaults to None.</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Ensure that the images described in images_file are present in image_folder. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the camera is not a Simple radial model</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras_file: PATH_TYPE,\n    images_file: PATH_TYPE,\n    image_folder: typing.Union[None, PATH_TYPE] = None,\n    validate_images: bool = False,\n):\n    \"\"\"\n    Create a camera set from the files exported by the open-source structure-from-motion\n    software COLMAP as defined here: https://colmap.github.io/format.html\n\n    Args:\n        cameras_file (PATH_TYPE):\n            Path to the file containing the camera models definitions\n        images_file (PATH_TYPE):\n            Path to the per-image information, including the pose and which camera model is used\n        image_folder (typing.Union[None, PATH_TYPE], optional):\n            Path to the folder of images used to generate the reconstruction. Defaults to None.\n        validate_images (bool, optional):\n            Ensure that the images described in images_file are present in image_folder.\n            Defaults to False.\n\n    Raises:\n        NotImplementedError: If the camera is not a Simple radial model\n    \"\"\"\n    # Parse the csv representation of the camera models\n    cameras_data = pd.read_csv(\n        cameras_file,\n        sep=\" \",\n        skiprows=[0, 1, 2],\n        header=None,\n        names=(\n            \"CAMERA_ID\",\n            \"MODEL\",\n            \"WIDTH\",\n            \"HEIGHT\",\n            \"PARAMS_F\",\n            \"PARAMS_CX\",\n            \"PARAMS_CY\",\n            \"PARAMS_RADIAL\",\n        ),\n    )\n    # Parse the csv of the per-image information\n    # Note that every image has first the useful information on one row and then unneeded\n    # keypoint information on the following row. Therefore, the keypoints are discarded.\n    images_data = pd.read_csv(\n        images_file,\n        sep=\" \",\n        skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n        header=None,\n        names=(\n            \"IMAGE_ID\",\n            \"QW\",\n            \"QX\",\n            \"QY\",\n            \"QZ\",\n            \"TX\",\n            \"TY\",\n            \"TZ\",\n            \"CAMERA_ID\",\n            \"NAME\",\n        ),\n        usecols=list(range(10)),\n    )\n\n    # TODO support more camera models\n    if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n        raise NotImplementedError(\"Not a supported camera model\")\n\n    # Parse the camera parameters, creating a dict for each distinct camera model\n    sensors_dict = {}\n    for _, row in cameras_data.iterrows():\n        # Note that the convention in this tool is for cx, cy to be defined from the center\n        # not the corner so it must be shifted\n        sensor_dict = {\n            \"image_width\": row[\"WIDTH\"],\n            \"image_height\": row[\"HEIGHT\"],\n            \"f\": row[\"PARAMS_F\"],\n            \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n            \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n            \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n        }\n        sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n    # Parse the per-image information\n    cam_to_world_transforms = []\n    sensor_IDs = []\n    image_filenames = []\n\n    for _, row in images_data.iterrows():\n        # Convert from the quaternion representation to the matrix one. Note that the W element\n        # is the first one in the COLMAP convention but the last one in scipy.\n        rot_mat = Rotation.from_quat(\n            (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n        ).as_matrix()\n        # Get the camera translation\n        translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n        # Create a 4x4 homogenous matrix representing the world_to_cam transform\n        world_to_cam = np.eye(4)\n        # Populate the sub-elements\n        world_to_cam[:3, :3] = rot_mat\n        world_to_cam[:3, 3] = translation_vec\n        # We need the cam to world transform. Since we're using a 4x4 representation, we can\n        # just invert the matrix\n        cam_to_world = np.linalg.inv(world_to_cam)\n        cam_to_world_transforms.append(cam_to_world)\n\n        # Record which camera model is used and the image filename\n        sensor_IDs.append(row[\"CAMERA_ID\"])\n        image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n    # Instantiate the camera set\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        sensor_IDs=sensor_IDs,\n        image_folder=image_folder,\n        validate_images=validate_images,\n    )\n</code></pre>"},{"location":"API_reference/cameras/segmentor/","title":"Segmentor Docstrings","text":""},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet","title":"<code>SegmentorPhotogrammetryCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>class SegmentorPhotogrammetryCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        base_camera_set: PhotogrammetryCameraSet,\n        segmentor: Segmentor,\n        dont_load_base_image: bool = True,\n    ):\n        \"\"\"Wraps a camera set to provide segmented versions of the image\n\n        Args:\n            base_camera_set (PhotogrammetryCameraSet): The original camera set\n            segmentor (Segmentor): A fully instantiated segmentor\n        \"\"\"\n        self.base_camera_set = base_camera_set\n        self.segmentor = segmentor\n        self.dont_load_base_image = dont_load_base_image\n\n        # This should allow all un-overridden methods to work as expected\n        self.cameras = self.base_camera_set.cameras\n        self._local_to_epsg_4978_transform = (\n            self.base_camera_set._local_to_epsg_4978_transform\n        )\n\n    def get_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        if self.dont_load_base_image:\n            raw_image = None\n        else:\n            raw_image = self.base_camera_set.get_image_by_index(index, image_scale)\n        image_filename = self.base_camera_set.get_image_filename(index, absolute=True)\n        segmented_image = self.segmentor.segment_image(\n            raw_image, filename=image_filename, image_scale=image_scale\n        )\n        return segmented_image\n\n    def get_raw_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        return self.base_camera_set.get_image_by_index(\n            index=index, image_scale=image_scale\n        )\n\n    def get_subset_cameras(self, inds: typing.List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set.cameras[i] for i in inds]\n        subset_camera_set.base_camera_set = (\n            subset_camera_set.base_camera_set.get_subset_cameras(inds)\n        )\n        return subset_camera_set\n\n    def n_image_channels(self) -&gt; int:\n        return self.segmentor.num_classes\n\n    def get_subset_with_valid_segmentation(self) -&gt; \"SegmentorPhotogrammetryCameraSet\":\n        \"\"\"Get a new camera set consisting of all images that have a valid segmentation result\n\n        Returns:\n            SegmentorPhotogrammetryCameraSet: The subset of cameras with valid segmentation\n        \"\"\"\n        valid_inds = []\n        for i in range(len(self)):\n            try:\n                # Try to get the segmented result\n                self.get_image_by_index(i)\n                # If successful, append it to the list of valid IDs\n                valid_inds.append(i)\n            except:\n                pass\n        # Return the valid subset\n        return self.get_subset_cameras(valid_inds)\n</code></pre>"},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet.__init__","title":"<code>__init__(base_camera_set, segmentor, dont_load_base_image=True)</code>","text":"<p>Wraps a camera set to provide segmented versions of the image</p> <p>Parameters:</p> Name Type Description Default <code>base_camera_set</code> <code>PhotogrammetryCameraSet</code> <p>The original camera set</p> required <code>segmentor</code> <code>Segmentor</code> <p>A fully instantiated segmentor</p> required Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>def __init__(\n    self,\n    base_camera_set: PhotogrammetryCameraSet,\n    segmentor: Segmentor,\n    dont_load_base_image: bool = True,\n):\n    \"\"\"Wraps a camera set to provide segmented versions of the image\n\n    Args:\n        base_camera_set (PhotogrammetryCameraSet): The original camera set\n        segmentor (Segmentor): A fully instantiated segmentor\n    \"\"\"\n    self.base_camera_set = base_camera_set\n    self.segmentor = segmentor\n    self.dont_load_base_image = dont_load_base_image\n\n    # This should allow all un-overridden methods to work as expected\n    self.cameras = self.base_camera_set.cameras\n    self._local_to_epsg_4978_transform = (\n        self.base_camera_set._local_to_epsg_4978_transform\n    )\n</code></pre>"},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet.get_subset_with_valid_segmentation","title":"<code>get_subset_with_valid_segmentation()</code>","text":"<p>Get a new camera set consisting of all images that have a valid segmentation result</p> <p>Returns:</p> Name Type Description <code>SegmentorPhotogrammetryCameraSet</code> <code>SegmentorPhotogrammetryCameraSet</code> <p>The subset of cameras with valid segmentation</p> Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>def get_subset_with_valid_segmentation(self) -&gt; \"SegmentorPhotogrammetryCameraSet\":\n    \"\"\"Get a new camera set consisting of all images that have a valid segmentation result\n\n    Returns:\n        SegmentorPhotogrammetryCameraSet: The subset of cameras with valid segmentation\n    \"\"\"\n    valid_inds = []\n    for i in range(len(self)):\n        try:\n            # Try to get the segmented result\n            self.get_image_by_index(i)\n            # If successful, append it to the list of valid IDs\n            valid_inds.append(i)\n        except:\n            pass\n    # Return the valid subset\n    return self.get_subset_cameras(valid_inds)\n</code></pre>"},{"location":"API_reference/meshes/","title":"Meshes","text":"<ul> <li> <p>Derived Meshes Docstrings</p> </li> <li> <p>Meshes Docstrings</p> </li> </ul>"},{"location":"API_reference/meshes/derived_meshes/","title":"Derived Mesh Docstrings","text":""},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked","title":"<code>TexturedPhotogrammetryMeshChunked</code>","text":"<p>               Bases: <code>TexturedPhotogrammetryMesh</code></p> <p>Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>class TexturedPhotogrammetryMeshChunked(TexturedPhotogrammetryMesh):\n    \"\"\"Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes\"\"\"\n\n    def get_mesh_chunks_for_cameras(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        include_texture: bool = False,\n    ):\n        \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The chunks of the mesh are generated by clustering the cameras\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n            include_texture (bool, optional): Should the texture from the full mesh be included\n                in the subset mesh. Defaults to False.\n\n        Yields:\n            pv.PolyData: The subset mesh\n            PhotogrammetryCameraSet: The cameras associated with that mesh\n            np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n        \"\"\"\n        # Extract the points depending on whether it's a single camera or a set\n        if isinstance(cameras, PhotogrammetryCamera):\n            camera_points = [Point(*cameras.get_lon_lat())]\n        else:\n            # Get the lat lon for each camera point and turn into a shapely Point\n            camera_points = [\n                Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n            ]\n\n        # Create a geodataframe from the points\n        camera_points = gpd.GeoDataFrame(\n            geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n        )\n        # Make sure the gdf has a gemetric CRS so there is no warping of the space\n        camera_points = ensure_projected_CRS(camera_points)\n        # Extract the x, y points now in a geometric CRS\n        camera_points_numpy = np.stack(\n            camera_points.geometry.apply(lambda point: (point.x, point.y))\n        )\n\n        # Assign each camera to a cluster\n        camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n            camera_points_numpy\n        )\n        if vis_clusters:\n            # Show the camera locations, colored by which one they were assigned to\n            plt.scatter(\n                camera_points_numpy[:, 0],\n                camera_points_numpy[:, 1],\n                c=camera_cluster_IDs,\n                cmap=\"tab20\",\n            )\n            plt.show()\n\n        # Get the texture from the full mesh\n        full_mesh_texture = (\n            self.get_texture(request_vertex_texture=False) if include_texture else None\n        )\n\n        # Iterate over the clusters of cameras\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n            # Get indices of cameras for that cluster\n            matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n            # Get the segmentor camera set for the subset of the camera inds\n            sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n            # Extract the rows in the dataframe for those IDs\n            subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n            # TODO this could be accellerated by computing the membership for all points at the begining.\n            # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n            # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n            # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n            # ROIs may actually lead to slower computations than doing it sequentially\n\n            # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n            sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n                region_of_interest=subset_camera_points,\n                buffer_meters=buffer_dist_meters,\n                return_original_IDs=True,\n            )\n            # Extract the corresponding texture elements for this sub mesh if needed\n            # If include_texture=False, the full_mesh_texture will not be set\n            # If there is no mesh, the texture should also be set to None, otherwise it will be\n            # ambigious whether it's a face or vertex texture\n            sub_mesh_texture = (\n                full_mesh_texture[face_IDs]\n                if full_mesh_texture is not None and len(face_IDs) &gt; 0\n                else None\n            )\n\n            # Wrap this pyvista mesh in a photogrammetry mesh\n            sub_mesh_TPM = TexturedPhotogrammetryMesh(\n                sub_mesh_pv, texture=sub_mesh_texture, input_CRS=self.CRS\n            )\n\n            # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n            # face IDs mapping the faces in the sub mesh back to the full one\n            yield sub_mesh_TPM, sub_camera_set, face_IDs\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output.\n        This version first clusters the cameras, extracts a region of the mesh surrounding each\n        cluster of cameras, and then performs rendering on each sub-region.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n            n_clusters (int, optional):\n                Number of clusters to break the cameras into. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                How far around the cameras to include the mesh. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the clusters of camera locations be shown. Defaults to False.\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        # Create a generator to chunked meshes based on clusters of cameras\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n            include_texture=True,\n        )\n\n        for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n            chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n        ):\n            # Create the render generator\n            render_gen = sub_mesh_TPM.render_flat(\n                sub_camera_set,\n                batch_size=batch_size,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n            # Yield items from the returned generator\n            for render_item in render_gen:\n                yield render_item\n\n            # This is another attempt to free memory\n            sub_mesh_TPM.pix2face_plotter.deep_clean()\n            # There's a strange memory leak, I think it may be because of the sub-mesh sticking around\n            print(\"About to delete sub mesh\")\n            del sub_mesh_TPM\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n        mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 250.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n\n        # Initialize the values that will be incremented per cluster\n        summed_projections = np.zeros(\n            (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n        )\n        projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n        # Create a generator to generate chunked meshes\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n        )\n\n        # Iterate over chunks in the mesh\n        for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n            # This means there was no mesh for these cameras\n            if len(face_IDs) == 0:\n                continue\n\n            # Aggregate the projections from a set of cameras corresponding to\n            _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n                sub_camera_set,\n                batch_size=batch_size,\n                aggregate_img_scale=aggregate_img_scale,\n                return_all=False,\n                **kwargs,\n            )\n\n            # Increment the summed predictions and counts\n            # Make sure that nans don't propogate, since they should just be treated as zeros\n            # TODO ensure this is correct\n            summed_projections[face_IDs] = np.nansum(\n                [\n                    summed_projections[face_IDs],\n                    additional_information_submesh[\"summed_projections\"],\n                ],\n                axis=0,\n            )\n            projection_counts[face_IDs] = (\n                projection_counts[face_IDs]\n                + additional_information_submesh[\"projection_counts\"]\n            )\n\n        # Same as the parent class\n        no_projections = projection_counts == 0\n        summed_projections[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projections,\n        }\n\n        average_projections = np.divide(\n            summed_projections, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2,\n        n_polygons_per_cluster: int = 1000,\n    ):\n        \"\"\"\n        Assign a class label to polygons using labels per face. This implementation is useful for\n        large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n        implementation first clusters the polygons and labels each cluster indepenently. This makes\n        use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n        Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n        better performance.\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n            n_polygons_per_cluster: (int):\n                Set the number of clusters so there are approximately this number polygons per\n                cluster on average. Defaults to 1000\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Load in the polygons\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract the centroid of each one and convert to a numpy array\n        centroids_xy = np.stack(\n            polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n        )\n        # Determine how many clusters there should be\n        n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n        # Assign each polygon to a cluster\n        polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n        # This will be set later once we figure out the datatype of the per-cluster labels\n        all_labels = None\n\n        # Loop over the individual clusters\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n            # Determine which polygons are part of that cluster\n            cluster_mask = polygon_cluster_IDs == cluster_ID\n            # Extract the polygons from one cluster\n            cluster_polygons = polygons_gdf.iloc[cluster_mask]\n            # Compute the labeling per polygon\n            cluster_labels = super().label_polygons(\n                face_labels,\n                cluster_polygons,\n                face_weighting,\n                sjoin_overlay,\n                return_class_labels,\n                unknown_class_label,\n                buffer_dist_meters,\n            )\n            # Convert to numpy array\n            cluster_labels = np.array(cluster_labels)\n            # Create the aggregation array with the appropriate datatype\n            if all_labels is None:\n                # We assume that this list will be at least one element since each cluster\n                # should be non-empty. All values should be overwritten so the default value doesn't matter\n                all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n            # Set the appropriate elements of the full array with the newly-computed cluster labels\n            all_labels[cluster_mask] = cluster_labels\n\n        # The output is expected to be a list\n        all_labels = all_labels.tolist()\n        return all_labels\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked-functions","title":"Functions","text":""},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges. This version chunks the mesh up and performs aggregation on sub-regions to decrease the runtime.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 250.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n    mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 250.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n\n    # Initialize the values that will be incremented per cluster\n    summed_projections = np.zeros(\n        (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n    )\n    projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n    # Create a generator to generate chunked meshes\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n    )\n\n    # Iterate over chunks in the mesh\n    for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n        # This means there was no mesh for these cameras\n        if len(face_IDs) == 0:\n            continue\n\n        # Aggregate the projections from a set of cameras corresponding to\n        _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n            sub_camera_set,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            return_all=False,\n            **kwargs,\n        )\n\n        # Increment the summed predictions and counts\n        # Make sure that nans don't propogate, since they should just be treated as zeros\n        # TODO ensure this is correct\n        summed_projections[face_IDs] = np.nansum(\n            [\n                summed_projections[face_IDs],\n                additional_information_submesh[\"summed_projections\"],\n            ],\n            axis=0,\n        )\n        projection_counts[face_IDs] = (\n            projection_counts[face_IDs]\n            + additional_information_submesh[\"projection_counts\"]\n        )\n\n    # Same as the parent class\n    no_projections = projection_counts == 0\n    summed_projections[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projections,\n    }\n\n    average_projections = np.divide(\n        summed_projections, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.get_mesh_chunks_for_cameras","title":"<code>get_mesh_chunks_for_cameras(cameras, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, include_texture=False)</code>","text":"<p>Return a generator of sub-meshes, chunked to align with clusters of cameras</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The chunks of the mesh are generated by clustering the cameras</p> required <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 50.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <code>include_texture</code> <code>bool</code> <p>Should the texture from the full mesh be included in the subset mesh. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <p>pv.PolyData: The subset mesh</p> <code>PhotogrammetryCameraSet</code> <p>The cameras associated with that mesh</p> <p>np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def get_mesh_chunks_for_cameras(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    include_texture: bool = False,\n):\n    \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The chunks of the mesh are generated by clustering the cameras\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n        include_texture (bool, optional): Should the texture from the full mesh be included\n            in the subset mesh. Defaults to False.\n\n    Yields:\n        pv.PolyData: The subset mesh\n        PhotogrammetryCameraSet: The cameras associated with that mesh\n        np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n    \"\"\"\n    # Extract the points depending on whether it's a single camera or a set\n    if isinstance(cameras, PhotogrammetryCamera):\n        camera_points = [Point(*cameras.get_lon_lat())]\n    else:\n        # Get the lat lon for each camera point and turn into a shapely Point\n        camera_points = [\n            Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n        ]\n\n    # Create a geodataframe from the points\n    camera_points = gpd.GeoDataFrame(\n        geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n    )\n    # Make sure the gdf has a gemetric CRS so there is no warping of the space\n    camera_points = ensure_projected_CRS(camera_points)\n    # Extract the x, y points now in a geometric CRS\n    camera_points_numpy = np.stack(\n        camera_points.geometry.apply(lambda point: (point.x, point.y))\n    )\n\n    # Assign each camera to a cluster\n    camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n        camera_points_numpy\n    )\n    if vis_clusters:\n        # Show the camera locations, colored by which one they were assigned to\n        plt.scatter(\n            camera_points_numpy[:, 0],\n            camera_points_numpy[:, 1],\n            c=camera_cluster_IDs,\n            cmap=\"tab20\",\n        )\n        plt.show()\n\n    # Get the texture from the full mesh\n    full_mesh_texture = (\n        self.get_texture(request_vertex_texture=False) if include_texture else None\n    )\n\n    # Iterate over the clusters of cameras\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n        # Get indices of cameras for that cluster\n        matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n        # Get the segmentor camera set for the subset of the camera inds\n        sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n        # Extract the rows in the dataframe for those IDs\n        subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n        # TODO this could be accellerated by computing the membership for all points at the begining.\n        # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n        # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n        # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n        # ROIs may actually lead to slower computations than doing it sequentially\n\n        # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n        sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n            region_of_interest=subset_camera_points,\n            buffer_meters=buffer_dist_meters,\n            return_original_IDs=True,\n        )\n        # Extract the corresponding texture elements for this sub mesh if needed\n        # If include_texture=False, the full_mesh_texture will not be set\n        # If there is no mesh, the texture should also be set to None, otherwise it will be\n        # ambigious whether it's a face or vertex texture\n        sub_mesh_texture = (\n            full_mesh_texture[face_IDs]\n            if full_mesh_texture is not None and len(face_IDs) &gt; 0\n            else None\n        )\n\n        # Wrap this pyvista mesh in a photogrammetry mesh\n        sub_mesh_TPM = TexturedPhotogrammetryMesh(\n            sub_mesh_pv, texture=sub_mesh_texture, input_CRS=self.CRS\n        )\n\n        # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n        # face IDs mapping the faces in the sub mesh back to the full one\n        yield sub_mesh_TPM, sub_camera_set, face_IDs\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2, n_polygons_per_cluster=1000)</code>","text":"<p>Assign a class label to polygons using labels per face. This implementation is useful for large numbers of polygons. To make the expensive sjoin/overlay more efficient, this implementation first clusters the polygons and labels each cluster indepenently. This makes use of the fact that the mesh faces around this cluster can be extracted relatively quickly. Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to better performance.</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2</code> <code>n_polygons_per_cluster</code> <code>int</code> <p>(int): Set the number of clusters so there are approximately this number polygons per cluster on average. Defaults to 1000</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2,\n    n_polygons_per_cluster: int = 1000,\n):\n    \"\"\"\n    Assign a class label to polygons using labels per face. This implementation is useful for\n    large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n    implementation first clusters the polygons and labels each cluster indepenently. This makes\n    use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n    Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n    better performance.\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n        n_polygons_per_cluster: (int):\n            Set the number of clusters so there are approximately this number polygons per\n            cluster on average. Defaults to 1000\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Load in the polygons\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract the centroid of each one and convert to a numpy array\n    centroids_xy = np.stack(\n        polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n    )\n    # Determine how many clusters there should be\n    n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n    # Assign each polygon to a cluster\n    polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n    # This will be set later once we figure out the datatype of the per-cluster labels\n    all_labels = None\n\n    # Loop over the individual clusters\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n        # Determine which polygons are part of that cluster\n        cluster_mask = polygon_cluster_IDs == cluster_ID\n        # Extract the polygons from one cluster\n        cluster_polygons = polygons_gdf.iloc[cluster_mask]\n        # Compute the labeling per polygon\n        cluster_labels = super().label_polygons(\n            face_labels,\n            cluster_polygons,\n            face_weighting,\n            sjoin_overlay,\n            return_class_labels,\n            unknown_class_label,\n            buffer_dist_meters,\n        )\n        # Convert to numpy array\n        cluster_labels = np.array(cluster_labels)\n        # Create the aggregation array with the appropriate datatype\n        if all_labels is None:\n            # We assume that this list will be at least one element since each cluster\n            # should be non-empty. All values should be overwritten so the default value doesn't matter\n            all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n        # Set the appropriate elements of the full array with the newly-computed cluster labels\n        all_labels[cluster_mask] = cluster_labels\n\n    # The output is expected to be a list\n    all_labels = all_labels.tolist()\n    return all_labels\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output. This version first clusters the cameras, extracts a region of the mesh surrounding each cluster of cameras, and then performs rendering on each sub-region.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters to break the cameras into. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>How far around the cameras to include the mesh. Defaults to 50.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the clusters of camera locations be shown. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output.\n    This version first clusters the cameras, extracts a region of the mesh surrounding each\n    cluster of cameras, and then performs rendering on each sub-region.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n        n_clusters (int, optional):\n            Number of clusters to break the cameras into. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            How far around the cameras to include the mesh. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the clusters of camera locations be shown. Defaults to False.\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    # Create a generator to chunked meshes based on clusters of cameras\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n        include_texture=True,\n    )\n\n    for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n        chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n    ):\n        # Create the render generator\n        render_gen = sub_mesh_TPM.render_flat(\n            sub_camera_set,\n            batch_size=batch_size,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n        # Yield items from the returned generator\n        for render_item in render_gen:\n            yield render_item\n\n        # This is another attempt to free memory\n        sub_mesh_TPM.pix2face_plotter.deep_clean()\n        # There's a strange memory leak, I think it may be because of the sub-mesh sticking around\n        print(\"About to delete sub mesh\")\n        del sub_mesh_TPM\n</code></pre>"},{"location":"API_reference/meshes/meshes/","title":"Mesh Docstrings","text":""},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh","title":"<code>TexturedPhotogrammetryMesh</code>","text":"Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>class TexturedPhotogrammetryMesh:\n    def __init__(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        input_CRS: pyproj.CRS,\n        downsample_target: float = 1.0,\n        texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n        texture_column_name: typing.Union[PATH_TYPE, None] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n        shift: typing.Union[np.ndarray, None] = None,\n        ROI: typing.Union[\n            gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n        ] = None,\n        ROI_buffer_meters: float = 0,\n        log_level: str = \"INFO\",\n    ):\n        \"\"\"\n        An object that represents a geospatial mesh with associated textures and supports various\n        rendering options.\n\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n                Path to the mesh in a filetype readable by pyvista or a pyvista mesh object.\n            input_CRS (pyproj.CRS):\n                The vertex coordinates of the input mesh should be interpreteted in this coordinate\n                references system to georeference them.\n            downsample_target (float, optional):\n                Downsample so this fraction of vertices remain. Defaults to 1.0.\n            texture (typing.Union[PATH_TYPE, np.ndarray, None], optional):\n                Texture or path to one. See more details in `load_texture` documentation. Defaults\n                to None.\n            texture_column_name (typing.Union[PATH_TYPE, None], optional):\n                The column to use as the label for a vector data input. Passed to `load_texture`.\n                Defaults to None.\n            IDs_to_labels (typing.Union[PATH_TYPE, dict, None], optional):\n                dictionary or path to JSON file containing the mapping from integer IDs to string\n                class names. Defaults to None.\n            shift (typing.Union[np.ndarray, None], optional):\n                Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex\n                coordinates by this amount in the input_CRS frame. Defaults to None.\n            ROI (typing.Union[ gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None ], optional):\n                Crop the mesh to this region. For more information see `select_mesh_ROI`. Defaults\n                to None.\n            ROI_buffer_meters (float, optional):\n                Buffer the cropped region by this distance. For more information see\n                `select_mesh_ROI`. Defaults to 0.\n            log_level (str, optional):\n                Controls what severity of messages are logged. Defaults to \"INFO\".\n        \"\"\"\n        self.downsample_target = downsample_target\n\n        self.pyvista_mesh = None\n        self.texture = None\n        self.vertex_texture = None\n        self.face_texture = None\n        self.IDs_to_labels = None\n        # Create the plotter that will later be used to compute correspondences between pixels\n        # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n        # plotters. See https://github.com/pyvista/pyvista/issues/2252\n        self.pix2face_plotter = create_pv_plotter(off_screen=True)\n        self.face_polygons_cache = {}\n        self.face_2d_3d_ratios_cache = {}\n\n        self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n        self.logger.setLevel(log_level)\n        # Potentially necessary for Jupyter\n        # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n        # If you don't check that there's already a handler, you can have situations with duplicated\n        # print outs if you have multiple mesh objects\n        if not self.logger.hasHandlers():\n            self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n        # Load the mesh with the pyvista loader\n        self.logger.info(\"Loading mesh\")\n        self.load_mesh(\n            mesh=mesh,\n            input_CRS=input_CRS,\n            downsample_target=downsample_target,\n            shift=shift,\n            ROI=ROI,\n            ROI_buffer_meters=ROI_buffer_meters,\n        )\n        # Load the texture\n        self.logger.info(\"Loading texture\")\n        # load IDs_to_labels\n        # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n        if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n            possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n            if possible_json.exists():\n                IDs_to_labels = possible_json\n        # convert IDs_to_labels from file to dict\n        if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n            with open(IDs_to_labels, \"r\") as file:\n                IDs_to_labels = json.load(file)\n                IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n        self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n\n    # Setup methods\n    def load_mesh(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        input_CRS: pyproj.CRS,\n        downsample_target: float = 1.0,\n        shift: typing.Union[np.ndarray, None] = None,\n        ROI=None,\n        ROI_buffer_meters=0,\n        ROI_simplify_tol_meters=2,\n    ):\n        \"\"\"Load the pyvista mesh and create the texture\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n                Path to the mesh or actual mesh\n            downsample_target (float, optional):\n                What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n            shift (typing.Union[np.ndarray, None], optional):\n                Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex\n                coordinates by this amount in the input_CRS frame. Defaults to None.\n            ROI:\n                See select_mesh_ROI. Defaults to None\n            ROI_buffer_meters:\n                See select_mesh_ROI. Defaults to 0.\n            ROI_simplify_tol_meters:\n                See select_mesh_ROI. Defaults to 2.\n        \"\"\"\n        self.CRS = input_CRS\n\n        if isinstance(mesh, pv.PolyData):\n            # If a mesh is provided directly, copy it so input mesh isn't modified\n            self.pyvista_mesh = mesh.copy()\n        else:\n            # Load the mesh using pyvista\n            # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n            self.logger.info(\"Reading the mesh\")\n            self.pyvista_mesh = pv.read(mesh)\n\n        # Up-cast to avoid quantization errors after we shift or transform to larger values\n        self.pyvista_mesh.points = self.pyvista_mesh.points.astype(float)\n\n        # If a shift is provided, shift all mesh vertices by this amount\n        if shift is not None:\n            self.pyvista_mesh.points += shift\n\n        self.logger.info(\"Selecting an ROI from mesh\")\n        # Select a region of interest if needed\n        self.pyvista_mesh = self.select_mesh_ROI(\n            region_of_interest=ROI,\n            buffer_meters=ROI_buffer_meters,\n            simplify_tol_meters=ROI_simplify_tol_meters,\n        )\n\n        # Reproject to a meters-based CRS. TODO consider if there's a better option than ECEF.\n        self.reproject_CRS(target_CRS=EARTH_CENTERED_EARTH_FIXED_CRS, inplace=True)\n\n        # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n        if downsample_target != 1.0:\n            # TODO try decimate_pro and compare quality and runtime\n            # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n            self.logger.info(\"Downsampling the mesh\")\n            # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n            downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n                target_reduction=(1 - downsample_target)\n            )\n            self.logger.info(\n                f\"Requested downsampling {downsample_target}, actual downsampling {downsampled_mesh_without_textures.n_points / self.pyvista_mesh.n_points}\"\n            )\n            self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n        self.logger.info(\"Extracting faces from mesh\")\n        # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n        self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n\n    def reproject_CRS(\n        self, target_CRS: pyproj.CRS, inplace: bool = True\n    ) -&gt; typing.Optional[pv.PolyData]:\n        \"\"\"\n        Convert the mesh into a new coordinate reference system. This is done by updating the\n        location of each vertex using the mappings between the current coordinate reference system\n        and the requested one, as implemented in pyproj.\n\n        Note that if the CRS of the mesh is None, this operation will do nothing and the original\n        vertex values will be returned un-transformed.\n\n        Args:\n            target_CRS (pyproj.CRS): The coordinate reference system to transform the mesh to.\n            inplace (bool, optional): Should the self.pyvista_mesh and self.CRS attributes be\n            updated. Otherwise, an updated copy of the mesh is returned and the original is left\n            unchanged. Defaults to True.\n\n        Returns:\n            (pv.PolyData, optional): If `inplace==False`, a transformed pyvista mesh will be returned\n        \"\"\"\n        # Check if the mesh has a valid CRS\n        if self.CRS is None:\n            self.logger.warning(\"mesh CRS is None, reproject_CRS is doing nothing\")\n            # If not, just return the original coordinates as if they had been transformed\n            verts_in_output_CRS = np.array(self.pyvista_mesh.points)\n        else:\n            # Build a pyproj transfrormer from the current to the desired CRS\n            transformer = pyproj.Transformer.from_crs(self.CRS, target_CRS)\n\n            # Convert the mesh vertices to a numpy array\n            mesh_verts = np.array(self.pyvista_mesh.points)\n\n            # Transform the coordinates\n            verts_in_output_CRS = transformer.transform(\n                xx=mesh_verts[:, 0],\n                yy=mesh_verts[:, 1],\n                zz=mesh_verts[:, 2],\n            )\n            # Stack and transpose\n            verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n            # TODO figure out how to deal with the fact that this may no longer be a right-handed coordinate system\n            # See comment in `get_vertices_in_CRS`\n\n        if inplace:\n            # Update the CRS\n            self.CRS = target_CRS\n            # Update the mesh points\n            self.pyvista_mesh.points = pv.pyvista_ndarray(verts_in_output_CRS)\n        else:\n            # Create a copy of the mesh\n            copied_mesh = self.pyvista_mesh.copy(deep=True)\n            # Update its points\n            copied_mesh.points = pv.pyvista_ndarray(verts_in_output_CRS)\n            # Return the updated copy\n            return copied_mesh\n\n    def transfer_texture(self, downsampled_mesh):\n        \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n        Args:\n            downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n        Returns:\n            pv.PolyData: The downsampled mesh with the transferred textures\n        \"\"\"\n        # Only transfer textures if there are point based scalars in the original mesh\n        if self.pyvista_mesh.point_data:\n            # Store original mesh points in KDTree for nearest neighbor search\n            kdtree = KDTree(self.pyvista_mesh.points)\n\n            # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n            _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n            # Iterate over all the point based scalars\n            for scalar_name in self.pyvista_mesh.point_data.keys():\n                # Retrieve scalar data of appropriate index using the nearest neighbor indices\n                transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                    nearest_neighbor_indices\n                ]\n                # Set the corresponding scalar data in the downsampled mesh\n                downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n            # Set active mesh of downsampled mesh\n            if self.pyvista_mesh.active_scalars_name:\n                downsampled_mesh.active_scalars_name = (\n                    self.pyvista_mesh.active_scalars_name\n                )\n        else:\n            self.logger.warning(\n                \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n            )\n        return downsampled_mesh\n\n    def standardize_texture(self, texture_array: np.ndarray):\n        # TODO consider coercing into a numpy array\n\n        # Check the dimensions\n        if texture_array.ndim == 1:\n            texture_array = np.expand_dims(texture_array, axis=1)\n        elif texture_array.ndim != 2:\n            raise ValueError(\n                f\"Input texture should have 1 or 2 dimensions but instead has {texture_array.ndim}\"\n            )\n        return texture_array\n\n    def get_texture(\n        self,\n        request_vertex_texture: typing.Union[bool, None] = None,\n        try_verts_faces_conversion: bool = True,\n    ):\n        if self.vertex_texture is None and self.face_texture is None:\n            return\n\n        # If this is unset, try to infer it\n        if request_vertex_texture is None:\n            if self.vertex_texture is not None and self.face_texture is not None:\n                raise ValueError(\n                    \"Ambigious which texture is requested, set request_vertex_texture appropriately\"\n                )\n\n            # Assume that the only one available is being requested\n            request_vertex_texture = self.vertex_texture is not None\n\n        if request_vertex_texture:\n            if self.vertex_texture is not None:\n                return self.standardize_texture(self.vertex_texture)\n            elif try_verts_faces_conversion:\n                self.set_texture(self.face_to_vert_texture(self.face_texture))\n                self.vertex_texture\n            else:\n                raise ValueError(\n                    \"Vertex texture not present and conversion was not requested\"\n                )\n        else:\n            if self.face_texture is not None:\n                return self.standardize_texture(self.face_texture)\n            elif try_verts_faces_conversion:\n                face_texture = self.vert_to_face_texture(\n                    self.vertex_texture, discrete=self.is_discrete_texture()\n                )\n                self.set_texture(face_texture)\n                return self.face_texture\n            else:\n                raise ValueError(\n                    \"Face texture not present and conversion was not requested\"\n                )\n\n    def is_discrete_texture(self):\n        return self.IDs_to_labels is not None\n\n    def set_texture(\n        self,\n        texture_array: np.ndarray,\n        IDs_to_labels: typing.Union[None, dict] = None,\n        all_discrete_texture_values: typing.Union[typing.List, None] = None,\n        is_vertex_texture: typing.Union[bool, None] = None,\n        delete_existing: bool = True,\n        update_IDs_to_labels: bool = True,\n    ):\n        \"\"\"Set the internal texture representation\n\n        Args:\n            texture_array (np.ndarray):\n                The array of texture values. The first dimension must be the length of faces or\n                verts. A second dimension is optional.\n            IDs_to_labels (typing.Union[None, dict], optional):\n                Mapping from integer IDs to string names. Defaults to None.\n            all_discrete_texture_values (typing.Union[typing.List, None], optional):\n                Are all the texture values known to be discrete, representing IDs. Computed from\n                the data if not set. Defaults to None.\n            is_vertex_texture (typing.Union[bool, None], optional):\n                Are the texture values supposed to correspond to the vertices. Computed from the\n                data if not set. Defaults to None.\n            delete_existing (bool, optional):\n                Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n            update_IDs_to_labels (bool, optional):\n                Should IDs to labels be updated based on either the provided IDs_to_labels or the\n                derived ones. Defaults to True.\n\n        Raises:\n            ValueError: If the size of the texture doesn't match the number of either faces or vertices\n            ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n        \"\"\"\n        # Ensure that the texture is 2D and a numpy array\n        texture_array = self.standardize_texture(texture_array)\n\n        if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n            # If it is more than one column, it's assumed to be a real-valued\n            # quantity and we try to cast it to a float\n            texture_array = texture_array.astype(float)\n            self.IDs_to_labels = None\n        else:\n            if IDs_to_labels is None:\n                texture_array, derived_IDs_to_labels = ensure_float_labels(\n                    texture_array, full_array=all_discrete_texture_values\n                )\n                # If requested, record these new IDs_to_labels\n                if update_IDs_to_labels:\n                    self.IDs_to_labels = derived_IDs_to_labels\n            else:\n                # Create the inverse mapping, returning nan for anything not in it\n                labels_to_IDs = defaultdict(lambda: np.nan)\n                labels_to_IDs.update({v: k for k, v in IDs_to_labels.items()})\n\n                # Ensure the mapping is 1-to-1, that there are no collisions in the mapping\n                if len(labels_to_IDs) != len(IDs_to_labels):\n                    raise ValueError(\"IDs_to_labels is not a one-to-one mapping\")\n\n                # Check that the mapping only produces ints\n                if not np.all([isinstance(v, int) for v in labels_to_IDs.values()]):\n                    raise ValueError(\n                        \"The labels to IDs mapping does not produce only floats\"\n                    )\n\n                # Perform the mapping\n                texture_array = np.array(\n                    [labels_to_IDs[l] for l in texture_array.squeeze()]\n                )\n                # Reinstate the squeezed dimension\n                texture_array = np.expand_dims(texture_array, axis=1)\n\n                # If requested, record these IDs to labels\n                if update_IDs_to_labels:\n                    self.IDs_to_labels = IDs_to_labels\n\n        # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n        # TODO consider refactoring to check whether it matches the number of one of them,\n        # no matter whether is_vertex_texture is specified\n        if is_vertex_texture is None:\n            # Check that the number of matches face or verts\n            n_values = texture_array.shape[0]\n            n_faces = self.faces.shape[0]\n            n_verts = self.pyvista_mesh.points.shape[0]\n\n            if n_verts == n_faces:\n                raise ValueError(\n                    \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n                )\n            elif n_values == n_verts:\n                is_vertex_texture = True\n            elif n_values == n_faces:\n                is_vertex_texture = False\n            else:\n                raise ValueError(\n                    f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n                )\n\n        # Set the appropriate texture and optionally delete the other one\n        if is_vertex_texture:\n            self.vertex_texture = texture_array\n            if delete_existing:\n                self.face_texture = None\n        else:\n            self.face_texture = texture_array\n            if delete_existing:\n                self.vertex_texture = None\n\n    def load_texture(\n        self,\n        texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n        texture_column_name: typing.Union[None, PATH_TYPE] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    ):\n        \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n           one will be left as None\n\n        Args:\n            texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n                * A numpy array file in \".npy\" format\n                * A vector file readable by geopandas and a label(s) specifying which column to use.\n                  This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n                  regions with different labels. These regions may be assigned based on the order of the rows.\n                * A raster file readable by rasterio. We may want to support using a subset of bands\n            texture_column_name: The column to use as the label for a vector data input\n            IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n        \"\"\"\n        # The easy case, a texture is passed in directly\n        if isinstance(texture, np.ndarray):\n            self.set_texture(\n                texture_array=texture,\n                IDs_to_labels=IDs_to_labels,\n            )\n        # If the texture is None, try to load it from the mesh\n        # Note that this requires us to have not decimated yet\n        elif texture is None:\n            # See if the mesh has a texture, else this will be None\n            texture_array = self.pyvista_mesh.active_scalars\n\n            if texture_array is not None:\n                # Check if this was a really one channel that had to be tiled to\n                # three for saving\n                if len(texture_array.shape) == 2:\n                    min_val_per_row = np.min(texture_array, axis=1)\n                    max_val_per_row = np.max(texture_array, axis=1)\n                    if np.array_equal(min_val_per_row, max_val_per_row):\n                        # This is supposted to be one channel\n                        texture_array = texture_array[:, 0].astype(float)\n                        # Set any values that are the ignore int value to nan\n                texture_array = texture_array.astype(float)\n                texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n                self.set_texture(\n                    texture_array,\n                    IDs_to_labels=IDs_to_labels,\n                )\n            else:\n                if IDs_to_labels is not None:\n                    self.IDs_to_labels = IDs_to_labels\n                # Assume that no texture will be needed, consider printing a warning\n                self.logger.warn(\"No texture provided\")\n        else:\n            # Try handling all the other supported filetypes\n            texture_array = None\n            all_values = None\n\n            # Name of scalar in the mesh\n            try:\n                self.logger.warn(\n                    \"Trying to read texture as a scalar from the pyvista mesh:\"\n                )\n                texture_array = self.pyvista_mesh[str(texture)]\n                self.logger.warn(\"- success\")\n            except (KeyError, ValueError):\n                self.logger.warn(\"- failed\")\n\n            # Numpy file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as a numpy file:\")\n                    texture_array = np.load(texture, allow_pickle=True)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Vector file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as vector file:\")\n                    # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                    texture_array, all_values = self.get_values_for_verts_from_vector(\n                        column_names=texture_column_name,\n                        vector_source=texture,\n                    )\n                    self.logger.warn(\"- success\")\n                except (IndexError, fiona.errors.DriverError):\n                    self.logger.warn(\"- failed\")\n\n            # Raster file\n            if texture_array is None:\n                try:\n                    # TODO\n                    self.logger.warn(\"Trying to read as texture as raster file: \")\n                    texture_array = self.get_vert_values_from_raster_file(texture)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Error out if not set, since we assume the intent was to have a texture at this point\n            if texture_array is None:\n                raise ValueError(f\"Could not load texture for {texture}\")\n\n            # This will error if something is wrong with the texture that was loaded\n            self.set_texture(\n                texture_array,\n                all_discrete_texture_values=all_values,\n                IDs_to_labels=IDs_to_labels,\n            )\n\n    def select_mesh_ROI(\n        self,\n        region_of_interest: typing.Union[\n            gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n        ],\n        buffer_meters: float = 0,\n        simplify_tol_meters: int = 0,\n        default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n        return_original_IDs: bool = False,\n    ):\n        \"\"\"Get a subset of the mesh based on geospatial data\n\n        Args:\n            region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n                Region of interest. Can be a\n                * dataframe, where all columns will be colapsed\n                * A shapely polygon/multipolygon\n                * A file that can be loaded by geopandas\n            buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n            simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n            default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n            return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n        Returns:\n            pyvista.PolyData: The subset of the mesh\n            np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n            np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n        \"\"\"\n        if region_of_interest is None:\n            return self.pyvista_mesh\n\n        # Get the ROI into a geopandas GeoDataFrame\n        self.logger.info(\"Standardizing ROI\")\n        if isinstance(region_of_interest, gpd.GeoDataFrame):\n            ROI_gpd = region_of_interest\n        elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n            ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n        else:\n            ROI_gpd = gpd.read_file(region_of_interest)\n\n        self.logger.info(\"Dissolving ROI\")\n        # Disolve to ensure there is only one row\n        ROI_gpd = ROI_gpd.dissolve()\n        self.logger.info(\"Setting CRS and buffering ROI\")\n        # Make sure we're using a projected CRS so a buffer can be applied\n        ROI_gpd = ensure_projected_CRS(ROI_gpd)\n        # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n        ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n        # Simplify the geometry to reduce the computational load\n        ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n        self.logger.info(\"Dissolving buffered ROI\")\n        # Disolve again in case\n        ROI_gpd = ROI_gpd.dissolve()\n\n        self.logger.info(\"Extracting verts for dataframe\")\n        # Get the vertices as a dataframe in the same CRS\n        verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n        self.logger.info(\"Checking intersection of verts with ROI\")\n        # Determine which vertices are within the ROI polygon\n        verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n        # Extract the IDs of the set within the polygon\n        vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n        self.logger.info(\"Extracting points from pyvista mesh\")\n        # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n        subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n        self.logger.info(\"Extraction surface from subset mesh\")\n        # Convert the unstructured grid to a PolyData (mesh) again\n        subset_mesh = subset_unstructured_grid.extract_surface()\n\n        # If we need the indices into the original mesh, return those\n        if return_original_IDs:\n            try:\n                point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n                face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n            except KeyError:\n                point_IDs = np.array([])\n                face_IDs = np.array([])\n\n            return (\n                subset_mesh,\n                point_IDs,\n                face_IDs,\n            )\n        # Else return just the mesh\n        return subset_mesh\n\n    def add_label(self, label_name, label_ID):\n        if label_ID is not np.nan:\n            self.IDs_to_labels[label_ID] = label_name\n\n    def get_IDs_to_labels(self):\n        # Convert to int type to avoid json serialization issues\n        if self.IDs_to_labels is None:\n            return None\n        return {int(k): v for k, v in self.IDs_to_labels.items()}\n\n    def get_label_names(self):\n        self.logger.warning(\n            \"This method will be deprecated in favor of get_IDs_to_labels since it doesn't handle non-sequential indices\"\n        )\n        if self.IDs_to_labels is None:\n            return None\n        return list(self.IDs_to_labels.values())\n\n    # Vertex methods\n    def get_vertices_in_CRS(\n        self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n    ):\n        \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n        Args:\n            output_CRS (pyproj.CRS): The coordinate reference system to transform to\n            force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n        Returns:\n            np.ndarray: (n_points, 3)\n        \"\"\"\n        # Reproject the mesh\n        reprojected_mesh = self.reproject_CRS(output_CRS, inplace=False)\n        verts_in_output_CRS = np.array(reprojected_mesh.points)\n\n        # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n        # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n        # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n        if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n            # Swap first two columns\n            verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n        return verts_in_output_CRS\n\n    def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n        \"\"\"Obtain the vertices as a dataframe\n\n        Args:\n            crs (pyproj.CRS): The CRS to use\n\n        Returns:\n            gpd.GeoDataFrame: A dataframe with all the vertices\n        \"\"\"\n        # Get the vertices in the same CRS as the geofile\n        verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n        df = pd.DataFrame(\n            {\n                \"east\": verts_in_geopolygon_crs[:, 0],\n                \"north\": verts_in_geopolygon_crs[:, 1],\n            }\n        )\n        # Create a column of Point objects to use as the geometry\n        df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n        points = gpd.GeoDataFrame(df, crs=crs)\n\n        # Add an index column because the normal index will not be preserved in future operations\n        points[VERT_ID] = df.index\n\n        return points\n\n    def get_faces_2d_gdf(\n        self,\n        crs: pyproj.CRS,\n        include_3d_2d_ratio: bool = False,\n        data_dict: dict = {},\n        faces_mask: typing.Union[np.ndarray, None] = None,\n        cache_data: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n        Args:\n            crs (pyproj.CRS):\n                Coordinate reference system of the dataframe\n            include_3d_2d_ratio (bool, optional):\n                Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n                slope of the face relative to horizontal. The computed data will be stored in the\n                column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n            data_dict (dict, optional):\n                Additional information to add to the dataframe. It must be a dict where the keys\n                are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n                Defaults to {}.\n            faces_mask (typing.Union[np.ndarray, None], optional):\n                A binary mask corresponding to which faces to return. Used to improve runtime of\n                creating the dataframe or downstream steps. Defaults to None.\n            cache_data (bool):\n                Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n        Returns:\n            geopandas.GeoDataFrame: A dataframe for each triangular face\n        \"\"\"\n        # Computing this data can be slow, and we might call it multiple times. This is especially\n        # true for doing clustered polygon labeling\n        if cache_data:\n            mesh_hash = self.get_mesh_hash()\n            faces_mask_hash = hash(\n                faces_mask.tobytes() if faces_mask is not None else 0\n            )\n            # Create a key that uniquely identifies the relavant inputs\n            cache_key = (mesh_hash, faces_mask_hash, crs)\n\n            # See if the face polygons were in the cache. If not, None will be returned\n            cached_values = self.face_polygons_cache.get(cache_key)\n        else:\n            cached_values = None\n\n        if cached_values is not None:\n            face_polygons, faces = cached_values\n            logging.info(\"Using cached face polygons\")\n        else:\n            self.logger.info(\"Computing faces in working CRS\")\n            # Get the mesh vertices in the desired export CRS\n            verts_in_crs = self.get_vertices_in_CRS(crs)\n            # Get a triangle in geospatial coords for each face\n            # (n_faces, 3 points, xyz)\n            faces = verts_in_crs[self.faces]\n\n            # Select only the requested faces\n            if faces_mask is not None:\n                faces = faces[faces_mask]\n\n            # Extract the first two columns and convert them to a list of tuples of tuples\n            faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n            face_polygons = [\n                Polygon(face_tuple)\n                for face_tuple in tqdm(\n                    faces_2d_tuples, desc=f\"Converting faces to polygons\"\n                )\n            ]\n            self.logger.info(\"Creating dataframe of faces\")\n\n            if cache_data:\n                # Save computed data to the cache for the future\n                self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n        # Remove data corresponding to masked faces\n        if faces_mask is not None:\n            data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n        # Compute the ratio between the 3D area and the projected top-down 2D area\n        if include_3d_2d_ratio:\n            if cache_data:\n                # Check if ratios are cached\n                ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n            else:\n                ratios = None\n\n            # Ratios need to be computed\n            if ratios is None:\n                ratios = []\n                for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                    area, area_2d = compute_3D_triangle_area(face)\n                    ratios.append(area / area_2d)\n\n                if cache_data:\n                    self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n            # Add the ratios to the data dict\n            data_dict[RATIO_3D_2D_KEY] = ratios\n\n        # Create the dataframe\n        faces_gdf = gpd.GeoDataFrame(\n            data=data_dict,\n            geometry=face_polygons,\n            crs=crs,\n        )\n\n        return faces_gdf\n\n    # Transform labels face&lt;-&gt;vertex methods\n\n    def face_to_vert_texture(self, face_IDs):\n        \"\"\"_summary_\n\n        Args:\n            face_IDs (np.array): (n_faces,) The integer IDs of the faces\n        \"\"\"\n        raise NotImplementedError()\n        # TODO figure how to have a NaN class that\n        for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n            # Find which faces are using this vertex\n            matching = np.sum(self.faces == i, axis=1)\n            # matching_inds = np.where(matching)[0]\n            # matching_IDs = face_IDs[matching_inds]\n            # most_common_ind = Counter(matching_IDs).most_common(1)\n\n    def vert_to_face_texture(self, vert_IDs, discrete=True):\n        if vert_IDs is None:\n            raise ValueError(\"None\")\n\n        vert_IDs = np.squeeze(vert_IDs)\n        if vert_IDs.ndim != 1 and discrete:\n            raise ValueError(\n                f\"Can only perform discrete conversion with one dimensional array but instead had {vert_IDs.ndim}\"\n            )\n\n        # Each row contains the IDs of each vertex\n        values_per_face = vert_IDs[self.faces]\n        if discrete:\n            # Now we need to \"vote\" for the best one\n            max_ID = np.nanmax(vert_IDs)\n            # This means that all textures are nans\n            if not np.isfinite(max_ID):\n                self.logger.warn(\n                    \"In vertex to face texture conversion, all nans encountered\"\n                )\n                # Return all nans\n                return np.full(values_per_face.shape[0], fill_value=np.nan)\n\n            # Compute the most common class per face, doing so by batch to avoid OOM. This is because\n            # this implementation creates a (n_faces, n_classes) array which can grow quite large\n            chunk_size = 100000\n            most_common_class_per_face = np.concatenate(\n                [\n                    fair_mode_non_nan(values_per_face[i : i + chunk_size])\n                    for i in tqdm(\n                        range(0, values_per_face.shape[0], chunk_size),\n                        desc=\"Computing most common class per face by batch\",\n                    )\n                ],\n                axis=0,\n            )\n\n            return most_common_class_per_face\n        else:\n            average_value_per_face = np.mean(values_per_face, axis=1)\n            return average_value_per_face\n\n    # Operations on vector data\n    def get_values_for_verts_from_vector(\n        self,\n        vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n        column_names: typing.Union[str, typing.List[str]],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the value from a dataframe for each vertex\n\n        Args:\n            vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n            column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n        Returns:\n            np.ndarray | dict[str, np.ndarray]:\n                An array or dict of string-&gt;array mappings, with one element per vector file polygon\n            np.ndarray | dict[str, np.ndarray]:\n                An array or dict of string-&gt;array mappings, with one element per mesh vertex\n        \"\"\"\n        # Lead the vector data if not already provided in memory\n        if isinstance(vector_source, gpd.GeoDataFrame):\n            gdf = vector_source\n        else:\n            # This will error if not readable\n            gdf = gpd.read_file(vector_source)\n\n        # Infer or standardize the column names\n        if column_names is None:\n            # Check if there is only one real column\n            if len(gdf.columns) == 2:\n                column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n            else:\n                # Log as well since this may be caught by an exception handler,\n                # and it's a user error that can be corrected\n                self.logger.error(\n                    \"No column name provided and ambigious which column to use\"\n                )\n                raise ValueError(\n                    \"No column name provided and ambigious which column to use\"\n                )\n        # If only one column is provided, make it a one-length list\n        elif isinstance(column_names, str):\n            column_names = [column_names]\n\n        # Get a dataframe of vertices\n        verts_df = self.get_verts_geodataframe(gdf.crs)\n\n        # See which vertices are in the geopolygons\n        points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n        # Get the index array\n        index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n        # This is one entry per vertex\n        labeled_verts_dict = {}\n        all_values_dict = {}\n        # Extract the data from each\n        for column_name in column_names:\n            # Create an array corresponding to all the points and initialize to NaN\n            column_values = points_in_polygons_gdf[column_name]\n            # TODO clean this up\n            if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n                # TODO be set to the default value for the type of the column\n                null_value = \"null\"\n            elif column_values.dtype == int:\n                null_value = 255\n            else:\n                null_value = np.nan\n            # Create an array, one per vertex, with the null value\n            values = np.full(\n                shape=verts_df.shape[0],\n                dtype=column_values.dtype,\n                fill_value=null_value,\n            )\n            # Assign the labeled values\n            values[index_array] = column_values\n\n            # Record the results\n            labeled_verts_dict[column_name] = values\n            all_values_dict[column_name] = gdf[column_name]\n\n        # If only one name was requested, just return that\n        if len(column_names) == 1:\n            labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n            all_values = np.array(list(all_values_dict.values())[0])\n\n            return labeled_verts, all_values\n        # Else return a dict of all requested values\n        return labeled_verts_dict, all_values_dict\n\n    def save_IDs_to_labels(self, savepath: PATH_TYPE):\n        \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n        Args:\n            savepath (PATH_TYPE): path to the file where the data must be saved\n        \"\"\"\n\n        # Save the classes filename\n        ensure_containing_folder(savepath)\n        if self.is_discrete_texture():\n            self.logger.info(\"discrete texture, saving classes\")\n            self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n            try:\n                with open(savepath, \"w\") as outfile_h:\n                    # Try to dump the mapping, falling back on the string encoder for types in the\n                    # dict values that cannot be JSON serialized. This is most common with\n                    # np.int64\n                    json.dump(\n                        self.get_IDs_to_labels(),\n                        outfile_h,\n                        ensure_ascii=False,\n                        indent=4,\n                        default=str,\n                    )\n            except:\n                self.logger.warn(\"Could not serialize IDs_to_labels due to JSON error\")\n        else:\n            self.logger.warn(\"non-discrete texture, not saving classes\")\n\n    def save_mesh(self, savepath: PATH_TYPE, save_vert_texture: bool = True):\n        # TODO consider moving most of this functionality to a utils file\n        if save_vert_texture:\n            vert_texture = self.get_texture(request_vertex_texture=True)\n            n_channels = vert_texture.shape[1]\n\n            if n_channels == 1:\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                vert_texture = np.tile(vert_texture, reps=(1, 3))\n            if n_channels &gt; 3:\n                self.logger.warning(\n                    \"Too many channels to save, attempting to treat them as class probabilities and take the argmax\"\n                )\n                # Take the argmax\n                vert_texture = np.nanargmax(vert_texture, axis=1, keepdims=True)\n                # Replace nan with 255\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                # Expand to the right number of channels\n                vert_texture = np.repeat(vert_texture, repeats=(1, 3))\n\n            vert_texture = vert_texture.astype(np.uint8)\n        else:\n            vert_texture = None\n\n        # Create folder if it doesn't exist\n        ensure_containing_folder(savepath)\n        # Actually save the mesh\n        self.pyvista_mesh.save(savepath, texture=vert_texture)\n        self.save_IDs_to_labels(Path(savepath).stem + \"_IDs_to_labels.json\")\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2.0,\n    ):\n        \"\"\"Assign a class label to polygons using labels per face\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Premptive error checking before expensive operations\n        face_labels = np.squeeze(face_labels)\n        if face_labels.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n            )\n        if face_weighting is not None:\n            face_weighting = np.squeeze(face_weighting)\n            if face_weighting.ndim != 1:\n                raise ValueError(\n                    f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n                )\n\n        # Ensure that the input is a geopandas dataframe\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract just the geometry\n        polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n        # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n        faces_mask = np.isfinite(face_labels)\n\n        # Get the faces of the mesh as a geopandas dataframe\n        # Include the predicted face labels as a column in the dataframe\n        faces_2d_gdf = self.get_faces_2d_gdf(\n            polygons_gdf.crs,\n            include_3d_2d_ratio=True,\n            data_dict={CLASS_ID_KEY: face_labels},\n            faces_mask=faces_mask,\n            cache_data=True,\n        )\n\n        # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n        if face_weighting is not None:\n            face_weighting = face_weighting[faces_mask]\n            faces_2d_gdf[\"face_weighting\"] = (\n                faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n            )\n        # If not, just use the ratio\n        else:\n            faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n        # Set the precision to avoid approximate coliniearity errors\n        faces_2d_gdf.geometry = shapely.set_precision(\n            faces_2d_gdf.geometry.values, 1e-6\n        )\n        polygons_gdf.geometry = shapely.set_precision(\n            polygons_gdf.geometry.values, 1e-6\n        )\n\n        # Set the ID field so it's available after the overlay operation\n        # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n        # one and the index would not start from 0\n        polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n        # Since overlay is expensive, we first discard faces that are not near the polygons\n\n        # Dissolve the polygons to form one ROI\n        merged_polygons = polygons_gdf.dissolve()\n        # Try to decrease the number of elements in the polygon by expanding\n        # and then simplifying the number of elements in the polygon\n        merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n        merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n        # Determine which face IDs intersect the ROI. This is slow\n        start = time()\n        self.logger.info(\"Starting to subset to ROI\")\n\n        # Check which faces are fully within the buffered regions around the query polygons\n        # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n        # to compute more information than something like gpd.within\n        contained_faces = gpd.sjoin(\n            faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n        )[\"index_right\"].notna()\n        faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n        self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n        start = time()\n        self.logger.info(\"Starting `overlay`\")\n        if sjoin_overlay:\n            overlay = gpd.sjoin(\n                faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n            )\n            self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n        else:\n            # Drop faces not included\n            overlay = polygons_gdf.overlay(\n                faces_2d_gdf, how=\"identity\", keep_geom_type=False\n            )\n            self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n        # Drop nan, for geometries that don't intersect the polygons\n        overlay.dropna(inplace=True)\n        # Compute the weighted area for each face, which may have been broken up by the overlay\n        overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n        # Extract only the neccessary columns\n        overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n        aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n        # Compute the highest weighted class prediction\n        # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n        max_rows = aggregated_data.loc[\n            aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n                \"weighted_area\"\n            ].idxmax()\n        ].reset_index()\n\n        # Make the class predictions a list of IDs with nans where no information is available\n        pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n        pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n        predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n        predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n            pred_subset_IDs\n        )\n        predicted_class_IDs = predicted_class_IDs.tolist()\n\n        # Post-process to string label names if requested and IDs_to_labels exists\n        if return_class_labels and (\n            (IDs_to_labels := self.get_IDs_to_labels()) is not None\n        ):\n            # convert the IDs into labels\n            # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n            predicted_class_IDs = [\n                (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n                for pi in predicted_class_IDs\n            ]\n        return predicted_class_IDs\n\n    def export_face_labels_vector(\n        self,\n        face_labels: typing.Union[np.ndarray, None] = None,\n        export_file: PATH_TYPE = None,\n        export_crs: pyproj.CRS = LAT_LON_CRS,\n        label_names: typing.Tuple = None,\n        ensure_non_overlapping: bool = False,\n        simplify_tol: float = 0.0,\n        drop_nan: bool = True,\n        vis: bool = True,\n        batched_unary_union_kwargs: typing.Dict = {\n            \"batch_size\": 500000,\n            \"sort_by_loc\": True,\n            \"grid_size\": 0.05,\n            \"simplify_tol\": 0.05,\n        },\n        vis_kwargs: typing.Dict = {},\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n        Args:\n            face_labels (np.ndarray):\n                This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n                is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n                nonzero element at (i, j) represents a class prediction for the ith faces and jth\n                class\n            export_file (PATH_TYPE, optional):\n                Where to export. The extension must be a filetype that geopandas can write.\n                Defaults to None, if unset, nothing will be written.\n            export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n            label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n            ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n            simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n            drop_nan (bool, optional): Don't export the nan class, often used for background\n            vis: should the result be visualzed\n            batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n            vis_kwargs: keyword argmument dict for visualization\n\n        Raises:\n            ValueError: If the wrong number of faces labels are provided\n\n        Returns:\n            gpd.GeoDataFrame: Merged data\n        \"\"\"\n        # Compute the working projected CRS\n        # This is important because having things in meters makes things easier\n        self.logger.info(\"Computing working CRS\")\n        lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_CRS)[0]\n        working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n        # Try to extract face labels if not set\n        if face_labels is None:\n            face_labels = self.get_texture(request_vertex_texture=False)\n\n        # Check that the correct number of labels are provided\n        if face_labels.shape[0] != self.faces.shape[0]:\n            raise ValueError()\n\n        # Get the geospatial faces dataframe\n        faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n        self.logger.info(\"Creating dataframe of multipolygons\")\n\n        # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n        face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n        if face_labels_is_2d:\n            # Non-null columns\n            unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n        else:\n            face_labels = np.squeeze(face_labels)\n            unique_IDs = np.unique(face_labels)\n\n        if drop_nan:\n            # Drop nan from the list of IDs\n            unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n        multipolygon_list = []\n        # For each unique ID, aggregate all the faces together\n        # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n        # due to the large number of polygons\n        # Instead, we replace the default shapely.unary_union with our batched implementation\n        for unique_ID in tqdm(unique_IDs, desc=\"Merging faces for each class\"):\n            if face_labels_is_2d:\n                # Nonzero elements of the column\n                matching_face_mask = face_labels[:, unique_ID] &gt; 0\n            else:\n                # Elements that match the ID in question\n                matching_face_mask = face_labels == unique_ID\n            matching_face_inds = np.nonzero(matching_face_mask)[0]\n            matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n            list_of_polygons = matching_face_polygons.geometry.values\n            multipolygon = batched_unary_union(\n                list_of_polygons, **batched_unary_union_kwargs\n            )\n            multipolygon_list.append(multipolygon)\n\n        working_gdf = gpd.GeoDataFrame(\n            {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n        )\n\n        if label_names is not None:\n            names = [\n                (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n                for ID in working_gdf[CLASS_ID_KEY]\n            ]\n            working_gdf[CLASS_NAMES_KEY] = names\n\n        # Simplify the output geometry\n        if simplify_tol &gt; 0.0:\n            self.logger.info(\"Running simplification\")\n            working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n        # Make sure that the polygons are non-overlapping\n        if ensure_non_overlapping:\n            # TODO create a version that tie-breaks based on the number of predicted faces for each\n            # class and optionally the ratios of 3D to top-down areas for the input triangles.\n            self.logger.info(\"Ensuring non-overlapping polygons\")\n            working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n        # Transform from the working crs to export crs\n        export_gdf = working_gdf.to_crs(export_crs)\n\n        # Export if a file is provided\n        if export_file is not None:\n            ensure_containing_folder(export_file)\n            export_gdf.to_file(export_file)\n\n        # Vis if requested\n        if vis:\n            self.logger.info(\"Plotting\")\n            export_gdf.plot(\n                column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n                aspect=1,\n                legend=True,\n                **vis_kwargs,\n            )\n            plt.show()\n\n        return export_gdf\n\n    # Operations on raster files\n\n    def get_vert_values_from_raster_file(\n        self,\n        raster_file: PATH_TYPE,\n        return_verts_in_CRS: bool = False,\n        nodata_fill_value: float = np.nan,\n    ):\n        \"\"\"Compute the height above groun for each point on the mesh\n\n        Args:\n            raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n            return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n            nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n        Returns:\n            np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n            np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n        \"\"\"\n        # Open the DTM file\n        raster = rio.open(raster_file)\n        # Get the mesh points in the coordinate reference system of the DTM\n        verts_in_raster_CRS = self.get_vertices_in_CRS(\n            raster.crs, force_easting_northing=True\n        )\n\n        # Get the points as a list\n        easting_points = verts_in_raster_CRS[:, 0].tolist()\n        northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n        # Zip them together\n        zipped_locations = zip(easting_points, northing_points)\n        sampling_iter = tqdm(\n            zipped_locations,\n            desc=f\"Sampling values from raster {raster_file}\",\n            total=verts_in_raster_CRS.shape[0],\n        )\n        # Sample the raster file and squeeze if single channel\n        sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n        # Set nodata locations to nan\n        # TODO figure out if it will ever be a problem to take the first value\n        sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n            nodata_fill_value\n        )\n\n        if return_verts_in_CRS:\n            return sampled_raster_values, verts_in_raster_CRS\n\n        return sampled_raster_values\n\n    def get_height_above_ground(\n        self, DTM_file: PATH_TYPE, threshold: float = None\n    ) -&gt; np.ndarray:\n        \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the digital terrain model raster\n            threshold (float, optional):\n                If not None, return a boolean mask for points under this height. Defaults to None.\n\n        Returns:\n            np.ndarray: Either the height above ground or a boolean mask for ground points\n        \"\"\"\n        # Get the height from the DTM and the points in the same CRS\n        DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n            DTM_file, return_verts_in_CRS=True\n        )\n        # Extract the vertex height as the third channel\n        verts_height = verts_in_raster_CRS[:, 2]\n        # Subtract the two to get the height above ground\n        height_above_ground = verts_height - DTM_heights\n\n        # If the threshold is not None, return a boolean mask that is true for ground points\n        if threshold is not None:\n            # Return boolean mask\n            # TODO see if this will break for nan values\n            return height_above_ground &lt; threshold\n        # Return height above ground\n        return height_above_ground\n\n    def label_ground_class(\n        self,\n        DTM_file: PATH_TYPE,\n        height_above_ground_threshold: float,\n        labels: typing.Union[None, np.ndarray] = None,\n        only_label_existing_labels: bool = True,\n        ground_class_name: str = \"ground\",\n        ground_ID: typing.Union[None, int] = None,\n        set_mesh_texture: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Set vertices to a potentially-new class with a thresholded height above the DTM.\n        TODO, consider handling face textures as well\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the DTM file\n            height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n            labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n            only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n            ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n            ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n        Returns:\n            np.ndarray: The updated labels\n        \"\"\"\n\n        if labels is None:\n            # Default to using vertex labels since it's the native way to check height above the DTM\n            use_vertex_labels = True\n        elif labels is not None:\n            # Check the size of the input labels and set what type they are. Note this could override existing value\n            if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n                use_vertex_labels = True\n            elif labels.shape[0] == self.faces.shape[0]:\n                use_vertex_labels = False\n            else:\n                raise ValueError(\n                    \"Labels were provided but didn't match the shape of vertices or faces\"\n                )\n\n        # if a labels are not provided, get it from the mesh\n        if labels is None:\n            # Get the vertex textures from the mesh\n            labels = self.get_texture(\n                request_vertex_texture=use_vertex_labels,\n            )\n\n        # Compute which vertices are part of the ground by thresholding the height above the DTM\n        ground_mask = self.get_height_above_ground(\n            DTM_file=DTM_file, threshold=height_above_ground_threshold\n        )\n        # If we needed a mask for the faces, compute that instead\n        if not use_vertex_labels:\n            ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n                bool\n            )\n\n        # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n        if only_label_existing_labels:\n            # Find which vertices are labeled\n            is_labeled = np.isfinite(labels[:, 0])\n            # Find which points are ground that were previously labeled as something else\n            ground_mask = np.logical_and(is_labeled, ground_mask)\n\n        # Get the existing label names\n        IDs_to_labels = self.get_IDs_to_labels()\n\n        if IDs_to_labels is None and ground_ID is None:\n            # This means that the label is continous, so the concept of ID is meaningless\n            ground_ID = np.nan\n        elif IDs_to_labels is not None and ground_class_name in IDs_to_labels.values():\n            # If the ground class name is already in the list, set newly-predicted vertices to that class\n            # Get the dictionary mapping in the reverse direction\n            labels_to_IDs = {v: k for k, v in IDs_to_labels.items()}\n            # Determine the ID corresponding to the ground class name\n            ground_ID = labels_to_IDs.get(ground_class_name)\n        elif IDs_to_labels is not None:\n            # If the label names are present, and the class is not already included, add it as the last element\n            if ground_ID is None:\n                # Set it to the first unused ID\n                # TODO improve this since it should be the max plus one\n                ground_ID = len(IDs_to_labels)\n\n        self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n        # Replace mask for ground_vertices\n        labels[ground_mask, 0] = ground_ID\n\n        # Optionally apply the texture to the mesh\n        if set_mesh_texture:\n            # TODO look into why this shouldn't update the IDs to labels.\n            # I guess because it may have been initially user-provided.\n            self.set_texture(labels, update_IDs_to_labels=False)\n\n        return labels\n\n    def get_mesh_hash(self):\n        \"\"\"Generates a hash value for the mesh based on its points and faces\n        Returns:\n            int: A hash value representing the current mesh.\n        \"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(self.pyvista_mesh.points.tobytes())\n        hasher.update(self.pyvista_mesh.faces.tobytes())\n        return hasher.hexdigest()\n\n    def get_mesh_in_cameras_coords(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        inplace: bool = False,\n    ) -&gt; typing.Optional[pv.PolyData]:\n        \"\"\"Obtain a mesh in the same local coordinate frame convention as the camera set.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Camera or camera set to match the convention of.\n            inplace (bool, optional):\n                Should the object be update to reflect this convetion. Otherwise, a pyvista mesh is\n                returned and the self object remains unchanged. Defaults to False.\n\n        Returns:\n            typing.Optional[pv.PolyData]: Mesh in the camera's coordinate system if inplace=False\n        \"\"\"\n        # Reproject the mesh into ECEF\n        mesh = self.reproject_CRS(EARTH_CENTERED_EARTH_FIXED_CRS, inplace=False)\n\n        # Get the inverse 4x4 transform, which maps from Earth Centered, Earth Fixed (EPSG:4978)\n        # to the coordinates that the cameras are in\n        local_to_epsg_4978_transform = cameras.get_local_to_epsg_4978_transform()\n        epsg_4978_to_camera = np.linalg.inv(local_to_epsg_4978_transform)\n        # Transform the mesh using this transform\n        mesh = mesh.transform(epsg_4978_to_camera, inplace=False)\n\n        if inplace:\n            # Overwrite the mesh with the updated version\n            self.pyvista_mesh = mesh\n            # Indicate that there is no longer a valid CRS\n            self.CRS = None\n            # Return None to match common convention for inplace methods\n            return None\n\n        return mesh\n\n    def pix2face(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        mesh: typing.Optional[pv.PolyData] = None,\n        render_img_scale: float = 1,\n        save_to_cache: bool = False,\n        cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n        distortion_set: typing.Optional[PhotogrammetryCameraSet] = None,\n        apply_distortion: bool = True,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                A single camera or set of cameras. For each camera, the correspondences between\n                pixels and the face IDs of the mesh will be computed. The images of all cameras\n                are assumed to be the same size.\n            render_img_scale (float, optional):\n                Create a pix2face map that is this fraction of the original image scale. Defaults\n                to 1.\n            save_to_cache (bool, optional):\n                Should newly-computed values be saved to the cache. This may speed up future operations\n                but can take up 100s of GBs of space. Defaults to False.\n            cache_folder ((PATH_TYPE, None), optional):\n                Where to check for and save to cached data. Only applicable if use_cache=True.\n                Defaults to CACHE_FOLDER\n            distortion_set (PhotogrammetryCameraSet, optional): camera set used for calculating\n                and caching the distortion/undistortion maps. This is only required if apply_distortion\n                is True. Note that if you are calling pix2face on batches, you should pass the\n                full camera set in as the distortion_set so that the maps are cached once.\n            apply_distortion (bool, optional):\n                Should distortion correction be applied. This is expensive but can be neccesary for\n                some camera models which significantly differ from a pinhole model.\n\n        Returns:\n            np.ndarray: For each camera, there is an array that is the shape of an image and\n            contains the integer face index for the ray originating at that pixel. Any pixel for\n            which the given ray does not intersect a face is given a value of -1. If the input is\n            a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n            (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n        \"\"\"\n\n        # Create a local mesh if it hasn't been created yet\n        if mesh is None:\n            mesh = self.get_mesh_in_cameras_coords(cameras)\n\n        # If a set of cameras is passed in, call this method on each camera and concatenate\n        # Other derived methods might be able to compute a batch of renders at once, but pyvista\n        # cannot as far as I can tell.\n        # Note that all inputs to pix2face need to be replicated here or those features won't be\n        # passed on.\n        if isinstance(cameras, PhotogrammetryCameraSet):\n            pix2face_list = [\n                self.pix2face(\n                    cameras=camera,\n                    mesh=mesh,\n                    render_img_scale=render_img_scale,\n                    save_to_cache=save_to_cache,\n                    cache_folder=cache_folder,\n                    distortion_set=distortion_set,\n                    apply_distortion=apply_distortion,\n                )\n                for camera in cameras\n            ]\n            pix2face = np.stack(pix2face_list, axis=0)\n            return pix2face\n\n        ## Single camera case\n\n        # Check if the cache contains a valid pix2face for the camera based on the dependencies\n        # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n        # The cache will generate a unique key for each combination of the dependencies\n        # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n        # If no match is found, recompute pix2face\n        # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n        mesh_hash = self.get_mesh_hash()\n        camera_hash = cameras.get_camera_hash()\n        cacher = ub.Cacher(\n            \"pix2face\",\n            depends=[mesh_hash, camera_hash, render_img_scale],\n            dpath=cache_folder,\n            verbose=0,\n        )\n        pix2face = cacher.tryload(on_error=\"clear\")\n        ## Cache is valid\n        if pix2face is not None:\n            return pix2face\n\n        # This needs to be an attribute of the class because creating a large number of plotters\n        # results in an un-fixable memory leak.\n        # See https://github.com/pyvista/pyvista/issues/2252\n        # The first step is to clear it\n        self.pix2face_plotter.clear()\n        # This is important so there aren't intermediate values\n        self.pix2face_plotter.disable_anti_aliasing()\n        # Set the camera to the corresponding viewpoint\n        self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n        ## Compute the base 256 encoding of the face ID\n        n_faces = self.faces.shape[0]\n        ID_values = np.arange(n_faces)\n\n        # determine how many channels will be required to represent the number of faces\n        n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n        channel_multipliers = [256**i for i in range(n_channels)]\n\n        # Compute the encoding of each value, least significant value first\n        base_256_encoding = [\n            np.mod(np.floor(ID_values / m).astype(int), 256)\n            for m in channel_multipliers\n        ]\n\n        # ensure that there's a multiple of three channels\n        n_padding = int(np.ceil(n_channels / 3.0) * 3 - n_channels)\n        base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n        # Assume that all images are the same size\n        image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n        # Initialize pix2face\n        pix2face = np.zeros(image_size, dtype=int)\n        # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n        for chunk_ind in range(int(len(base_256_encoding) / 3)):\n            chunk_scalars = np.stack(\n                base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n            ).astype(np.uint8)\n            # Add the mesh with the associated scalars\n            self.pix2face_plotter.add_mesh(\n                mesh,\n                scalars=chunk_scalars.copy(),\n                rgb=True,\n                diffuse=0.0,\n                ambient=1.0,\n            )\n\n            # Perform rendering, this is the slow step\n            rendered_img = self.pix2face_plotter.screenshot(\n                window_size=(image_size[1], image_size[0]),\n            )\n            # Take the rendered values and interpret them as the encoded value\n            # Make sure to not try to interpret channels that are not used in the encoding\n            channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n            for i in range(channels_to_decode):\n                channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n                channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n                pix2face += channel_value\n\n        # Mask out pixels for which the mesh was not visible\n        # This is because the background will render as white\n        # If there happen to be an exact power of (256^3) number of faces, the last one may get\n        # erronously masked. This seems like a minimal concern but it could be addressed by adding\n        # another channel or something like that\n        pix2face[pix2face &gt; n_faces] = -1\n\n        if save_to_cache:\n            # Save the most recently computed pix2face correspondance in the cache\n            cacher.save(pix2face)\n\n        if apply_distortion:\n            # Warp the pix2face mask so it matches the warping of the real image which does not\n            # conform to the pinhole model.\n            # Note this step is slow, and especially on the first iteration which may take multiple\n            # minutes.\n            pix2face = distortion_set.warp_dewarp_image(\n                camera=cameras,\n                input_image=pix2face,\n                warped_to_ideal=False,\n                fill_value=-1,\n                interpolation_order=0,  # nearest neighbor interpolation\n                image_scale=render_img_scale,\n            )\n\n        return pix2face\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        return_camera: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n            return_camera (bool, optional):\n                Should the camera be yielded as the second value\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        # Create a local mesh\n        mesh = self.get_mesh_in_cameras_coords(cameras)\n\n        if isinstance(cameras, PhotogrammetryCamera):\n            # Construct a camera set of length one\n            cameras = PhotogrammetryCameraSet([cameras])\n        elif not isinstance(cameras, PhotogrammetryCameraSet):\n            raise TypeError()\n\n        # Get the face texture from the mesh\n        # TODO consider whether the user should be able to pass a texture to this method. It could\n        # make the user's life easier but makes this method more complex\n        face_texture = self.get_texture(\n            request_vertex_texture=False, try_verts_faces_conversion=True\n        )\n        texture_dim = face_texture.shape[1]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_end = batch_start + batch_size\n            batch_cameras = cameras[batch_start:batch_end]\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                mesh=mesh,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n\n            # Iterate over the batch dimension\n            for i, pix2face in enumerate(batch_pix2face):\n                # Record the original shape of the image\n                img_shape = pix2face.shape[:2]\n                # Flatten for indexing\n                pix2face = pix2face.flatten()\n                # Compute which pixels intersected the mesh\n                mesh_pixel_inds = np.where(pix2face != -1)[0]\n                # Initialize and all-nan array\n                rendered_flattened = np.full(\n                    (pix2face.shape[0], texture_dim), fill_value=np.nan\n                )\n                # Fill the values for which correspondences exist\n                rendered_flattened[mesh_pixel_inds] = face_texture[\n                    pix2face[mesh_pixel_inds]\n                ]\n                # reshape to an image, where the last dimension is the texture dimension\n                rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n\n                if return_camera:\n                    yield (rendered_img, batch_cameras[i])\n                else:\n                    yield rendered_img\n\n    def project_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        check_null_image: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to project images from. cam.get_image() will be called on each one\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            check_null_image (bool, optional):\n                Only do indexing if there are non-null image values. This adds additional overhead,\n                but can save the expensive operation of indexing in cases where it would be a no-op.\n\n        Yields:\n            np.ndarray: The per-face projection of an image in the camera set\n        \"\"\"\n        # Create a local mesh\n        mesh = self.get_mesh_in_cameras_coords(cameras)\n\n        n_faces = self.faces.shape[0]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_inds = list(range(batch_start, batch_start + batch_size))\n            batch_cameras = cameras.get_subset_cameras(batch_inds)\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                mesh=mesh,\n                render_img_scale=aggregate_img_scale,\n                **pix2face_kwargs,\n            )\n            for i, pix2face in enumerate(batch_pix2face):\n                img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n                n_channels = 1 if img.ndim == 2 else img.shape[-1]\n                textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n                # Only do the expensive indexing step if there are finite values in the image. This is most\n                # significant for sparse detection tasks where some images may have no real data\n                if not check_null_image or np.any(np.isfinite(img)):\n                    flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                    flat_pix2face = pix2face.flatten()\n                    # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                    # my guess is the later pixel in the flattened array will override the former\n                    # TODO make sure that null pix2face values are handled properly\n                    textured_faces[flat_pix2face] = flat_img\n                yield textured_faces\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        return_all: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            return_all (bool, optional):\n                Return the projection of each individual image, rather than just the aggregates.\n                Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n        project_images_generator = self.project_images(\n            cameras=cameras,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            **kwargs,\n        )\n\n        if return_all:\n            all_projections = []\n\n        # TODO this should be a convenience method\n        n_faces = self.faces.shape[0]\n\n        projection_counts = np.zeros(n_faces)\n        summed_projection = None\n\n        for projection_for_image in tqdm(\n            project_images_generator,\n            total=len(cameras),\n            desc=\"Aggregating projected viewpoints\",\n        ):\n            if return_all:\n                all_projections.append(projection_for_image)\n\n            if summed_projection is None:\n                summed_projection = projection_for_image.astype(float)\n            else:\n                summed_projection = np.nansum(\n                    [summed_projection, projection_for_image], axis=0\n                )\n\n            projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n                int\n            )\n            projection_counts += projected_faces\n\n        no_projections = projection_counts == 0\n        summed_projection[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projection,\n        }\n\n        if return_all:\n            additional_information[\"all_projections\"] = all_projections\n\n        average_projections = np.divide(\n            summed_projection, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    # Visualization and saving methods\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        interactive: bool = True,\n        camera_set: PhotogrammetryCameraSet = None,\n        screenshot_filename: PATH_TYPE = None,\n        vis_scalars: typing.Union[None, np.ndarray] = None,\n        mesh_kwargs: typing.Dict = None,\n        interactive_jupyter: bool = False,\n        plotter_kwargs: typing.Dict = {},\n        enable_ssao: bool = True,\n        force_xvfb: bool = False,\n        frustum_scale: float = 2,\n        IDs_to_labels: typing.Union[None, dict] = None,\n    ):\n        \"\"\"Show the mesh and cameras\n\n        Args:\n            plotter (pyvista.Plotter, optional):\n                Plotter to use, else one will be created\n            off_screen (bool, optional):\n                Show offscreen\n            camera_set (PhotogrammetryCameraSet, optional):\n                Cameras to visualize. Defaults to None.\n            screenshot_filename (PATH_TYPE, optional):\n                Filepath to save to, will show interactively if None. Defaults to None.\n            vis_scalars (None, np.ndarray):\n                Scalars to show\n            mesh_kwargs:\n                dict of keyword arguments for the mesh\n            interactive_jupyter (bool):\n                Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n            plotter_kwargs:\n                dict of keyword arguments for the plotter\n            frustum_scale (float, optional):\n                Size of cameras in world units. Defaults to None.\n            IDs_to_labels ([None, dict], optional):\n                Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n                IDs_to_labels if unset.\n        \"\"\"\n        # TODO conside reprojecting to ensure axes are both meters-based\n        off_screen = (not interactive) or (screenshot_filename is not None)\n\n        # If the IDs to labels is not set, use the default ones for this mesh\n        if IDs_to_labels is None:\n            IDs_to_labels = self.get_IDs_to_labels()\n\n        # Set the mesh kwargs if not set\n        if mesh_kwargs is None:\n            # This needs to be a dict, even if it's empty\n            mesh_kwargs = {}\n\n            # If there are discrete labels, set the colormap and limits inteligently\n            if IDs_to_labels is not None:\n                # Compute the largest ID\n                max_ID = max(IDs_to_labels.keys())\n                if max_ID &lt; 20:\n                    colors = [\n                        matplotlib.colors.to_hex(c)\n                        for c in plt.get_cmap(\n                            (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                        ).colors\n                    ]\n                    mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                    mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n        # Create the plotter if it's None\n        plotter = create_pv_plotter(\n            off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n        )\n\n        # If the vis scalars are None, use the saved texture\n        if vis_scalars is None:\n            vis_scalars = self.get_texture(\n                # Request vertex texture if both are available\n                request_vertex_texture=(\n                    True\n                    if (\n                        self.vertex_texture is not None\n                        and self.face_texture is not None\n                    )\n                    else None\n                )\n            )\n\n        is_rgb = (\n            self.pyvista_mesh.active_scalars_name == \"RGB\"\n            if vis_scalars is None\n            else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n        )\n\n        # Data in the range [0, 255] must be uint8 type\n        if is_rgb and np.nanmax(vis_scalars) &gt; 1.0:\n            vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n        scalar_bar_args = {\"vertical\": True}\n        if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n            mesh_kwargs[\"annotations\"] = IDs_to_labels\n            scalar_bar_args[\"n_labels\"] = 0\n\n        vis_mesh = self.reproject_CRS(EARTH_CENTERED_EARTH_FIXED_CRS, inplace=False)\n\n        # If camera set is provided, transform the mesh into those coordinates\n        if camera_set is not None:\n            # Compute the transform mapping from the earth centered, earth fixed coordinate frame\n            # (EPSG:4978) to the coordinate of the camera\n            epsg_4978_to_camera = np.linalg.inv(\n                camera_set.get_local_to_epsg_4978_transform()\n            )\n            # Apply the 4x4 transform using the pyvista transform method to get the mesh into the\n            # same coordinate frame as the cameras.\n            vis_mesh.transform(epsg_4978_to_camera, inplace=True)\n\n        # Add the mesh\n        plotter.add_mesh(\n            vis_mesh,\n            scalars=vis_scalars,\n            rgb=is_rgb,\n            scalar_bar_args=scalar_bar_args,\n            **mesh_kwargs,\n        )\n        # If the camera set is provided, show this too\n        if camera_set is not None:\n            # Adjust the frustum scale if the mesh came from metashape\n            # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n            if (\n                camera_set.get_local_to_epsg_4978_transform() is not None\n                and frustum_scale is not None\n            ):\n                transform_determinant = np.linalg.det(\n                    camera_set.get_local_to_epsg_4978_transform()[:3, :3]\n                )\n                scale_factor = np.cbrt(transform_determinant)\n                frustum_scale = frustum_scale / scale_factor\n            camera_set.vis(\n                plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n            )\n\n        # Enable screen space shading\n        if enable_ssao:\n            plotter.enable_ssao()\n\n        # Create parent folder if none exists\n        if screenshot_filename is not None:\n            ensure_containing_folder(screenshot_filename)\n\n        if \"jupyter_backend\" not in plotter_kwargs:\n            if interactive_jupyter:\n                plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n            else:\n                plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n        if \"title\" not in plotter_kwargs:\n            plotter_kwargs[\"title\"] = \"Geograypher mesh viewer\"\n\n        # Show\n        return plotter.show(\n            screenshot=screenshot_filename,\n            **plotter_kwargs,\n        )\n\n    def save_renders(\n        self,\n        camera_set: PhotogrammetryCameraSet,\n        render_image_scale=1.0,\n        output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n        make_composites: bool = False,\n        save_native_resolution: bool = False,\n        cast_to_uint8: bool = True,\n        save_as_npy: bool = False,\n        uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n        **render_kwargs,\n    ):\n        \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n        Args:\n            camera_set (PhotogrammetryCameraSet):\n                Camera set to use for rendering\n            render_image_scale (float, optional):\n                Multiplier on the real image scale to obtain size for rendering. Lower values\n                yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n            render_folder (PATH_TYPE, optional):\n                Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n            make_composites (bool, optional):\n                Should a triple pane composite with the original image be saved rather than the\n                raw label\n            cast_to_uint8: (bool, optional):\n                cast the float valued data to unit8 for saving efficiency. May dramatically increase\n                efficiency due to tif compression. Saves as tif unless save_as_npy is specified as True.\n            save_as_npy (bool, optional):\n                Save the rendered images as numpy arrays rather than TIF images. Defaults to False.\n            uint8_value_for_null_texture (np.uint8, optional):\n                What value to assign for values that can't be represented as unsigned 8-bit data.\n                Defaults to NULL_TEXTURE_INT_VALUE\n            render_kwargs:\n                keyword arguments passed to the render.\n        \"\"\"\n\n        ensure_folder(output_folder)\n        self.logger.info(f\"Saving renders to {output_folder}\")\n\n        # Save the classes filename\n        self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n        # Create the generator object to render the images\n        # Since this is a generator, this will be fast\n        render_gen = self.render_flat(\n            camera_set,\n            render_img_scale=render_image_scale,\n            return_camera=True,\n            distortion_set=camera_set,\n            **render_kwargs,\n        )\n\n        # The computation only happens when items are requested from the generator\n        for rendered, camera in tqdm(\n            render_gen,\n            total=len(camera_set),\n            desc=\"Computing and saving renders\",\n        ):\n            ## All this is post-processing to visualize the rendered label.\n            # rendered could either be a one channel image of integer IDs,\n            # a one-channel image of scalars, or a three-channel image of\n            # RGB. It could also be multi-channel image corresponding to anything,\n            # but we don't expect that yet\n            if save_native_resolution and render_image_scale != 1:\n                native_size = camera.get_image_size()\n                # Upsample using nearest neighbor interpolation for discrete labels and\n                # bilinear for non-discrete\n                # TODO this will need to be fixed for multi-channel images since I don't think resize works\n                rendered = resize(\n                    rendered,\n                    native_size,\n                    order=(0 if self.is_discrete_texture() else 1),\n                )\n\n            if cast_to_uint8:\n                # Deterimine values that cannot be represented as uint8\n                mask = np.logical_or.reduce(\n                    [\n                        rendered &lt; 0,\n                        rendered &gt; 255,\n                        np.logical_not(np.isfinite(rendered)),\n                    ]\n                )\n                rendered[mask] = uint8_value_for_null_texture\n                # Cast and squeeze since you can't save a one-channel image\n                rendered = np.squeeze(rendered.astype(np.uint8))\n\n            if make_composites:\n                RGB_image = camera.get_image(\n                    image_scale=(1.0 if save_native_resolution else render_image_scale)\n                )\n                rendered = create_composite(\n                    RGB_image=RGB_image,\n                    label_image=rendered,\n                    IDs_to_labels=self.get_IDs_to_labels(),\n                )\n            else:\n                # Clip channels if needed\n                if rendered.ndim == 3:\n                    rendered = rendered[..., :3]\n\n            try:\n                # If the filename stored with the camera data [camera.get_image_filename]\n                # is a subpath of your camera set image folder, use the same subpath for\n                # the output data.\n                camera_filename = camera.get_image_filename().relative_to(\n                    camera_set.image_folder\n                )\n            except ValueError:\n                raise ValueError(\n                    \"Tried to find the relative path of the camera path\"\n                    f\" ({camera.get_image_filename()}) inside of the camera set image\"\n                    f\" folder ({camera_set.image_folder}), but failed. The tool being called\"\n                    \" may have an 'original_image_folder' argument, which could be used to\"\n                    \" delete the initial, mismatched portion of the camera path. See more here:\"\n                    \" https://github.com/open-forest-observatory/automate-metashape/issues/90.\"\n                )\n            output_filename = Path(output_folder, camera_filename)\n\n            # This may create nested folders in the output dir\n            ensure_containing_folder(output_filename)\n\n            if save_as_npy is True:\n                output_filename = str(output_filename.with_suffix(\".npy\"))\n                # Save the image\n                np.save(output_filename, rendered)\n            else:\n                # Save image as TIF\n                output_filename = str(output_filename.with_suffix(\".tif\"))\n                # Remove singleton channel dimension (1, H, W) -&gt; (H, W) to save single-channel TIF\n                rendered = np.squeeze(rendered)\n                # TODO: Consider supporting TIF files with float data (like CHM renders) by adding a separate flag.\n                # Evaluate whether this offers more space savings than npy files.\n                # If cast_to_uint8 is True, rendered is already in uint8\n                if cast_to_uint8 is False:\n                    # Check if max value in the rendered image is within the range of uint16\n                    if np.nanmax(rendered) &lt;= np.iinfo(np.uint16).max:\n                        # Cast from float to uint16\n                        rendered = rendered.astype(np.uint16)\n                    else:\n                        rendered = rendered.astype(np.uint32)\n\n                # Save the image\n                skimage.io.imsave(\n                    output_filename,\n                    rendered,\n                    compression=\"deflate\",\n                    check_contrast=False,\n                )\n\n    def export_covering_meshes(\n        self,\n        N: int,\n        z_buffer: tuple = (0, 0),\n        subsample: typing.Union[int, None] = None,\n    ) -&gt; typing.Tuple[pv.PolyData, pv.PolyData]:\n        \"\"\"\n        This function will process self.pyvista_mesh. It will start by identifying the (x, y)\n        boundaries of that mesh, then create an (N, N) grid of (x, y) points over that area.\n        At each (x, y) square in the grid, the pyvista mesh will be sampled for the highest and\n        lowest value. This will be used to set the Z heights for that grid point (plus the\n        z_buffer). Then upper and lower bound surfaces will be made from these grid points\n        using delaunay_2d and returned.\n\n        Args:\n            N (int): Number of sample points to take as a grid\n            z_buffer (tuple): Offset in Z to give the sampled points, in the units of the mesh.\n                [0] is for the upper mesh, [1] is for the lower.\n            subsample (int / None): If not None, we will naively subsample self.pyvista_mesh.points\n                to speed up runtime by [::subsample]. A larger number will subsample more.\n\n        Returns:\n            tuple: A tuple of two pyvista.PolyData objects, the first is the upper\n            covering mesh, the second the lower covering mesh.\n        \"\"\"\n\n        assert self.pyvista_mesh is not None, \"Requires a populated mesh\"\n        assert len(z_buffer) == 2, \"2 buffers (top, bottom) are required\"\n\n        # Get mesh points\n        points = self.pyvista_mesh.points\n        if len(points) == 0:\n            return (self.pyvista_mesh.copy(), self.pyvista_mesh.copy())\n\n        if subsample is not None:\n            points = points[::subsample]\n        x_min, y_min = points[:, 0].min(), points[:, 1].min()\n        x_max, y_max = points[:, 0].max(), points[:, 1].max()\n\n        # Create grid\n        grid_ind = np.indices((N, N)).reshape(2, -1).T\n        x_grid = np.linspace(x_min, x_max, N)\n        y_grid = np.linspace(y_min, y_max, N)\n        grid_points = np.column_stack([x_grid[grid_ind[:, 0]], y_grid[grid_ind[:, 1]]])\n\n        # For each grid point, find mesh points within the cell and get z value\n        cell_w_half = (x_max - x_min) / (N - 1) / 2\n        cell_h_half = (y_max - y_min) / (N - 1) / 2\n        # Positive\n        z_p_values = np.full(grid_points.shape[0], np.nan)\n        # Negative\n        z_n_values = np.full(grid_points.shape[0], np.nan)\n\n        # Precompute mask stripes to save time\n        x_masks = {\n            index: (points[:, 0] &gt;= x_grid[index] - cell_w_half)\n            &amp; (points[:, 0] &lt;= x_grid[index] + cell_w_half)\n            for index in range(N)\n        }\n        y_masks = {\n            index: (points[:, 1] &gt;= y_grid[index] - cell_h_half)\n            &amp; (points[:, 1] &lt;= y_grid[index] + cell_h_half)\n            for index in range(N)\n        }\n        for i, (xi, yi) in enumerate(tqdm(grid_ind, desc=\"Building covering meshes\")):\n            # Find mesh points within the cell\n            mask = x_masks[xi] &amp; y_masks[yi]\n            z_candidates = points[mask, 2]\n            if len(z_candidates) &gt; 0:\n                z_p_values[i] = np.max(z_candidates) + z_buffer[0]\n                z_n_values[i] = np.min(z_candidates) + z_buffer[1]\n\n        # Create 3D points for the grid\n        grid_p_3d = np.column_stack([grid_points, z_p_values])\n        grid_n_3d = np.column_stack([grid_points, z_n_values])\n        # Remove points with no Z values\n        grid_p_3d = grid_p_3d[~np.isnan(z_p_values)]\n        grid_n_3d = grid_n_3d[~np.isnan(z_n_values)]\n\n        # Create pyvista point cloud and surface\n        return (\n            pv.PolyData(grid_p_3d).delaunay_2d(),\n            pv.PolyData(grid_n_3d).delaunay_2d(),\n        )\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh-functions","title":"Functions","text":""},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.__init__","title":"<code>__init__(mesh, input_CRS, downsample_target=1.0, texture=None, texture_column_name=None, IDs_to_labels=None, shift=None, ROI=None, ROI_buffer_meters=0, log_level='INFO')</code>","text":"<p>An object that represents a geospatial mesh with associated textures and supports various rendering options.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh in a filetype readable by pyvista or a pyvista mesh object.</p> required <code>input_CRS</code> <code>CRS</code> <p>The vertex coordinates of the input mesh should be interpreteted in this coordinate references system to georeference them.</p> required <code>downsample_target</code> <code>float</code> <p>Downsample so this fraction of vertices remain. Defaults to 1.0.</p> <code>1.0</code> <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>Texture or path to one. See more details in <code>load_texture</code> documentation. Defaults to None.</p> <code>None</code> <code>texture_column_name</code> <code>Union[PATH_TYPE, None]</code> <p>The column to use as the label for a vector data input. Passed to <code>load_texture</code>. Defaults to None.</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[PATH_TYPE, dict, None]</code> <p>dictionary or path to JSON file containing the mapping from integer IDs to string class names. Defaults to None.</p> <code>None</code> <code>shift</code> <code>Union[ndarray, None]</code> <p>Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex coordinates by this amount in the input_CRS frame. Defaults to None.</p> <code>None</code> <code>ROI</code> <code>Union[GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None]</code> <p>Crop the mesh to this region. For more information see <code>select_mesh_ROI</code>. Defaults to None.</p> <code>None</code> <code>ROI_buffer_meters</code> <code>float</code> <p>Buffer the cropped region by this distance. For more information see <code>select_mesh_ROI</code>. Defaults to 0.</p> <code>0</code> <code>log_level</code> <code>str</code> <p>Controls what severity of messages are logged. Defaults to \"INFO\".</p> <code>'INFO'</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def __init__(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    input_CRS: pyproj.CRS,\n    downsample_target: float = 1.0,\n    texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n    texture_column_name: typing.Union[PATH_TYPE, None] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    shift: typing.Union[np.ndarray, None] = None,\n    ROI: typing.Union[\n        gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n    ] = None,\n    ROI_buffer_meters: float = 0,\n    log_level: str = \"INFO\",\n):\n    \"\"\"\n    An object that represents a geospatial mesh with associated textures and supports various\n    rendering options.\n\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n            Path to the mesh in a filetype readable by pyvista or a pyvista mesh object.\n        input_CRS (pyproj.CRS):\n            The vertex coordinates of the input mesh should be interpreteted in this coordinate\n            references system to georeference them.\n        downsample_target (float, optional):\n            Downsample so this fraction of vertices remain. Defaults to 1.0.\n        texture (typing.Union[PATH_TYPE, np.ndarray, None], optional):\n            Texture or path to one. See more details in `load_texture` documentation. Defaults\n            to None.\n        texture_column_name (typing.Union[PATH_TYPE, None], optional):\n            The column to use as the label for a vector data input. Passed to `load_texture`.\n            Defaults to None.\n        IDs_to_labels (typing.Union[PATH_TYPE, dict, None], optional):\n            dictionary or path to JSON file containing the mapping from integer IDs to string\n            class names. Defaults to None.\n        shift (typing.Union[np.ndarray, None], optional):\n            Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex\n            coordinates by this amount in the input_CRS frame. Defaults to None.\n        ROI (typing.Union[ gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None ], optional):\n            Crop the mesh to this region. For more information see `select_mesh_ROI`. Defaults\n            to None.\n        ROI_buffer_meters (float, optional):\n            Buffer the cropped region by this distance. For more information see\n            `select_mesh_ROI`. Defaults to 0.\n        log_level (str, optional):\n            Controls what severity of messages are logged. Defaults to \"INFO\".\n    \"\"\"\n    self.downsample_target = downsample_target\n\n    self.pyvista_mesh = None\n    self.texture = None\n    self.vertex_texture = None\n    self.face_texture = None\n    self.IDs_to_labels = None\n    # Create the plotter that will later be used to compute correspondences between pixels\n    # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n    # plotters. See https://github.com/pyvista/pyvista/issues/2252\n    self.pix2face_plotter = create_pv_plotter(off_screen=True)\n    self.face_polygons_cache = {}\n    self.face_2d_3d_ratios_cache = {}\n\n    self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n    self.logger.setLevel(log_level)\n    # Potentially necessary for Jupyter\n    # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n    # If you don't check that there's already a handler, you can have situations with duplicated\n    # print outs if you have multiple mesh objects\n    if not self.logger.hasHandlers():\n        self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n    # Load the mesh with the pyvista loader\n    self.logger.info(\"Loading mesh\")\n    self.load_mesh(\n        mesh=mesh,\n        input_CRS=input_CRS,\n        downsample_target=downsample_target,\n        shift=shift,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_meters,\n    )\n    # Load the texture\n    self.logger.info(\"Loading texture\")\n    # load IDs_to_labels\n    # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n    if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n        possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n        if possible_json.exists():\n            IDs_to_labels = possible_json\n    # convert IDs_to_labels from file to dict\n    if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n        with open(IDs_to_labels, \"r\") as file:\n            IDs_to_labels = json.load(file)\n            IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n    self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, return_all=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>return_all</code> <code>bool</code> <p>Return the projection of each individual image, rather than just the aggregates. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    return_all: bool = False,\n    **kwargs,\n):\n    \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        return_all (bool, optional):\n            Return the projection of each individual image, rather than just the aggregates.\n            Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n    project_images_generator = self.project_images(\n        cameras=cameras,\n        batch_size=batch_size,\n        aggregate_img_scale=aggregate_img_scale,\n        **kwargs,\n    )\n\n    if return_all:\n        all_projections = []\n\n    # TODO this should be a convenience method\n    n_faces = self.faces.shape[0]\n\n    projection_counts = np.zeros(n_faces)\n    summed_projection = None\n\n    for projection_for_image in tqdm(\n        project_images_generator,\n        total=len(cameras),\n        desc=\"Aggregating projected viewpoints\",\n    ):\n        if return_all:\n            all_projections.append(projection_for_image)\n\n        if summed_projection is None:\n            summed_projection = projection_for_image.astype(float)\n        else:\n            summed_projection = np.nansum(\n                [summed_projection, projection_for_image], axis=0\n            )\n\n        projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n            int\n        )\n        projection_counts += projected_faces\n\n    no_projections = projection_counts == 0\n    summed_projection[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projection,\n    }\n\n    if return_all:\n        additional_information[\"all_projections\"] = all_projections\n\n    average_projections = np.divide(\n        summed_projection, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.export_covering_meshes","title":"<code>export_covering_meshes(N, z_buffer=(0, 0), subsample=None)</code>","text":"<p>This function will process self.pyvista_mesh. It will start by identifying the (x, y) boundaries of that mesh, then create an (N, N) grid of (x, y) points over that area. At each (x, y) square in the grid, the pyvista mesh will be sampled for the highest and lowest value. This will be used to set the Z heights for that grid point (plus the z_buffer). Then upper and lower bound surfaces will be made from these grid points using delaunay_2d and returned.</p> <p>Parameters:</p> Name Type Description Default <code>N</code> <code>int</code> <p>Number of sample points to take as a grid</p> required <code>z_buffer</code> <code>tuple</code> <p>Offset in Z to give the sampled points, in the units of the mesh. [0] is for the upper mesh, [1] is for the lower.</p> <code>(0, 0)</code> <code>subsample</code> <code>int / None</code> <p>If not None, we will naively subsample self.pyvista_mesh.points to speed up runtime by [::subsample]. A larger number will subsample more.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>PolyData</code> <p>A tuple of two pyvista.PolyData objects, the first is the upper</p> <code>PolyData</code> <p>covering mesh, the second the lower covering mesh.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def export_covering_meshes(\n    self,\n    N: int,\n    z_buffer: tuple = (0, 0),\n    subsample: typing.Union[int, None] = None,\n) -&gt; typing.Tuple[pv.PolyData, pv.PolyData]:\n    \"\"\"\n    This function will process self.pyvista_mesh. It will start by identifying the (x, y)\n    boundaries of that mesh, then create an (N, N) grid of (x, y) points over that area.\n    At each (x, y) square in the grid, the pyvista mesh will be sampled for the highest and\n    lowest value. This will be used to set the Z heights for that grid point (plus the\n    z_buffer). Then upper and lower bound surfaces will be made from these grid points\n    using delaunay_2d and returned.\n\n    Args:\n        N (int): Number of sample points to take as a grid\n        z_buffer (tuple): Offset in Z to give the sampled points, in the units of the mesh.\n            [0] is for the upper mesh, [1] is for the lower.\n        subsample (int / None): If not None, we will naively subsample self.pyvista_mesh.points\n            to speed up runtime by [::subsample]. A larger number will subsample more.\n\n    Returns:\n        tuple: A tuple of two pyvista.PolyData objects, the first is the upper\n        covering mesh, the second the lower covering mesh.\n    \"\"\"\n\n    assert self.pyvista_mesh is not None, \"Requires a populated mesh\"\n    assert len(z_buffer) == 2, \"2 buffers (top, bottom) are required\"\n\n    # Get mesh points\n    points = self.pyvista_mesh.points\n    if len(points) == 0:\n        return (self.pyvista_mesh.copy(), self.pyvista_mesh.copy())\n\n    if subsample is not None:\n        points = points[::subsample]\n    x_min, y_min = points[:, 0].min(), points[:, 1].min()\n    x_max, y_max = points[:, 0].max(), points[:, 1].max()\n\n    # Create grid\n    grid_ind = np.indices((N, N)).reshape(2, -1).T\n    x_grid = np.linspace(x_min, x_max, N)\n    y_grid = np.linspace(y_min, y_max, N)\n    grid_points = np.column_stack([x_grid[grid_ind[:, 0]], y_grid[grid_ind[:, 1]]])\n\n    # For each grid point, find mesh points within the cell and get z value\n    cell_w_half = (x_max - x_min) / (N - 1) / 2\n    cell_h_half = (y_max - y_min) / (N - 1) / 2\n    # Positive\n    z_p_values = np.full(grid_points.shape[0], np.nan)\n    # Negative\n    z_n_values = np.full(grid_points.shape[0], np.nan)\n\n    # Precompute mask stripes to save time\n    x_masks = {\n        index: (points[:, 0] &gt;= x_grid[index] - cell_w_half)\n        &amp; (points[:, 0] &lt;= x_grid[index] + cell_w_half)\n        for index in range(N)\n    }\n    y_masks = {\n        index: (points[:, 1] &gt;= y_grid[index] - cell_h_half)\n        &amp; (points[:, 1] &lt;= y_grid[index] + cell_h_half)\n        for index in range(N)\n    }\n    for i, (xi, yi) in enumerate(tqdm(grid_ind, desc=\"Building covering meshes\")):\n        # Find mesh points within the cell\n        mask = x_masks[xi] &amp; y_masks[yi]\n        z_candidates = points[mask, 2]\n        if len(z_candidates) &gt; 0:\n            z_p_values[i] = np.max(z_candidates) + z_buffer[0]\n            z_n_values[i] = np.min(z_candidates) + z_buffer[1]\n\n    # Create 3D points for the grid\n    grid_p_3d = np.column_stack([grid_points, z_p_values])\n    grid_n_3d = np.column_stack([grid_points, z_n_values])\n    # Remove points with no Z values\n    grid_p_3d = grid_p_3d[~np.isnan(z_p_values)]\n    grid_n_3d = grid_n_3d[~np.isnan(z_n_values)]\n\n    # Create pyvista point cloud and surface\n    return (\n        pv.PolyData(grid_p_3d).delaunay_2d(),\n        pv.PolyData(grid_n_3d).delaunay_2d(),\n    )\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.export_face_labels_vector","title":"<code>export_face_labels_vector(face_labels=None, export_file=None, export_crs=LAT_LON_CRS, label_names=None, ensure_non_overlapping=False, simplify_tol=0.0, drop_nan=True, vis=True, batched_unary_union_kwargs={'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}, vis_kwargs={})</code>","text":"<p>Export the labels for each face as a on-per-class multipolygon</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a nonzero element at (i, j) represents a class prediction for the ith faces and jth class</p> <code>None</code> <code>export_file</code> <code>PATH_TYPE</code> <p>Where to export. The extension must be a filetype that geopandas can write. Defaults to None, if unset, nothing will be written.</p> <code>None</code> <code>export_crs</code> <code>CRS</code> <p>What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.</p> <code>LAT_LON_CRS</code> <code>label_names</code> <code>Tuple</code> <p>Optional names, that are indexed by the labels. Defaults to None.</p> <code>None</code> <code>ensure_non_overlapping</code> <code>bool</code> <p>Should regions where two classes are predicted at different z heights be assigned to one class</p> <code>False</code> <code>simplify_tol</code> <code>float</code> <p>(float, optional): Tolerence in meters to use to simplify geometry</p> <code>0.0</code> <code>drop_nan</code> <code>bool</code> <p>Don't export the nan class, often used for background</p> <code>True</code> <code>vis</code> <code>bool</code> <p>should the result be visualzed</p> <code>True</code> <code>batched_unary_union_kwargs</code> <code>dict</code> <p>Keyword arguments for batched_unary_union_call</p> <code>{'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}</code> <code>vis_kwargs</code> <code>Dict</code> <p>keyword argmument dict for visualization</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the wrong number of faces labels are provided</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Merged data</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def export_face_labels_vector(\n    self,\n    face_labels: typing.Union[np.ndarray, None] = None,\n    export_file: PATH_TYPE = None,\n    export_crs: pyproj.CRS = LAT_LON_CRS,\n    label_names: typing.Tuple = None,\n    ensure_non_overlapping: bool = False,\n    simplify_tol: float = 0.0,\n    drop_nan: bool = True,\n    vis: bool = True,\n    batched_unary_union_kwargs: typing.Dict = {\n        \"batch_size\": 500000,\n        \"sort_by_loc\": True,\n        \"grid_size\": 0.05,\n        \"simplify_tol\": 0.05,\n    },\n    vis_kwargs: typing.Dict = {},\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n    Args:\n        face_labels (np.ndarray):\n            This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n            is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n            nonzero element at (i, j) represents a class prediction for the ith faces and jth\n            class\n        export_file (PATH_TYPE, optional):\n            Where to export. The extension must be a filetype that geopandas can write.\n            Defaults to None, if unset, nothing will be written.\n        export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n        label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n        ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n        simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n        drop_nan (bool, optional): Don't export the nan class, often used for background\n        vis: should the result be visualzed\n        batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n        vis_kwargs: keyword argmument dict for visualization\n\n    Raises:\n        ValueError: If the wrong number of faces labels are provided\n\n    Returns:\n        gpd.GeoDataFrame: Merged data\n    \"\"\"\n    # Compute the working projected CRS\n    # This is important because having things in meters makes things easier\n    self.logger.info(\"Computing working CRS\")\n    lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_CRS)[0]\n    working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n    # Try to extract face labels if not set\n    if face_labels is None:\n        face_labels = self.get_texture(request_vertex_texture=False)\n\n    # Check that the correct number of labels are provided\n    if face_labels.shape[0] != self.faces.shape[0]:\n        raise ValueError()\n\n    # Get the geospatial faces dataframe\n    faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n    self.logger.info(\"Creating dataframe of multipolygons\")\n\n    # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n    face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n    if face_labels_is_2d:\n        # Non-null columns\n        unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n    else:\n        face_labels = np.squeeze(face_labels)\n        unique_IDs = np.unique(face_labels)\n\n    if drop_nan:\n        # Drop nan from the list of IDs\n        unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n    multipolygon_list = []\n    # For each unique ID, aggregate all the faces together\n    # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n    # due to the large number of polygons\n    # Instead, we replace the default shapely.unary_union with our batched implementation\n    for unique_ID in tqdm(unique_IDs, desc=\"Merging faces for each class\"):\n        if face_labels_is_2d:\n            # Nonzero elements of the column\n            matching_face_mask = face_labels[:, unique_ID] &gt; 0\n        else:\n            # Elements that match the ID in question\n            matching_face_mask = face_labels == unique_ID\n        matching_face_inds = np.nonzero(matching_face_mask)[0]\n        matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n        list_of_polygons = matching_face_polygons.geometry.values\n        multipolygon = batched_unary_union(\n            list_of_polygons, **batched_unary_union_kwargs\n        )\n        multipolygon_list.append(multipolygon)\n\n    working_gdf = gpd.GeoDataFrame(\n        {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n    )\n\n    if label_names is not None:\n        names = [\n            (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n            for ID in working_gdf[CLASS_ID_KEY]\n        ]\n        working_gdf[CLASS_NAMES_KEY] = names\n\n    # Simplify the output geometry\n    if simplify_tol &gt; 0.0:\n        self.logger.info(\"Running simplification\")\n        working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n    # Make sure that the polygons are non-overlapping\n    if ensure_non_overlapping:\n        # TODO create a version that tie-breaks based on the number of predicted faces for each\n        # class and optionally the ratios of 3D to top-down areas for the input triangles.\n        self.logger.info(\"Ensuring non-overlapping polygons\")\n        working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n    # Transform from the working crs to export crs\n    export_gdf = working_gdf.to_crs(export_crs)\n\n    # Export if a file is provided\n    if export_file is not None:\n        ensure_containing_folder(export_file)\n        export_gdf.to_file(export_file)\n\n    # Vis if requested\n    if vis:\n        self.logger.info(\"Plotting\")\n        export_gdf.plot(\n            column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n            aspect=1,\n            legend=True,\n            **vis_kwargs,\n        )\n        plt.show()\n\n    return export_gdf\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.face_to_vert_texture","title":"<code>face_to_vert_texture(face_IDs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>face_IDs</code> <code>array</code> <p>(n_faces,) The integer IDs of the faces</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def face_to_vert_texture(self, face_IDs):\n    \"\"\"_summary_\n\n    Args:\n        face_IDs (np.array): (n_faces,) The integer IDs of the faces\n    \"\"\"\n    raise NotImplementedError()\n    # TODO figure how to have a NaN class that\n    for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n        # Find which faces are using this vertex\n        matching = np.sum(self.faces == i, axis=1)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_faces_2d_gdf","title":"<code>get_faces_2d_gdf(crs, include_3d_2d_ratio=False, data_dict={}, faces_mask=None, cache_data=False)</code>","text":"<p>Get a geodataframe of triangles for the 2D projection of each face of the mesh</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>Coordinate reference system of the dataframe</p> required <code>include_3d_2d_ratio</code> <code>bool</code> <p>Compute the ratio of the 3D area of the face to the 2D area. This relates to the slope of the face relative to horizontal. The computed data will be stored in the column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.</p> <code>False</code> <code>data_dict</code> <code>dict</code> <p>Additional information to add to the dataframe. It must be a dict where the keys are the names of the columns and the data is a np.ndarray of n_faces elemenets. Defaults to {}.</p> <code>{}</code> <code>faces_mask</code> <code>Union[ndarray, None]</code> <p>A binary mask corresponding to which faces to return. Used to improve runtime of creating the dataframe or downstream steps. Defaults to None.</p> <code>None</code> <code>cache_data</code> <code>bool</code> <p>Whether to cache expensive results in memory as object attributes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A dataframe for each triangular face</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_faces_2d_gdf(\n    self,\n    crs: pyproj.CRS,\n    include_3d_2d_ratio: bool = False,\n    data_dict: dict = {},\n    faces_mask: typing.Union[np.ndarray, None] = None,\n    cache_data: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n    Args:\n        crs (pyproj.CRS):\n            Coordinate reference system of the dataframe\n        include_3d_2d_ratio (bool, optional):\n            Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n            slope of the face relative to horizontal. The computed data will be stored in the\n            column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n        data_dict (dict, optional):\n            Additional information to add to the dataframe. It must be a dict where the keys\n            are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n            Defaults to {}.\n        faces_mask (typing.Union[np.ndarray, None], optional):\n            A binary mask corresponding to which faces to return. Used to improve runtime of\n            creating the dataframe or downstream steps. Defaults to None.\n        cache_data (bool):\n            Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n    Returns:\n        geopandas.GeoDataFrame: A dataframe for each triangular face\n    \"\"\"\n    # Computing this data can be slow, and we might call it multiple times. This is especially\n    # true for doing clustered polygon labeling\n    if cache_data:\n        mesh_hash = self.get_mesh_hash()\n        faces_mask_hash = hash(\n            faces_mask.tobytes() if faces_mask is not None else 0\n        )\n        # Create a key that uniquely identifies the relavant inputs\n        cache_key = (mesh_hash, faces_mask_hash, crs)\n\n        # See if the face polygons were in the cache. If not, None will be returned\n        cached_values = self.face_polygons_cache.get(cache_key)\n    else:\n        cached_values = None\n\n    if cached_values is not None:\n        face_polygons, faces = cached_values\n        logging.info(\"Using cached face polygons\")\n    else:\n        self.logger.info(\"Computing faces in working CRS\")\n        # Get the mesh vertices in the desired export CRS\n        verts_in_crs = self.get_vertices_in_CRS(crs)\n        # Get a triangle in geospatial coords for each face\n        # (n_faces, 3 points, xyz)\n        faces = verts_in_crs[self.faces]\n\n        # Select only the requested faces\n        if faces_mask is not None:\n            faces = faces[faces_mask]\n\n        # Extract the first two columns and convert them to a list of tuples of tuples\n        faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n        face_polygons = [\n            Polygon(face_tuple)\n            for face_tuple in tqdm(\n                faces_2d_tuples, desc=f\"Converting faces to polygons\"\n            )\n        ]\n        self.logger.info(\"Creating dataframe of faces\")\n\n        if cache_data:\n            # Save computed data to the cache for the future\n            self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n    # Remove data corresponding to masked faces\n    if faces_mask is not None:\n        data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n    # Compute the ratio between the 3D area and the projected top-down 2D area\n    if include_3d_2d_ratio:\n        if cache_data:\n            # Check if ratios are cached\n            ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n        else:\n            ratios = None\n\n        # Ratios need to be computed\n        if ratios is None:\n            ratios = []\n            for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                area, area_2d = compute_3D_triangle_area(face)\n                ratios.append(area / area_2d)\n\n            if cache_data:\n                self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n        # Add the ratios to the data dict\n        data_dict[RATIO_3D_2D_KEY] = ratios\n\n    # Create the dataframe\n    faces_gdf = gpd.GeoDataFrame(\n        data=data_dict,\n        geometry=face_polygons,\n        crs=crs,\n    )\n\n    return faces_gdf\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_height_above_ground","title":"<code>get_height_above_ground(DTM_file, threshold=None)</code>","text":"<p>Return height above ground for a points in the mesh and a given DTM</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the digital terrain model raster</p> required <code>threshold</code> <code>float</code> <p>If not None, return a boolean mask for points under this height. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Either the height above ground or a boolean mask for ground points</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_height_above_ground(\n    self, DTM_file: PATH_TYPE, threshold: float = None\n) -&gt; np.ndarray:\n    \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the digital terrain model raster\n        threshold (float, optional):\n            If not None, return a boolean mask for points under this height. Defaults to None.\n\n    Returns:\n        np.ndarray: Either the height above ground or a boolean mask for ground points\n    \"\"\"\n    # Get the height from the DTM and the points in the same CRS\n    DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n        DTM_file, return_verts_in_CRS=True\n    )\n    # Extract the vertex height as the third channel\n    verts_height = verts_in_raster_CRS[:, 2]\n    # Subtract the two to get the height above ground\n    height_above_ground = verts_height - DTM_heights\n\n    # If the threshold is not None, return a boolean mask that is true for ground points\n    if threshold is not None:\n        # Return boolean mask\n        # TODO see if this will break for nan values\n        return height_above_ground &lt; threshold\n    # Return height above ground\n    return height_above_ground\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_mesh_hash","title":"<code>get_mesh_hash()</code>","text":"<p>Generates a hash value for the mesh based on its points and faces Returns:     int: A hash value representing the current mesh.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_mesh_hash(self):\n    \"\"\"Generates a hash value for the mesh based on its points and faces\n    Returns:\n        int: A hash value representing the current mesh.\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(self.pyvista_mesh.points.tobytes())\n    hasher.update(self.pyvista_mesh.faces.tobytes())\n    return hasher.hexdigest()\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_mesh_in_cameras_coords","title":"<code>get_mesh_in_cameras_coords(cameras, inplace=False)</code>","text":"<p>Obtain a mesh in the same local coordinate frame convention as the camera set.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Camera or camera set to match the convention of.</p> required <code>inplace</code> <code>bool</code> <p>Should the object be update to reflect this convetion. Otherwise, a pyvista mesh is returned and the self object remains unchanged. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[PolyData]</code> <p>typing.Optional[pv.PolyData]: Mesh in the camera's coordinate system if inplace=False</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_mesh_in_cameras_coords(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    inplace: bool = False,\n) -&gt; typing.Optional[pv.PolyData]:\n    \"\"\"Obtain a mesh in the same local coordinate frame convention as the camera set.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Camera or camera set to match the convention of.\n        inplace (bool, optional):\n            Should the object be update to reflect this convetion. Otherwise, a pyvista mesh is\n            returned and the self object remains unchanged. Defaults to False.\n\n    Returns:\n        typing.Optional[pv.PolyData]: Mesh in the camera's coordinate system if inplace=False\n    \"\"\"\n    # Reproject the mesh into ECEF\n    mesh = self.reproject_CRS(EARTH_CENTERED_EARTH_FIXED_CRS, inplace=False)\n\n    # Get the inverse 4x4 transform, which maps from Earth Centered, Earth Fixed (EPSG:4978)\n    # to the coordinates that the cameras are in\n    local_to_epsg_4978_transform = cameras.get_local_to_epsg_4978_transform()\n    epsg_4978_to_camera = np.linalg.inv(local_to_epsg_4978_transform)\n    # Transform the mesh using this transform\n    mesh = mesh.transform(epsg_4978_to_camera, inplace=False)\n\n    if inplace:\n        # Overwrite the mesh with the updated version\n        self.pyvista_mesh = mesh\n        # Indicate that there is no longer a valid CRS\n        self.CRS = None\n        # Return None to match common convention for inplace methods\n        return None\n\n    return mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_values_for_verts_from_vector","title":"<code>get_values_for_verts_from_vector(vector_source, column_names)</code>","text":"<p>Get the value from a dataframe for each vertex</p> <p>Parameters:</p> Name Type Description Default <code>vector_source</code> <code>Union[GeoDataFrame, PATH_TYPE]</code> <p>geo data frame or path to data that can be loaded by geopandas</p> required <code>column_names</code> <code>Union[str, List[str]]</code> <p>Which columns to obtain data from</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray | dict[str, np.ndarray]: An array or dict of string-&gt;array mappings, with one element per vector file polygon</p> <code>ndarray</code> <p>np.ndarray | dict[str, np.ndarray]: An array or dict of string-&gt;array mappings, with one element per mesh vertex</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_values_for_verts_from_vector(\n    self,\n    vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n    column_names: typing.Union[str, typing.List[str]],\n) -&gt; np.ndarray:\n    \"\"\"Get the value from a dataframe for each vertex\n\n    Args:\n        vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n        column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n    Returns:\n        np.ndarray | dict[str, np.ndarray]:\n            An array or dict of string-&gt;array mappings, with one element per vector file polygon\n        np.ndarray | dict[str, np.ndarray]:\n            An array or dict of string-&gt;array mappings, with one element per mesh vertex\n    \"\"\"\n    # Lead the vector data if not already provided in memory\n    if isinstance(vector_source, gpd.GeoDataFrame):\n        gdf = vector_source\n    else:\n        # This will error if not readable\n        gdf = gpd.read_file(vector_source)\n\n    # Infer or standardize the column names\n    if column_names is None:\n        # Check if there is only one real column\n        if len(gdf.columns) == 2:\n            column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n        else:\n            # Log as well since this may be caught by an exception handler,\n            # and it's a user error that can be corrected\n            self.logger.error(\n                \"No column name provided and ambigious which column to use\"\n            )\n            raise ValueError(\n                \"No column name provided and ambigious which column to use\"\n            )\n    # If only one column is provided, make it a one-length list\n    elif isinstance(column_names, str):\n        column_names = [column_names]\n\n    # Get a dataframe of vertices\n    verts_df = self.get_verts_geodataframe(gdf.crs)\n\n    # See which vertices are in the geopolygons\n    points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n    # Get the index array\n    index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n    # This is one entry per vertex\n    labeled_verts_dict = {}\n    all_values_dict = {}\n    # Extract the data from each\n    for column_name in column_names:\n        # Create an array corresponding to all the points and initialize to NaN\n        column_values = points_in_polygons_gdf[column_name]\n        # TODO clean this up\n        if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n            # TODO be set to the default value for the type of the column\n            null_value = \"null\"\n        elif column_values.dtype == int:\n            null_value = 255\n        else:\n            null_value = np.nan\n        # Create an array, one per vertex, with the null value\n        values = np.full(\n            shape=verts_df.shape[0],\n            dtype=column_values.dtype,\n            fill_value=null_value,\n        )\n        # Assign the labeled values\n        values[index_array] = column_values\n\n        # Record the results\n        labeled_verts_dict[column_name] = values\n        all_values_dict[column_name] = gdf[column_name]\n\n    # If only one name was requested, just return that\n    if len(column_names) == 1:\n        labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n        all_values = np.array(list(all_values_dict.values())[0])\n\n        return labeled_verts, all_values\n    # Else return a dict of all requested values\n    return labeled_verts_dict, all_values_dict\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vert_values_from_raster_file","title":"<code>get_vert_values_from_raster_file(raster_file, return_verts_in_CRS=False, nodata_fill_value=np.nan)</code>","text":"<p>Compute the height above groun for each point on the mesh</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>The path to the geospatial raster file.</p> required <code>return_verts_in_CRS</code> <code>bool</code> <p>Return the vertices transformed into the raster CRS</p> <code>False</code> <code>nodata_fill_value</code> <code>float</code> <p>Set data defined by the opened file as NODATAVAL to this value</p> <code>nan</code> <p>Returns:</p> Type Description <p>np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)</p> <p>np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vert_values_from_raster_file(\n    self,\n    raster_file: PATH_TYPE,\n    return_verts_in_CRS: bool = False,\n    nodata_fill_value: float = np.nan,\n):\n    \"\"\"Compute the height above groun for each point on the mesh\n\n    Args:\n        raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n        return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n        nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n    Returns:\n        np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n        np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n    \"\"\"\n    # Open the DTM file\n    raster = rio.open(raster_file)\n    # Get the mesh points in the coordinate reference system of the DTM\n    verts_in_raster_CRS = self.get_vertices_in_CRS(\n        raster.crs, force_easting_northing=True\n    )\n\n    # Get the points as a list\n    easting_points = verts_in_raster_CRS[:, 0].tolist()\n    northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n    # Zip them together\n    zipped_locations = zip(easting_points, northing_points)\n    sampling_iter = tqdm(\n        zipped_locations,\n        desc=f\"Sampling values from raster {raster_file}\",\n        total=verts_in_raster_CRS.shape[0],\n    )\n    # Sample the raster file and squeeze if single channel\n    sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n    # Set nodata locations to nan\n    # TODO figure out if it will ever be a problem to take the first value\n    sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n        nodata_fill_value\n    )\n\n    if return_verts_in_CRS:\n        return sampled_raster_values, verts_in_raster_CRS\n\n    return sampled_raster_values\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vertices_in_CRS","title":"<code>get_vertices_in_CRS(output_CRS, force_easting_northing=True)</code>","text":"<p>Return the coordinates of the mesh vertices in a given CRS</p> <p>Parameters:</p> Name Type Description Default <code>output_CRS</code> <code>CRS</code> <p>The coordinate reference system to transform to</p> required <code>force_easting_northing</code> <code>bool</code> <p>Ensure that the returned points are east first, then north</p> <code>True</code> <p>Returns:</p> Type Description <p>np.ndarray: (n_points, 3)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vertices_in_CRS(\n    self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n):\n    \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n    Args:\n        output_CRS (pyproj.CRS): The coordinate reference system to transform to\n        force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n    Returns:\n        np.ndarray: (n_points, 3)\n    \"\"\"\n    # Reproject the mesh\n    reprojected_mesh = self.reproject_CRS(output_CRS, inplace=False)\n    verts_in_output_CRS = np.array(reprojected_mesh.points)\n\n    # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n    # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n    # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n    if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n        # Swap first two columns\n        verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n    return verts_in_output_CRS\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_verts_geodataframe","title":"<code>get_verts_geodataframe(crs)</code>","text":"<p>Obtain the vertices as a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>The CRS to use</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A dataframe with all the vertices</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n    \"\"\"Obtain the vertices as a dataframe\n\n    Args:\n        crs (pyproj.CRS): The CRS to use\n\n    Returns:\n        gpd.GeoDataFrame: A dataframe with all the vertices\n    \"\"\"\n    # Get the vertices in the same CRS as the geofile\n    verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n    df = pd.DataFrame(\n        {\n            \"east\": verts_in_geopolygon_crs[:, 0],\n            \"north\": verts_in_geopolygon_crs[:, 1],\n        }\n    )\n    # Create a column of Point objects to use as the geometry\n    df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n    points = gpd.GeoDataFrame(df, crs=crs)\n\n    # Add an index column because the normal index will not be preserved in future operations\n    points[VERT_ID] = df.index\n\n    return points\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_ground_class","title":"<code>label_ground_class(DTM_file, height_above_ground_threshold, labels=None, only_label_existing_labels=True, ground_class_name='ground', ground_ID=None, set_mesh_texture=False)</code>","text":"<p>Set vertices to a potentially-new class with a thresholded height above the DTM. TODO, consider handling face textures as well</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the DTM file</p> required <code>height_above_ground_threshold</code> <code>float</code> <p>Height (meters) above that DTM that points below are considered ground</p> required <code>labels</code> <code>Union[None, ndarray]</code> <p>Vertex texture, otherwise will be queried from mesh. Defaults to None.</p> <code>None</code> <code>only_label_existing_labels</code> <code>bool</code> <p>Only label points that already have non-null labels. Defaults to True.</p> <code>True</code> <code>ground_class_name</code> <code>str</code> <p>The potentially-new ground class name. Defaults to \"ground\".</p> <code>'ground'</code> <code>ground_ID</code> <code>Union[None, int]</code> <p>What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The updated labels</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_ground_class(\n    self,\n    DTM_file: PATH_TYPE,\n    height_above_ground_threshold: float,\n    labels: typing.Union[None, np.ndarray] = None,\n    only_label_existing_labels: bool = True,\n    ground_class_name: str = \"ground\",\n    ground_ID: typing.Union[None, int] = None,\n    set_mesh_texture: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Set vertices to a potentially-new class with a thresholded height above the DTM.\n    TODO, consider handling face textures as well\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the DTM file\n        height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n        labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n        only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n        ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n        ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n    Returns:\n        np.ndarray: The updated labels\n    \"\"\"\n\n    if labels is None:\n        # Default to using vertex labels since it's the native way to check height above the DTM\n        use_vertex_labels = True\n    elif labels is not None:\n        # Check the size of the input labels and set what type they are. Note this could override existing value\n        if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n            use_vertex_labels = True\n        elif labels.shape[0] == self.faces.shape[0]:\n            use_vertex_labels = False\n        else:\n            raise ValueError(\n                \"Labels were provided but didn't match the shape of vertices or faces\"\n            )\n\n    # if a labels are not provided, get it from the mesh\n    if labels is None:\n        # Get the vertex textures from the mesh\n        labels = self.get_texture(\n            request_vertex_texture=use_vertex_labels,\n        )\n\n    # Compute which vertices are part of the ground by thresholding the height above the DTM\n    ground_mask = self.get_height_above_ground(\n        DTM_file=DTM_file, threshold=height_above_ground_threshold\n    )\n    # If we needed a mask for the faces, compute that instead\n    if not use_vertex_labels:\n        ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n            bool\n        )\n\n    # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n    if only_label_existing_labels:\n        # Find which vertices are labeled\n        is_labeled = np.isfinite(labels[:, 0])\n        # Find which points are ground that were previously labeled as something else\n        ground_mask = np.logical_and(is_labeled, ground_mask)\n\n    # Get the existing label names\n    IDs_to_labels = self.get_IDs_to_labels()\n\n    if IDs_to_labels is None and ground_ID is None:\n        # This means that the label is continous, so the concept of ID is meaningless\n        ground_ID = np.nan\n    elif IDs_to_labels is not None and ground_class_name in IDs_to_labels.values():\n        # If the ground class name is already in the list, set newly-predicted vertices to that class\n        # Get the dictionary mapping in the reverse direction\n        labels_to_IDs = {v: k for k, v in IDs_to_labels.items()}\n        # Determine the ID corresponding to the ground class name\n        ground_ID = labels_to_IDs.get(ground_class_name)\n    elif IDs_to_labels is not None:\n        # If the label names are present, and the class is not already included, add it as the last element\n        if ground_ID is None:\n            # Set it to the first unused ID\n            # TODO improve this since it should be the max plus one\n            ground_ID = len(IDs_to_labels)\n\n    self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n    # Replace mask for ground_vertices\n    labels[ground_mask, 0] = ground_ID\n\n    # Optionally apply the texture to the mesh\n    if set_mesh_texture:\n        # TODO look into why this shouldn't update the IDs to labels.\n        # I guess because it may have been initially user-provided.\n        self.set_texture(labels, update_IDs_to_labels=False)\n\n    return labels\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2.0)</code>","text":"<p>Assign a class label to polygons using labels per face</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2.0,\n):\n    \"\"\"Assign a class label to polygons using labels per face\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Premptive error checking before expensive operations\n    face_labels = np.squeeze(face_labels)\n    if face_labels.ndim != 1:\n        raise ValueError(\n            f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n        )\n    if face_weighting is not None:\n        face_weighting = np.squeeze(face_weighting)\n        if face_weighting.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n            )\n\n    # Ensure that the input is a geopandas dataframe\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract just the geometry\n    polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n    # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n    faces_mask = np.isfinite(face_labels)\n\n    # Get the faces of the mesh as a geopandas dataframe\n    # Include the predicted face labels as a column in the dataframe\n    faces_2d_gdf = self.get_faces_2d_gdf(\n        polygons_gdf.crs,\n        include_3d_2d_ratio=True,\n        data_dict={CLASS_ID_KEY: face_labels},\n        faces_mask=faces_mask,\n        cache_data=True,\n    )\n\n    # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n    if face_weighting is not None:\n        face_weighting = face_weighting[faces_mask]\n        faces_2d_gdf[\"face_weighting\"] = (\n            faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n        )\n    # If not, just use the ratio\n    else:\n        faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n    # Set the precision to avoid approximate coliniearity errors\n    faces_2d_gdf.geometry = shapely.set_precision(\n        faces_2d_gdf.geometry.values, 1e-6\n    )\n    polygons_gdf.geometry = shapely.set_precision(\n        polygons_gdf.geometry.values, 1e-6\n    )\n\n    # Set the ID field so it's available after the overlay operation\n    # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n    # one and the index would not start from 0\n    polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n    # Since overlay is expensive, we first discard faces that are not near the polygons\n\n    # Dissolve the polygons to form one ROI\n    merged_polygons = polygons_gdf.dissolve()\n    # Try to decrease the number of elements in the polygon by expanding\n    # and then simplifying the number of elements in the polygon\n    merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n    merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n    # Determine which face IDs intersect the ROI. This is slow\n    start = time()\n    self.logger.info(\"Starting to subset to ROI\")\n\n    # Check which faces are fully within the buffered regions around the query polygons\n    # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n    # to compute more information than something like gpd.within\n    contained_faces = gpd.sjoin(\n        faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n    )[\"index_right\"].notna()\n    faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n    self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n    start = time()\n    self.logger.info(\"Starting `overlay`\")\n    if sjoin_overlay:\n        overlay = gpd.sjoin(\n            faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n        )\n        self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n    else:\n        # Drop faces not included\n        overlay = polygons_gdf.overlay(\n            faces_2d_gdf, how=\"identity\", keep_geom_type=False\n        )\n        self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n    # Drop nan, for geometries that don't intersect the polygons\n    overlay.dropna(inplace=True)\n    # Compute the weighted area for each face, which may have been broken up by the overlay\n    overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n    # Extract only the neccessary columns\n    overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n    aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n    # Compute the highest weighted class prediction\n    # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n    max_rows = aggregated_data.loc[\n        aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n            \"weighted_area\"\n        ].idxmax()\n    ].reset_index()\n\n    # Make the class predictions a list of IDs with nans where no information is available\n    pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n    pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n    predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n    predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n        pred_subset_IDs\n    )\n    predicted_class_IDs = predicted_class_IDs.tolist()\n\n    # Post-process to string label names if requested and IDs_to_labels exists\n    if return_class_labels and (\n        (IDs_to_labels := self.get_IDs_to_labels()) is not None\n    ):\n        # convert the IDs into labels\n        # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n        predicted_class_IDs = [\n            (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n            for pi in predicted_class_IDs\n        ]\n    return predicted_class_IDs\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_mesh","title":"<code>load_mesh(mesh, input_CRS, downsample_target=1.0, shift=None, ROI=None, ROI_buffer_meters=0, ROI_simplify_tol_meters=2)</code>","text":"<p>Load the pyvista mesh and create the texture</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh or actual mesh</p> required <code>downsample_target</code> <code>float</code> <p>What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).</p> <code>1.0</code> <code>shift</code> <code>Union[ndarray, None]</code> <p>Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex coordinates by this amount in the input_CRS frame. Defaults to None.</p> <code>None</code> <code>ROI</code> <p>See select_mesh_ROI. Defaults to None</p> <code>None</code> <code>ROI_buffer_meters</code> <p>See select_mesh_ROI. Defaults to 0.</p> <code>0</code> <code>ROI_simplify_tol_meters</code> <p>See select_mesh_ROI. Defaults to 2.</p> <code>2</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_mesh(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    input_CRS: pyproj.CRS,\n    downsample_target: float = 1.0,\n    shift: typing.Union[np.ndarray, None] = None,\n    ROI=None,\n    ROI_buffer_meters=0,\n    ROI_simplify_tol_meters=2,\n):\n    \"\"\"Load the pyvista mesh and create the texture\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n            Path to the mesh or actual mesh\n        downsample_target (float, optional):\n            What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n        shift (typing.Union[np.ndarray, None], optional):\n            Represents an [x, y, z] shift as a (3,) array. If provided, shift all vertex\n            coordinates by this amount in the input_CRS frame. Defaults to None.\n        ROI:\n            See select_mesh_ROI. Defaults to None\n        ROI_buffer_meters:\n            See select_mesh_ROI. Defaults to 0.\n        ROI_simplify_tol_meters:\n            See select_mesh_ROI. Defaults to 2.\n    \"\"\"\n    self.CRS = input_CRS\n\n    if isinstance(mesh, pv.PolyData):\n        # If a mesh is provided directly, copy it so input mesh isn't modified\n        self.pyvista_mesh = mesh.copy()\n    else:\n        # Load the mesh using pyvista\n        # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n        self.logger.info(\"Reading the mesh\")\n        self.pyvista_mesh = pv.read(mesh)\n\n    # Up-cast to avoid quantization errors after we shift or transform to larger values\n    self.pyvista_mesh.points = self.pyvista_mesh.points.astype(float)\n\n    # If a shift is provided, shift all mesh vertices by this amount\n    if shift is not None:\n        self.pyvista_mesh.points += shift\n\n    self.logger.info(\"Selecting an ROI from mesh\")\n    # Select a region of interest if needed\n    self.pyvista_mesh = self.select_mesh_ROI(\n        region_of_interest=ROI,\n        buffer_meters=ROI_buffer_meters,\n        simplify_tol_meters=ROI_simplify_tol_meters,\n    )\n\n    # Reproject to a meters-based CRS. TODO consider if there's a better option than ECEF.\n    self.reproject_CRS(target_CRS=EARTH_CENTERED_EARTH_FIXED_CRS, inplace=True)\n\n    # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n    if downsample_target != 1.0:\n        # TODO try decimate_pro and compare quality and runtime\n        # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n        self.logger.info(\"Downsampling the mesh\")\n        # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n        downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n            target_reduction=(1 - downsample_target)\n        )\n        self.logger.info(\n            f\"Requested downsampling {downsample_target}, actual downsampling {downsampled_mesh_without_textures.n_points / self.pyvista_mesh.n_points}\"\n        )\n        self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n    self.logger.info(\"Extracting faces from mesh\")\n    # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n    self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_texture","title":"<code>load_texture(texture, texture_column_name=None, IDs_to_labels=None)</code>","text":"<p>Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other    one will be left as None</p> <p>Parameters:</p> Name Type Description Default <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>This is either a numpy array or a file to one of the following * A numpy array file in \".npy\" format * A vector file readable by geopandas and a label(s) specifying which column to use.   This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between   regions with different labels. These regions may be assigned based on the order of the rows. * A raster file readable by rasterio. We may want to support using a subset of bands</p> required <code>texture_column_name</code> <code>Union[None, PATH_TYPE]</code> <p>The column to use as the label for a vector data input</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Dictionary mapping from integer IDs to string class names</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_texture(\n    self,\n    texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n    texture_column_name: typing.Union[None, PATH_TYPE] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n):\n    \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n       one will be left as None\n\n    Args:\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n            * A numpy array file in \".npy\" format\n            * A vector file readable by geopandas and a label(s) specifying which column to use.\n              This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n              regions with different labels. These regions may be assigned based on the order of the rows.\n            * A raster file readable by rasterio. We may want to support using a subset of bands\n        texture_column_name: The column to use as the label for a vector data input\n        IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n    \"\"\"\n    # The easy case, a texture is passed in directly\n    if isinstance(texture, np.ndarray):\n        self.set_texture(\n            texture_array=texture,\n            IDs_to_labels=IDs_to_labels,\n        )\n    # If the texture is None, try to load it from the mesh\n    # Note that this requires us to have not decimated yet\n    elif texture is None:\n        # See if the mesh has a texture, else this will be None\n        texture_array = self.pyvista_mesh.active_scalars\n\n        if texture_array is not None:\n            # Check if this was a really one channel that had to be tiled to\n            # three for saving\n            if len(texture_array.shape) == 2:\n                min_val_per_row = np.min(texture_array, axis=1)\n                max_val_per_row = np.max(texture_array, axis=1)\n                if np.array_equal(min_val_per_row, max_val_per_row):\n                    # This is supposted to be one channel\n                    texture_array = texture_array[:, 0].astype(float)\n                    # Set any values that are the ignore int value to nan\n            texture_array = texture_array.astype(float)\n            texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n            self.set_texture(\n                texture_array,\n                IDs_to_labels=IDs_to_labels,\n            )\n        else:\n            if IDs_to_labels is not None:\n                self.IDs_to_labels = IDs_to_labels\n            # Assume that no texture will be needed, consider printing a warning\n            self.logger.warn(\"No texture provided\")\n    else:\n        # Try handling all the other supported filetypes\n        texture_array = None\n        all_values = None\n\n        # Name of scalar in the mesh\n        try:\n            self.logger.warn(\n                \"Trying to read texture as a scalar from the pyvista mesh:\"\n            )\n            texture_array = self.pyvista_mesh[str(texture)]\n            self.logger.warn(\"- success\")\n        except (KeyError, ValueError):\n            self.logger.warn(\"- failed\")\n\n        # Numpy file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as a numpy file:\")\n                texture_array = np.load(texture, allow_pickle=True)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Vector file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as vector file:\")\n                # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                texture_array, all_values = self.get_values_for_verts_from_vector(\n                    column_names=texture_column_name,\n                    vector_source=texture,\n                )\n                self.logger.warn(\"- success\")\n            except (IndexError, fiona.errors.DriverError):\n                self.logger.warn(\"- failed\")\n\n        # Raster file\n        if texture_array is None:\n            try:\n                # TODO\n                self.logger.warn(\"Trying to read as texture as raster file: \")\n                texture_array = self.get_vert_values_from_raster_file(texture)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Error out if not set, since we assume the intent was to have a texture at this point\n        if texture_array is None:\n            raise ValueError(f\"Could not load texture for {texture}\")\n\n        # This will error if something is wrong with the texture that was loaded\n        self.set_texture(\n            texture_array,\n            all_discrete_texture_values=all_values,\n            IDs_to_labels=IDs_to_labels,\n        )\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.pix2face","title":"<code>pix2face(cameras, mesh=None, render_img_scale=1, save_to_cache=False, cache_folder=CACHE_FOLDER, distortion_set=None, apply_distortion=True)</code>","text":"<p>Compute the face that a ray from each pixel would intersect for each camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>A single camera or set of cameras. For each camera, the correspondences between pixels and the face IDs of the mesh will be computed. The images of all cameras are assumed to be the same size.</p> required <code>render_img_scale</code> <code>float</code> <p>Create a pix2face map that is this fraction of the original image scale. Defaults to 1.</p> <code>1</code> <code>save_to_cache</code> <code>bool</code> <p>Should newly-computed values be saved to the cache. This may speed up future operations but can take up 100s of GBs of space. Defaults to False.</p> <code>False</code> <code>cache_folder</code> <code>(PATH_TYPE, None)</code> <p>Where to check for and save to cached data. Only applicable if use_cache=True. Defaults to CACHE_FOLDER</p> <code>CACHE_FOLDER</code> <code>distortion_set</code> <code>PhotogrammetryCameraSet</code> <p>camera set used for calculating and caching the distortion/undistortion maps. This is only required if apply_distortion is True. Note that if you are calling pix2face on batches, you should pass the full camera set in as the distortion_set so that the maps are cached once.</p> <code>None</code> <code>apply_distortion</code> <code>bool</code> <p>Should distortion correction be applied. This is expensive but can be neccesary for some camera models which significantly differ from a pinhole model.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: For each camera, there is an array that is the shape of an image and</p> <code>ndarray</code> <p>contains the integer face index for the ray originating at that pixel. Any pixel for</p> <code>ndarray</code> <p>which the given ray does not intersect a face is given a value of -1. If the input is</p> <code>ndarray</code> <p>a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is</p> <code>ndarray</code> <p>(n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def pix2face(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    mesh: typing.Optional[pv.PolyData] = None,\n    render_img_scale: float = 1,\n    save_to_cache: bool = False,\n    cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n    distortion_set: typing.Optional[PhotogrammetryCameraSet] = None,\n    apply_distortion: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            A single camera or set of cameras. For each camera, the correspondences between\n            pixels and the face IDs of the mesh will be computed. The images of all cameras\n            are assumed to be the same size.\n        render_img_scale (float, optional):\n            Create a pix2face map that is this fraction of the original image scale. Defaults\n            to 1.\n        save_to_cache (bool, optional):\n            Should newly-computed values be saved to the cache. This may speed up future operations\n            but can take up 100s of GBs of space. Defaults to False.\n        cache_folder ((PATH_TYPE, None), optional):\n            Where to check for and save to cached data. Only applicable if use_cache=True.\n            Defaults to CACHE_FOLDER\n        distortion_set (PhotogrammetryCameraSet, optional): camera set used for calculating\n            and caching the distortion/undistortion maps. This is only required if apply_distortion\n            is True. Note that if you are calling pix2face on batches, you should pass the\n            full camera set in as the distortion_set so that the maps are cached once.\n        apply_distortion (bool, optional):\n            Should distortion correction be applied. This is expensive but can be neccesary for\n            some camera models which significantly differ from a pinhole model.\n\n    Returns:\n        np.ndarray: For each camera, there is an array that is the shape of an image and\n        contains the integer face index for the ray originating at that pixel. Any pixel for\n        which the given ray does not intersect a face is given a value of -1. If the input is\n        a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n        (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n    \"\"\"\n\n    # Create a local mesh if it hasn't been created yet\n    if mesh is None:\n        mesh = self.get_mesh_in_cameras_coords(cameras)\n\n    # If a set of cameras is passed in, call this method on each camera and concatenate\n    # Other derived methods might be able to compute a batch of renders at once, but pyvista\n    # cannot as far as I can tell.\n    # Note that all inputs to pix2face need to be replicated here or those features won't be\n    # passed on.\n    if isinstance(cameras, PhotogrammetryCameraSet):\n        pix2face_list = [\n            self.pix2face(\n                cameras=camera,\n                mesh=mesh,\n                render_img_scale=render_img_scale,\n                save_to_cache=save_to_cache,\n                cache_folder=cache_folder,\n                distortion_set=distortion_set,\n                apply_distortion=apply_distortion,\n            )\n            for camera in cameras\n        ]\n        pix2face = np.stack(pix2face_list, axis=0)\n        return pix2face\n\n    ## Single camera case\n\n    # Check if the cache contains a valid pix2face for the camera based on the dependencies\n    # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n    # The cache will generate a unique key for each combination of the dependencies\n    # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n    # If no match is found, recompute pix2face\n    # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n    mesh_hash = self.get_mesh_hash()\n    camera_hash = cameras.get_camera_hash()\n    cacher = ub.Cacher(\n        \"pix2face\",\n        depends=[mesh_hash, camera_hash, render_img_scale],\n        dpath=cache_folder,\n        verbose=0,\n    )\n    pix2face = cacher.tryload(on_error=\"clear\")\n    ## Cache is valid\n    if pix2face is not None:\n        return pix2face\n\n    # This needs to be an attribute of the class because creating a large number of plotters\n    # results in an un-fixable memory leak.\n    # See https://github.com/pyvista/pyvista/issues/2252\n    # The first step is to clear it\n    self.pix2face_plotter.clear()\n    # This is important so there aren't intermediate values\n    self.pix2face_plotter.disable_anti_aliasing()\n    # Set the camera to the corresponding viewpoint\n    self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n    ## Compute the base 256 encoding of the face ID\n    n_faces = self.faces.shape[0]\n    ID_values = np.arange(n_faces)\n\n    # determine how many channels will be required to represent the number of faces\n    n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n    channel_multipliers = [256**i for i in range(n_channels)]\n\n    # Compute the encoding of each value, least significant value first\n    base_256_encoding = [\n        np.mod(np.floor(ID_values / m).astype(int), 256)\n        for m in channel_multipliers\n    ]\n\n    # ensure that there's a multiple of three channels\n    n_padding = int(np.ceil(n_channels / 3.0) * 3 - n_channels)\n    base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n    # Assume that all images are the same size\n    image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n    # Initialize pix2face\n    pix2face = np.zeros(image_size, dtype=int)\n    # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n    for chunk_ind in range(int(len(base_256_encoding) / 3)):\n        chunk_scalars = np.stack(\n            base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n        ).astype(np.uint8)\n        # Add the mesh with the associated scalars\n        self.pix2face_plotter.add_mesh(\n            mesh,\n            scalars=chunk_scalars.copy(),\n            rgb=True,\n            diffuse=0.0,\n            ambient=1.0,\n        )\n\n        # Perform rendering, this is the slow step\n        rendered_img = self.pix2face_plotter.screenshot(\n            window_size=(image_size[1], image_size[0]),\n        )\n        # Take the rendered values and interpret them as the encoded value\n        # Make sure to not try to interpret channels that are not used in the encoding\n        channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n        for i in range(channels_to_decode):\n            channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n            channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n            pix2face += channel_value\n\n    # Mask out pixels for which the mesh was not visible\n    # This is because the background will render as white\n    # If there happen to be an exact power of (256^3) number of faces, the last one may get\n    # erronously masked. This seems like a minimal concern but it could be addressed by adding\n    # another channel or something like that\n    pix2face[pix2face &gt; n_faces] = -1\n\n    if save_to_cache:\n        # Save the most recently computed pix2face correspondance in the cache\n        cacher.save(pix2face)\n\n    if apply_distortion:\n        # Warp the pix2face mask so it matches the warping of the real image which does not\n        # conform to the pinhole model.\n        # Note this step is slow, and especially on the first iteration which may take multiple\n        # minutes.\n        pix2face = distortion_set.warp_dewarp_image(\n            camera=cameras,\n            input_image=pix2face,\n            warped_to_ideal=False,\n            fill_value=-1,\n            interpolation_order=0,  # nearest neighbor interpolation\n            image_scale=render_img_scale,\n        )\n\n    return pix2face\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.project_images","title":"<code>project_images(cameras, batch_size=1, aggregate_img_scale=1, check_null_image=False, **pix2face_kwargs)</code>","text":"<p>Find the per-face projection for each of a set of images and associated camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to project images from. cam.get_image() will be called on each one</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>check_null_image</code> <code>bool</code> <p>Only do indexing if there are non-null image values. This adds additional overhead, but can save the expensive operation of indexing in cases where it would be a no-op.</p> <code>False</code> <p>Yields:</p> Type Description <p>np.ndarray: The per-face projection of an image in the camera set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def project_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    check_null_image: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to project images from. cam.get_image() will be called on each one\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        check_null_image (bool, optional):\n            Only do indexing if there are non-null image values. This adds additional overhead,\n            but can save the expensive operation of indexing in cases where it would be a no-op.\n\n    Yields:\n        np.ndarray: The per-face projection of an image in the camera set\n    \"\"\"\n    # Create a local mesh\n    mesh = self.get_mesh_in_cameras_coords(cameras)\n\n    n_faces = self.faces.shape[0]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_inds = list(range(batch_start, batch_start + batch_size))\n        batch_cameras = cameras.get_subset_cameras(batch_inds)\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            mesh=mesh,\n            render_img_scale=aggregate_img_scale,\n            **pix2face_kwargs,\n        )\n        for i, pix2face in enumerate(batch_pix2face):\n            img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n            n_channels = 1 if img.ndim == 2 else img.shape[-1]\n            textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n            # Only do the expensive indexing step if there are finite values in the image. This is most\n            # significant for sparse detection tasks where some images may have no real data\n            if not check_null_image or np.any(np.isfinite(img)):\n                flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                flat_pix2face = pix2face.flatten()\n                # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                # my guess is the later pixel in the flattened array will override the former\n                # TODO make sure that null pix2face values are handled properly\n                textured_faces[flat_pix2face] = flat_img\n            yield textured_faces\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, return_camera=False, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <code>return_camera</code> <code>bool</code> <p>Should the camera be yielded as the second value</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    return_camera: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n        return_camera (bool, optional):\n            Should the camera be yielded as the second value\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    # Create a local mesh\n    mesh = self.get_mesh_in_cameras_coords(cameras)\n\n    if isinstance(cameras, PhotogrammetryCamera):\n        # Construct a camera set of length one\n        cameras = PhotogrammetryCameraSet([cameras])\n    elif not isinstance(cameras, PhotogrammetryCameraSet):\n        raise TypeError()\n\n    # Get the face texture from the mesh\n    # TODO consider whether the user should be able to pass a texture to this method. It could\n    # make the user's life easier but makes this method more complex\n    face_texture = self.get_texture(\n        request_vertex_texture=False, try_verts_faces_conversion=True\n    )\n    texture_dim = face_texture.shape[1]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_end = batch_start + batch_size\n        batch_cameras = cameras[batch_start:batch_end]\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            mesh=mesh,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n\n        # Iterate over the batch dimension\n        for i, pix2face in enumerate(batch_pix2face):\n            # Record the original shape of the image\n            img_shape = pix2face.shape[:2]\n            # Flatten for indexing\n            pix2face = pix2face.flatten()\n            # Compute which pixels intersected the mesh\n            mesh_pixel_inds = np.where(pix2face != -1)[0]\n            # Initialize and all-nan array\n            rendered_flattened = np.full(\n                (pix2face.shape[0], texture_dim), fill_value=np.nan\n            )\n            # Fill the values for which correspondences exist\n            rendered_flattened[mesh_pixel_inds] = face_texture[\n                pix2face[mesh_pixel_inds]\n            ]\n            # reshape to an image, where the last dimension is the texture dimension\n            rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n\n            if return_camera:\n                yield (rendered_img, batch_cameras[i])\n            else:\n                yield rendered_img\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.reproject_CRS","title":"<code>reproject_CRS(target_CRS, inplace=True)</code>","text":"<p>Convert the mesh into a new coordinate reference system. This is done by updating the location of each vertex using the mappings between the current coordinate reference system and the requested one, as implemented in pyproj.</p> <p>Note that if the CRS of the mesh is None, this operation will do nothing and the original vertex values will be returned un-transformed.</p> <p>Parameters:</p> Name Type Description Default <code>target_CRS</code> <code>CRS</code> <p>The coordinate reference system to transform the mesh to.</p> required <code>inplace</code> <code>bool</code> <p>Should the self.pyvista_mesh and self.CRS attributes be</p> <code>True</code> <p>Returns:</p> Type Description <code>(PolyData, optional)</code> <p>If <code>inplace==False</code>, a transformed pyvista mesh will be returned</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def reproject_CRS(\n    self, target_CRS: pyproj.CRS, inplace: bool = True\n) -&gt; typing.Optional[pv.PolyData]:\n    \"\"\"\n    Convert the mesh into a new coordinate reference system. This is done by updating the\n    location of each vertex using the mappings between the current coordinate reference system\n    and the requested one, as implemented in pyproj.\n\n    Note that if the CRS of the mesh is None, this operation will do nothing and the original\n    vertex values will be returned un-transformed.\n\n    Args:\n        target_CRS (pyproj.CRS): The coordinate reference system to transform the mesh to.\n        inplace (bool, optional): Should the self.pyvista_mesh and self.CRS attributes be\n        updated. Otherwise, an updated copy of the mesh is returned and the original is left\n        unchanged. Defaults to True.\n\n    Returns:\n        (pv.PolyData, optional): If `inplace==False`, a transformed pyvista mesh will be returned\n    \"\"\"\n    # Check if the mesh has a valid CRS\n    if self.CRS is None:\n        self.logger.warning(\"mesh CRS is None, reproject_CRS is doing nothing\")\n        # If not, just return the original coordinates as if they had been transformed\n        verts_in_output_CRS = np.array(self.pyvista_mesh.points)\n    else:\n        # Build a pyproj transfrormer from the current to the desired CRS\n        transformer = pyproj.Transformer.from_crs(self.CRS, target_CRS)\n\n        # Convert the mesh vertices to a numpy array\n        mesh_verts = np.array(self.pyvista_mesh.points)\n\n        # Transform the coordinates\n        verts_in_output_CRS = transformer.transform(\n            xx=mesh_verts[:, 0],\n            yy=mesh_verts[:, 1],\n            zz=mesh_verts[:, 2],\n        )\n        # Stack and transpose\n        verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n        # TODO figure out how to deal with the fact that this may no longer be a right-handed coordinate system\n        # See comment in `get_vertices_in_CRS`\n\n    if inplace:\n        # Update the CRS\n        self.CRS = target_CRS\n        # Update the mesh points\n        self.pyvista_mesh.points = pv.pyvista_ndarray(verts_in_output_CRS)\n    else:\n        # Create a copy of the mesh\n        copied_mesh = self.pyvista_mesh.copy(deep=True)\n        # Update its points\n        copied_mesh.points = pv.pyvista_ndarray(verts_in_output_CRS)\n        # Return the updated copy\n        return copied_mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_IDs_to_labels","title":"<code>save_IDs_to_labels(savepath)</code>","text":"<p>saves the contents of the IDs_to_labels to the file savepath provided</p> <p>Parameters:</p> Name Type Description Default <code>savepath</code> <code>PATH_TYPE</code> <p>path to the file where the data must be saved</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_IDs_to_labels(self, savepath: PATH_TYPE):\n    \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n    Args:\n        savepath (PATH_TYPE): path to the file where the data must be saved\n    \"\"\"\n\n    # Save the classes filename\n    ensure_containing_folder(savepath)\n    if self.is_discrete_texture():\n        self.logger.info(\"discrete texture, saving classes\")\n        self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n        try:\n            with open(savepath, \"w\") as outfile_h:\n                # Try to dump the mapping, falling back on the string encoder for types in the\n                # dict values that cannot be JSON serialized. This is most common with\n                # np.int64\n                json.dump(\n                    self.get_IDs_to_labels(),\n                    outfile_h,\n                    ensure_ascii=False,\n                    indent=4,\n                    default=str,\n                )\n        except:\n            self.logger.warn(\"Could not serialize IDs_to_labels due to JSON error\")\n    else:\n        self.logger.warn(\"non-discrete texture, not saving classes\")\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_renders","title":"<code>save_renders(camera_set, render_image_scale=1.0, output_folder=Path(VIS_FOLDER, 'renders'), make_composites=False, save_native_resolution=False, cast_to_uint8=True, save_as_npy=False, uint8_value_for_null_texture=NULL_TEXTURE_INT_VALUE, **render_kwargs)</code>","text":"<p>Render an image from the viewpoint of each specified camera and save a composite</p> <p>Parameters:</p> Name Type Description Default <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Camera set to use for rendering</p> required <code>render_image_scale</code> <code>float</code> <p>Multiplier on the real image scale to obtain size for rendering. Lower values yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.</p> <code>1.0</code> <code>render_folder</code> <code>PATH_TYPE</code> <p>Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")</p> required <code>make_composites</code> <code>bool</code> <p>Should a triple pane composite with the original image be saved rather than the raw label</p> <code>False</code> <code>cast_to_uint8</code> <code>bool</code> <p>(bool, optional): cast the float valued data to unit8 for saving efficiency. May dramatically increase efficiency due to tif compression. Saves as tif unless save_as_npy is specified as True.</p> <code>True</code> <code>save_as_npy</code> <code>bool</code> <p>Save the rendered images as numpy arrays rather than TIF images. Defaults to False.</p> <code>False</code> <code>uint8_value_for_null_texture</code> <code>uint8</code> <p>What value to assign for values that can't be represented as unsigned 8-bit data. Defaults to NULL_TEXTURE_INT_VALUE</p> <code>NULL_TEXTURE_INT_VALUE</code> <code>render_kwargs</code> <p>keyword arguments passed to the render.</p> <code>{}</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_renders(\n    self,\n    camera_set: PhotogrammetryCameraSet,\n    render_image_scale=1.0,\n    output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n    make_composites: bool = False,\n    save_native_resolution: bool = False,\n    cast_to_uint8: bool = True,\n    save_as_npy: bool = False,\n    uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n    **render_kwargs,\n):\n    \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n    Args:\n        camera_set (PhotogrammetryCameraSet):\n            Camera set to use for rendering\n        render_image_scale (float, optional):\n            Multiplier on the real image scale to obtain size for rendering. Lower values\n            yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n        render_folder (PATH_TYPE, optional):\n            Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n        make_composites (bool, optional):\n            Should a triple pane composite with the original image be saved rather than the\n            raw label\n        cast_to_uint8: (bool, optional):\n            cast the float valued data to unit8 for saving efficiency. May dramatically increase\n            efficiency due to tif compression. Saves as tif unless save_as_npy is specified as True.\n        save_as_npy (bool, optional):\n            Save the rendered images as numpy arrays rather than TIF images. Defaults to False.\n        uint8_value_for_null_texture (np.uint8, optional):\n            What value to assign for values that can't be represented as unsigned 8-bit data.\n            Defaults to NULL_TEXTURE_INT_VALUE\n        render_kwargs:\n            keyword arguments passed to the render.\n    \"\"\"\n\n    ensure_folder(output_folder)\n    self.logger.info(f\"Saving renders to {output_folder}\")\n\n    # Save the classes filename\n    self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n    # Create the generator object to render the images\n    # Since this is a generator, this will be fast\n    render_gen = self.render_flat(\n        camera_set,\n        render_img_scale=render_image_scale,\n        return_camera=True,\n        distortion_set=camera_set,\n        **render_kwargs,\n    )\n\n    # The computation only happens when items are requested from the generator\n    for rendered, camera in tqdm(\n        render_gen,\n        total=len(camera_set),\n        desc=\"Computing and saving renders\",\n    ):\n        ## All this is post-processing to visualize the rendered label.\n        # rendered could either be a one channel image of integer IDs,\n        # a one-channel image of scalars, or a three-channel image of\n        # RGB. It could also be multi-channel image corresponding to anything,\n        # but we don't expect that yet\n        if save_native_resolution and render_image_scale != 1:\n            native_size = camera.get_image_size()\n            # Upsample using nearest neighbor interpolation for discrete labels and\n            # bilinear for non-discrete\n            # TODO this will need to be fixed for multi-channel images since I don't think resize works\n            rendered = resize(\n                rendered,\n                native_size,\n                order=(0 if self.is_discrete_texture() else 1),\n            )\n\n        if cast_to_uint8:\n            # Deterimine values that cannot be represented as uint8\n            mask = np.logical_or.reduce(\n                [\n                    rendered &lt; 0,\n                    rendered &gt; 255,\n                    np.logical_not(np.isfinite(rendered)),\n                ]\n            )\n            rendered[mask] = uint8_value_for_null_texture\n            # Cast and squeeze since you can't save a one-channel image\n            rendered = np.squeeze(rendered.astype(np.uint8))\n\n        if make_composites:\n            RGB_image = camera.get_image(\n                image_scale=(1.0 if save_native_resolution else render_image_scale)\n            )\n            rendered = create_composite(\n                RGB_image=RGB_image,\n                label_image=rendered,\n                IDs_to_labels=self.get_IDs_to_labels(),\n            )\n        else:\n            # Clip channels if needed\n            if rendered.ndim == 3:\n                rendered = rendered[..., :3]\n\n        try:\n            # If the filename stored with the camera data [camera.get_image_filename]\n            # is a subpath of your camera set image folder, use the same subpath for\n            # the output data.\n            camera_filename = camera.get_image_filename().relative_to(\n                camera_set.image_folder\n            )\n        except ValueError:\n            raise ValueError(\n                \"Tried to find the relative path of the camera path\"\n                f\" ({camera.get_image_filename()}) inside of the camera set image\"\n                f\" folder ({camera_set.image_folder}), but failed. The tool being called\"\n                \" may have an 'original_image_folder' argument, which could be used to\"\n                \" delete the initial, mismatched portion of the camera path. See more here:\"\n                \" https://github.com/open-forest-observatory/automate-metashape/issues/90.\"\n            )\n        output_filename = Path(output_folder, camera_filename)\n\n        # This may create nested folders in the output dir\n        ensure_containing_folder(output_filename)\n\n        if save_as_npy is True:\n            output_filename = str(output_filename.with_suffix(\".npy\"))\n            # Save the image\n            np.save(output_filename, rendered)\n        else:\n            # Save image as TIF\n            output_filename = str(output_filename.with_suffix(\".tif\"))\n            # Remove singleton channel dimension (1, H, W) -&gt; (H, W) to save single-channel TIF\n            rendered = np.squeeze(rendered)\n            # TODO: Consider supporting TIF files with float data (like CHM renders) by adding a separate flag.\n            # Evaluate whether this offers more space savings than npy files.\n            # If cast_to_uint8 is True, rendered is already in uint8\n            if cast_to_uint8 is False:\n                # Check if max value in the rendered image is within the range of uint16\n                if np.nanmax(rendered) &lt;= np.iinfo(np.uint16).max:\n                    # Cast from float to uint16\n                    rendered = rendered.astype(np.uint16)\n                else:\n                    rendered = rendered.astype(np.uint32)\n\n            # Save the image\n            skimage.io.imsave(\n                output_filename,\n                rendered,\n                compression=\"deflate\",\n                check_contrast=False,\n            )\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.select_mesh_ROI","title":"<code>select_mesh_ROI(region_of_interest, buffer_meters=0, simplify_tol_meters=0, default_CRS=pyproj.CRS.from_epsg(4326), return_original_IDs=False)</code>","text":"<p>Get a subset of the mesh based on geospatial data</p> <p>Parameters:</p> Name Type Description Default <code>region_of_interest</code> <code>Union[GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]</code> <p>Region of interest. Can be a * dataframe, where all columns will be colapsed * A shapely polygon/multipolygon * A file that can be loaded by geopandas</p> required <code>buffer_meters</code> <code>float</code> <p>Expand the geometry by this amount of meters. Defaults to 0.</p> <code>0</code> <code>simplify_tol_meters</code> <code>float</code> <p>Simplify the geometry using this as the tolerance. Defaults to 0.</p> <code>0</code> <code>default_CRS</code> <code>CRS</code> <p>The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).</p> <code>from_epsg(4326)</code> <code>return_original_IDs</code> <code>bool</code> <p>Return the indices into the original mesh. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pyvista.PolyData: The subset of the mesh</p> <p>np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)</p> <p>np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def select_mesh_ROI(\n    self,\n    region_of_interest: typing.Union[\n        gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n    ],\n    buffer_meters: float = 0,\n    simplify_tol_meters: int = 0,\n    default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n    return_original_IDs: bool = False,\n):\n    \"\"\"Get a subset of the mesh based on geospatial data\n\n    Args:\n        region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n            Region of interest. Can be a\n            * dataframe, where all columns will be colapsed\n            * A shapely polygon/multipolygon\n            * A file that can be loaded by geopandas\n        buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n        simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n        default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n        return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n    Returns:\n        pyvista.PolyData: The subset of the mesh\n        np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n        np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n    \"\"\"\n    if region_of_interest is None:\n        return self.pyvista_mesh\n\n    # Get the ROI into a geopandas GeoDataFrame\n    self.logger.info(\"Standardizing ROI\")\n    if isinstance(region_of_interest, gpd.GeoDataFrame):\n        ROI_gpd = region_of_interest\n    elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n        ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n    else:\n        ROI_gpd = gpd.read_file(region_of_interest)\n\n    self.logger.info(\"Dissolving ROI\")\n    # Disolve to ensure there is only one row\n    ROI_gpd = ROI_gpd.dissolve()\n    self.logger.info(\"Setting CRS and buffering ROI\")\n    # Make sure we're using a projected CRS so a buffer can be applied\n    ROI_gpd = ensure_projected_CRS(ROI_gpd)\n    # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n    ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n    # Simplify the geometry to reduce the computational load\n    ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n    self.logger.info(\"Dissolving buffered ROI\")\n    # Disolve again in case\n    ROI_gpd = ROI_gpd.dissolve()\n\n    self.logger.info(\"Extracting verts for dataframe\")\n    # Get the vertices as a dataframe in the same CRS\n    verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n    self.logger.info(\"Checking intersection of verts with ROI\")\n    # Determine which vertices are within the ROI polygon\n    verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n    # Extract the IDs of the set within the polygon\n    vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n    self.logger.info(\"Extracting points from pyvista mesh\")\n    # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n    subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n    self.logger.info(\"Extraction surface from subset mesh\")\n    # Convert the unstructured grid to a PolyData (mesh) again\n    subset_mesh = subset_unstructured_grid.extract_surface()\n\n    # If we need the indices into the original mesh, return those\n    if return_original_IDs:\n        try:\n            point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n            face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n        except KeyError:\n            point_IDs = np.array([])\n            face_IDs = np.array([])\n\n        return (\n            subset_mesh,\n            point_IDs,\n            face_IDs,\n        )\n    # Else return just the mesh\n    return subset_mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.set_texture","title":"<code>set_texture(texture_array, IDs_to_labels=None, all_discrete_texture_values=None, is_vertex_texture=None, delete_existing=True, update_IDs_to_labels=True)</code>","text":"<p>Set the internal texture representation</p> <p>Parameters:</p> Name Type Description Default <code>texture_array</code> <code>ndarray</code> <p>The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.</p> required <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping from integer IDs to string names. Defaults to None.</p> <code>None</code> <code>all_discrete_texture_values</code> <code>Union[List, None]</code> <p>Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>is_vertex_texture</code> <code>Union[bool, None]</code> <p>Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>delete_existing</code> <code>bool</code> <p>Delete the existing texture when the other one (face, vertex) is set. Defaults to True.</p> <code>True</code> <code>update_IDs_to_labels</code> <code>bool</code> <p>Should IDs to labels be updated based on either the provided IDs_to_labels or the derived ones. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the size of the texture doesn't match the number of either faces or vertices</p> <code>ValueError</code> <p>If the number of faces and vertices are the same and is_vertex_texture isn't set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def set_texture(\n    self,\n    texture_array: np.ndarray,\n    IDs_to_labels: typing.Union[None, dict] = None,\n    all_discrete_texture_values: typing.Union[typing.List, None] = None,\n    is_vertex_texture: typing.Union[bool, None] = None,\n    delete_existing: bool = True,\n    update_IDs_to_labels: bool = True,\n):\n    \"\"\"Set the internal texture representation\n\n    Args:\n        texture_array (np.ndarray):\n            The array of texture values. The first dimension must be the length of faces or\n            verts. A second dimension is optional.\n        IDs_to_labels (typing.Union[None, dict], optional):\n            Mapping from integer IDs to string names. Defaults to None.\n        all_discrete_texture_values (typing.Union[typing.List, None], optional):\n            Are all the texture values known to be discrete, representing IDs. Computed from\n            the data if not set. Defaults to None.\n        is_vertex_texture (typing.Union[bool, None], optional):\n            Are the texture values supposed to correspond to the vertices. Computed from the\n            data if not set. Defaults to None.\n        delete_existing (bool, optional):\n            Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n        update_IDs_to_labels (bool, optional):\n            Should IDs to labels be updated based on either the provided IDs_to_labels or the\n            derived ones. Defaults to True.\n\n    Raises:\n        ValueError: If the size of the texture doesn't match the number of either faces or vertices\n        ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n    \"\"\"\n    # Ensure that the texture is 2D and a numpy array\n    texture_array = self.standardize_texture(texture_array)\n\n    if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n        # If it is more than one column, it's assumed to be a real-valued\n        # quantity and we try to cast it to a float\n        texture_array = texture_array.astype(float)\n        self.IDs_to_labels = None\n    else:\n        if IDs_to_labels is None:\n            texture_array, derived_IDs_to_labels = ensure_float_labels(\n                texture_array, full_array=all_discrete_texture_values\n            )\n            # If requested, record these new IDs_to_labels\n            if update_IDs_to_labels:\n                self.IDs_to_labels = derived_IDs_to_labels\n        else:\n            # Create the inverse mapping, returning nan for anything not in it\n            labels_to_IDs = defaultdict(lambda: np.nan)\n            labels_to_IDs.update({v: k for k, v in IDs_to_labels.items()})\n\n            # Ensure the mapping is 1-to-1, that there are no collisions in the mapping\n            if len(labels_to_IDs) != len(IDs_to_labels):\n                raise ValueError(\"IDs_to_labels is not a one-to-one mapping\")\n\n            # Check that the mapping only produces ints\n            if not np.all([isinstance(v, int) for v in labels_to_IDs.values()]):\n                raise ValueError(\n                    \"The labels to IDs mapping does not produce only floats\"\n                )\n\n            # Perform the mapping\n            texture_array = np.array(\n                [labels_to_IDs[l] for l in texture_array.squeeze()]\n            )\n            # Reinstate the squeezed dimension\n            texture_array = np.expand_dims(texture_array, axis=1)\n\n            # If requested, record these IDs to labels\n            if update_IDs_to_labels:\n                self.IDs_to_labels = IDs_to_labels\n\n    # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n    # TODO consider refactoring to check whether it matches the number of one of them,\n    # no matter whether is_vertex_texture is specified\n    if is_vertex_texture is None:\n        # Check that the number of matches face or verts\n        n_values = texture_array.shape[0]\n        n_faces = self.faces.shape[0]\n        n_verts = self.pyvista_mesh.points.shape[0]\n\n        if n_verts == n_faces:\n            raise ValueError(\n                \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n            )\n        elif n_values == n_verts:\n            is_vertex_texture = True\n        elif n_values == n_faces:\n            is_vertex_texture = False\n        else:\n            raise ValueError(\n                f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n            )\n\n    # Set the appropriate texture and optionally delete the other one\n    if is_vertex_texture:\n        self.vertex_texture = texture_array\n        if delete_existing:\n            self.face_texture = None\n    else:\n        self.face_texture = texture_array\n        if delete_existing:\n            self.vertex_texture = None\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.transfer_texture","title":"<code>transfer_texture(downsampled_mesh)</code>","text":"<p>Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches</p> <p>Parameters:</p> Name Type Description Default <code>downsampled_mesh</code> <code>PolyData</code> <p>The downsampled version of the original mesh</p> required <p>Returns:</p> Type Description <p>pv.PolyData: The downsampled mesh with the transferred textures</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def transfer_texture(self, downsampled_mesh):\n    \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n    Args:\n        downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n    Returns:\n        pv.PolyData: The downsampled mesh with the transferred textures\n    \"\"\"\n    # Only transfer textures if there are point based scalars in the original mesh\n    if self.pyvista_mesh.point_data:\n        # Store original mesh points in KDTree for nearest neighbor search\n        kdtree = KDTree(self.pyvista_mesh.points)\n\n        # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n        _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n        # Iterate over all the point based scalars\n        for scalar_name in self.pyvista_mesh.point_data.keys():\n            # Retrieve scalar data of appropriate index using the nearest neighbor indices\n            transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                nearest_neighbor_indices\n            ]\n            # Set the corresponding scalar data in the downsampled mesh\n            downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n        # Set active mesh of downsampled mesh\n        if self.pyvista_mesh.active_scalars_name:\n            downsampled_mesh.active_scalars_name = (\n                self.pyvista_mesh.active_scalars_name\n            )\n    else:\n        self.logger.warning(\n            \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n        )\n    return downsampled_mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.vis","title":"<code>vis(plotter=None, interactive=True, camera_set=None, screenshot_filename=None, vis_scalars=None, mesh_kwargs=None, interactive_jupyter=False, plotter_kwargs={}, enable_ssao=True, force_xvfb=False, frustum_scale=2, IDs_to_labels=None)</code>","text":"<p>Show the mesh and cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to use, else one will be created</p> <code>None</code> <code>off_screen</code> <code>bool</code> <p>Show offscreen</p> required <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Cameras to visualize. Defaults to None.</p> <code>None</code> <code>screenshot_filename</code> <code>PATH_TYPE</code> <p>Filepath to save to, will show interactively if None. Defaults to None.</p> <code>None</code> <code>vis_scalars</code> <code>(None, ndarray)</code> <p>Scalars to show</p> <code>None</code> <code>mesh_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the mesh</p> <code>None</code> <code>interactive_jupyter</code> <code>bool</code> <p>Should jupyter windows be interactive. This doesn't always work, especially on VSCode.</p> <code>False</code> <code>plotter_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the plotter</p> <code>{}</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. Defaults to None.</p> <code>2</code> <code>IDs_to_labels</code> <code>[None, dict]</code> <p>Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh IDs_to_labels if unset.</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    interactive: bool = True,\n    camera_set: PhotogrammetryCameraSet = None,\n    screenshot_filename: PATH_TYPE = None,\n    vis_scalars: typing.Union[None, np.ndarray] = None,\n    mesh_kwargs: typing.Dict = None,\n    interactive_jupyter: bool = False,\n    plotter_kwargs: typing.Dict = {},\n    enable_ssao: bool = True,\n    force_xvfb: bool = False,\n    frustum_scale: float = 2,\n    IDs_to_labels: typing.Union[None, dict] = None,\n):\n    \"\"\"Show the mesh and cameras\n\n    Args:\n        plotter (pyvista.Plotter, optional):\n            Plotter to use, else one will be created\n        off_screen (bool, optional):\n            Show offscreen\n        camera_set (PhotogrammetryCameraSet, optional):\n            Cameras to visualize. Defaults to None.\n        screenshot_filename (PATH_TYPE, optional):\n            Filepath to save to, will show interactively if None. Defaults to None.\n        vis_scalars (None, np.ndarray):\n            Scalars to show\n        mesh_kwargs:\n            dict of keyword arguments for the mesh\n        interactive_jupyter (bool):\n            Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n        plotter_kwargs:\n            dict of keyword arguments for the plotter\n        frustum_scale (float, optional):\n            Size of cameras in world units. Defaults to None.\n        IDs_to_labels ([None, dict], optional):\n            Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n            IDs_to_labels if unset.\n    \"\"\"\n    # TODO conside reprojecting to ensure axes are both meters-based\n    off_screen = (not interactive) or (screenshot_filename is not None)\n\n    # If the IDs to labels is not set, use the default ones for this mesh\n    if IDs_to_labels is None:\n        IDs_to_labels = self.get_IDs_to_labels()\n\n    # Set the mesh kwargs if not set\n    if mesh_kwargs is None:\n        # This needs to be a dict, even if it's empty\n        mesh_kwargs = {}\n\n        # If there are discrete labels, set the colormap and limits inteligently\n        if IDs_to_labels is not None:\n            # Compute the largest ID\n            max_ID = max(IDs_to_labels.keys())\n            if max_ID &lt; 20:\n                colors = [\n                    matplotlib.colors.to_hex(c)\n                    for c in plt.get_cmap(\n                        (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                    ).colors\n                ]\n                mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n    # Create the plotter if it's None\n    plotter = create_pv_plotter(\n        off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n    )\n\n    # If the vis scalars are None, use the saved texture\n    if vis_scalars is None:\n        vis_scalars = self.get_texture(\n            # Request vertex texture if both are available\n            request_vertex_texture=(\n                True\n                if (\n                    self.vertex_texture is not None\n                    and self.face_texture is not None\n                )\n                else None\n            )\n        )\n\n    is_rgb = (\n        self.pyvista_mesh.active_scalars_name == \"RGB\"\n        if vis_scalars is None\n        else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n    )\n\n    # Data in the range [0, 255] must be uint8 type\n    if is_rgb and np.nanmax(vis_scalars) &gt; 1.0:\n        vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n    scalar_bar_args = {\"vertical\": True}\n    if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n        mesh_kwargs[\"annotations\"] = IDs_to_labels\n        scalar_bar_args[\"n_labels\"] = 0\n\n    vis_mesh = self.reproject_CRS(EARTH_CENTERED_EARTH_FIXED_CRS, inplace=False)\n\n    # If camera set is provided, transform the mesh into those coordinates\n    if camera_set is not None:\n        # Compute the transform mapping from the earth centered, earth fixed coordinate frame\n        # (EPSG:4978) to the coordinate of the camera\n        epsg_4978_to_camera = np.linalg.inv(\n            camera_set.get_local_to_epsg_4978_transform()\n        )\n        # Apply the 4x4 transform using the pyvista transform method to get the mesh into the\n        # same coordinate frame as the cameras.\n        vis_mesh.transform(epsg_4978_to_camera, inplace=True)\n\n    # Add the mesh\n    plotter.add_mesh(\n        vis_mesh,\n        scalars=vis_scalars,\n        rgb=is_rgb,\n        scalar_bar_args=scalar_bar_args,\n        **mesh_kwargs,\n    )\n    # If the camera set is provided, show this too\n    if camera_set is not None:\n        # Adjust the frustum scale if the mesh came from metashape\n        # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n        if (\n            camera_set.get_local_to_epsg_4978_transform() is not None\n            and frustum_scale is not None\n        ):\n            transform_determinant = np.linalg.det(\n                camera_set.get_local_to_epsg_4978_transform()[:3, :3]\n            )\n            scale_factor = np.cbrt(transform_determinant)\n            frustum_scale = frustum_scale / scale_factor\n        camera_set.vis(\n            plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n        )\n\n    # Enable screen space shading\n    if enable_ssao:\n        plotter.enable_ssao()\n\n    # Create parent folder if none exists\n    if screenshot_filename is not None:\n        ensure_containing_folder(screenshot_filename)\n\n    if \"jupyter_backend\" not in plotter_kwargs:\n        if interactive_jupyter:\n            plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n        else:\n            plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n    if \"title\" not in plotter_kwargs:\n        plotter_kwargs[\"title\"] = \"Geograypher mesh viewer\"\n\n    # Show\n    return plotter.show(\n        screenshot=screenshot_filename,\n        **plotter_kwargs,\n    )\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/","title":"Derived Segmentors","text":""},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.BrightnessSegmentor","title":"<code>BrightnessSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class BrightnessSegmentor(Segmentor):\n    def __init__(self, brightness_threshold: float = np.sqrt(0.75)):\n        self.brightness_threshold = brightness_threshold\n        self.num_classes = 2\n\n    def segment_image(self, image: np.ndarray, **kwargs):\n        image_brightness = np.linalg.norm(image, axis=-1)\n        thresholded_image = image_brightness &gt; self.brightness_threshold\n        class_index_image = thresholded_image.astype(np.uint8)\n        one_hot_image = self.inds_to_one_hot(class_index_image)\n        return one_hot_image\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.LookUpSegmentor","title":"<code>LookUpSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class LookUpSegmentor(Segmentor):\n    def __init__(self, base_folder, lookup_folder, num_classes=10):\n        self.base_folder = Path(base_folder)\n        self.lookup_folder = lookup_folder\n        self.num_classes = num_classes\n\n    def segment_image(self, image: np.ndarray, filename: PATH_TYPE, image_scale: float):\n        relative_path = Path(filename).relative_to(self.base_folder)\n        lookup_path = Path(self.lookup_folder, relative_path)\n        lookup_path = lookup_path.with_suffix(\".png\")\n\n        image = imread(lookup_path)\n        if image_scale != 1:\n            image = resize(\n                image,\n                (int(image.shape[0] * image_scale), int(image.shape[1] * image_scale)),\n                order=0,  # Nearest neighbor interpolation\n            )\n        one_hot_image = self.inds_to_one_hot(image, num_classes=self.num_classes)\n        return one_hot_image\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor","title":"<code>TabularRectangleSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class TabularRectangleSegmentor(Segmentor):\n    def __init__(\n        self,\n        detection_file_or_folder: PATH_TYPE,\n        image_shape: tuple,\n        label_key: str = \"instance_ID\",\n        image_path_key: str = \"image_path\",\n        imin_key: str = \"ymin\",\n        imax_key: str = \"ymax\",\n        jmin_key: str = \"xmin\",\n        jmax_key: str = \"xmax\",\n        detection_file_extension: str = \"csv\",\n        strip_image_extension: bool = False,\n        use_absolute_filepaths: bool = False,\n        split_bbox: bool = True,\n        image_folder: typing.Union[PATH_TYPE, None] = None,\n    ):\n        \"\"\"Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.\n\n        Args:\n            detection_file_or_folder (PATH_TYPE):\n                Path to the CSV file with detections or a folder thereof\n            image_shape (tuple):\n                The (height, width) shape of the image in pixels.\n            label_key (str, optional):\n                The column that corresponds to the class. Defaults to \"label\".\n            image_path_key (str, optional):\n                The column that has the image filename. Defaults to \"image_path\".\n            imin_key (str, optional):\n                Column of the minimum i dimension. Defaults to \"ymin\".\n            imax_key (str, optional):\n                Column of the max i dimension. Defaults to \"ymax\".\n            jmin_key (str, optional):\n                Column of the min j dimension. Defaults to \"xmin\".\n            jmax_key (str, optional):\n                Column of the max j dimension. Defaults to \"xmax\".\n            detection_file_extension (str, optional):\n                File extension of the detection files. Defaults to \"csv\".\n            strip_image_extension (bool, optional):\n                Remove the extension from the image filenames. Defaults to True.\n            use_absolute_filepaths (bool, optional):\n                Add the absolute path from the image folder to the filenames. Defaults to False.\n            split_bbox (bool, optional):\n                Split the bounding box from one column rather than having seperate columns for imin,\n                imax, jmin, jmax. Defaults to True.\n            image_folder (PATH_TYPE, optional): Path to the image folder. Defaults to None.\n        \"\"\"\n        self.image_shape = image_shape\n\n        self.label_key = label_key\n        self.image_path_key = image_path_key\n        self.imin_key = imin_key\n        self.imax_key = imax_key\n        self.jmin_key = jmin_key\n        self.jmax_key = jmax_key\n        self.split_bbox = split_bbox\n\n        # Load the detections\n        self.labels_df = self.load_detection_files(\n            detection_file_or_folder=detection_file_or_folder,\n            detection_file_extension=detection_file_extension,\n            image_folder=image_folder,\n            use_absolute_filepaths=use_absolute_filepaths,\n            strip_image_extension=strip_image_extension,\n            image_path_key=image_path_key,\n        )\n\n        # Group the predictions\n        self.grouped_labels_df = self.labels_df.groupby(by=self.image_path_key)\n\n        # List the images\n        self.image_names = list(self.grouped_labels_df.groups.keys())\n        # Record the class names and number of classes\n        self.class_names = np.unique(self.labels_df[self.label_key]).tolist()\n        self.num_classes = len(self.class_names)\n\n    def load_detection_files(\n        self,\n        detection_file_or_folder: PATH_TYPE,\n        detection_file_extension: str,\n        image_folder: PATH_TYPE,\n        use_absolute_filepaths: bool,\n        strip_image_extension: bool,\n        image_path_key: str,\n    ):\n        # Determine whether the input is a file or folder\n        if Path(detection_file_or_folder).is_file():\n            # If it's a file, make a one-length list\n            files = [detection_file_or_folder]\n        else:\n            # List all the files in the folder with the requested extesion\n            files = sorted(\n                Path(detection_file_or_folder).glob(\"*\" + detection_file_extension)\n            )\n\n        # Read the individual files\n        dfs = [pd.read_csv(f) for f in files]\n\n        # Concatenate the dataframes into one\n        labels_df = pd.concat(dfs, ignore_index=True)\n\n        # Add an sequential instance ID column if not present\n        if \"instance_ID\" not in labels_df.columns:\n            labels_df[\"instance_ID\"] = labels_df.index\n\n        # Prepend the image folder to the image filenames if requested to make an absolute filepath\n        if image_folder is not None and use_absolute_filepaths:\n            absolute_filepaths = [\n                str(Path(image_folder, img_path))\n                for img_path in labels_df[image_path_key].tolist()\n            ]\n            labels_df[image_path_key] = absolute_filepaths\n\n        # Strip the extension from the image filenames if requested\n        if strip_image_extension:\n            image_path_without_ext = [\n                str(Path(img_path).with_suffix(\"\"))\n                for img_path in labels_df[image_path_key].tolist()\n            ]\n            labels_df[image_path_key] = image_path_without_ext\n\n        return labels_df\n\n    def get_all_detections(self) -&gt; pd.DataFrame:\n        \"\"\"Return the aggregated detections dataframe\"\"\"\n        return self.labels_df\n\n    def save_detection_data(self, output_csv_file: PATH_TYPE):\n        \"\"\"Save the aggregated detections to a file\n\n        Args:\n            output_csv_file (PATH_TYPE):\n                A path to a CSV file to save the detections to. The containing folder will be\n                created if needed.\n        \"\"\"\n        ensure_containing_folder(output_csv_file)\n        self.labels_df.to_csv(output_csv_file)\n\n    def get_corners(self, data, as_int=True):\n        if self.split_bbox:\n            # TODO split row\n            bbox = data[\"bbox\"]\n            bbox = bbox[1:-1]\n            splits = bbox.split(\", \")\n            jmin, imin, width, height = [float(s) for s in splits]\n\n            imax = imin + height\n            jmax = jmin + width\n\n            imin = imin\n            jmin = jmin\n        else:\n            imin = data[self.imin_key]\n            imax = data[self.imax_key]\n            jmin = data[self.jmin_key]\n            jmax = data[self.jmax_key]\n\n        corners = imin, jmin, imax, jmax\n        if as_int:\n            corners = list(map(int, corners))\n\n        return corners\n\n    def segment_image(self, image, filename, image_scale, vis=False):\n        output_shape = self.image_shape\n        label_image = np.full(output_shape, fill_value=np.nan, dtype=float)\n\n        name = filename.name\n        if name in self.image_names:\n            df = self.grouped_labels_df.get_group(name)\n        # Return an all-zero segmentation image\n        else:\n            return label_image\n\n        for _, row in df.iterrows():\n            label = row[self.label_key]\n            label_ind = self.class_names.index(label)\n\n            imin, jmin, imax, jmax = self.get_corners(\n                row,\n            )\n\n            label_image[imin:imax, jmin:jmax] = label_ind\n\n        if vis:\n            plt.imshow(label_image, vmin=0, vmax=10, cmap=\"tab10\")\n            plt.show()\n\n        if image_scale != 1.0:\n            output_size = (int(image_scale * x) for x in label_image.shape[:2])\n            label_image = resize(label_image, output_size, order=0)\n\n        return label_image\n\n    def get_detection_centers(self, filename):\n        \"\"\"_summary_\n\n        Args:\n            filename (_type_): _description_\n\n        Returns:\n            _type_: (n,2) array for (i,j) centers for each detection\n        \"\"\"\n        if filename not in self.image_names:\n            # Empty array of detection centers\n            return np.zeros((0, 2))\n\n        # Extract the corresponding dataframe\n        df = self.grouped_labels_df.get_group(filename)\n\n        all_corners = []\n        for _, row in df.iterrows():\n            corners = self.get_corners(row, as_int=False)\n            all_corners.append(corners)\n\n        all_corners = zip(*all_corners)\n        all_corners = [np.array(x) for x in all_corners]\n\n        imin, jmin, imax, jmax = all_corners\n\n        # Average the left-right, top-bottom pairs\n        centers = np.vstack([(imin + imax) / 2, (jmin + jmax) / 2]).T\n        return centers\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.__init__","title":"<code>__init__(detection_file_or_folder, image_shape, label_key='instance_ID', image_path_key='image_path', imin_key='ymin', imax_key='ymax', jmin_key='xmin', jmax_key='xmax', detection_file_extension='csv', strip_image_extension=False, use_absolute_filepaths=False, split_bbox=True, image_folder=None)</code>","text":"<p>Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.</p> <p>Parameters:</p> Name Type Description Default <code>detection_file_or_folder</code> <code>PATH_TYPE</code> <p>Path to the CSV file with detections or a folder thereof</p> required <code>image_shape</code> <code>tuple</code> <p>The (height, width) shape of the image in pixels.</p> required <code>label_key</code> <code>str</code> <p>The column that corresponds to the class. Defaults to \"label\".</p> <code>'instance_ID'</code> <code>image_path_key</code> <code>str</code> <p>The column that has the image filename. Defaults to \"image_path\".</p> <code>'image_path'</code> <code>imin_key</code> <code>str</code> <p>Column of the minimum i dimension. Defaults to \"ymin\".</p> <code>'ymin'</code> <code>imax_key</code> <code>str</code> <p>Column of the max i dimension. Defaults to \"ymax\".</p> <code>'ymax'</code> <code>jmin_key</code> <code>str</code> <p>Column of the min j dimension. Defaults to \"xmin\".</p> <code>'xmin'</code> <code>jmax_key</code> <code>str</code> <p>Column of the max j dimension. Defaults to \"xmax\".</p> <code>'xmax'</code> <code>detection_file_extension</code> <code>str</code> <p>File extension of the detection files. Defaults to \"csv\".</p> <code>'csv'</code> <code>strip_image_extension</code> <code>bool</code> <p>Remove the extension from the image filenames. Defaults to True.</p> <code>False</code> <code>use_absolute_filepaths</code> <code>bool</code> <p>Add the absolute path from the image folder to the filenames. Defaults to False.</p> <code>False</code> <code>split_bbox</code> <code>bool</code> <p>Split the bounding box from one column rather than having seperate columns for imin, imax, jmin, jmax. Defaults to True.</p> <code>True</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the image folder. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def __init__(\n    self,\n    detection_file_or_folder: PATH_TYPE,\n    image_shape: tuple,\n    label_key: str = \"instance_ID\",\n    image_path_key: str = \"image_path\",\n    imin_key: str = \"ymin\",\n    imax_key: str = \"ymax\",\n    jmin_key: str = \"xmin\",\n    jmax_key: str = \"xmax\",\n    detection_file_extension: str = \"csv\",\n    strip_image_extension: bool = False,\n    use_absolute_filepaths: bool = False,\n    split_bbox: bool = True,\n    image_folder: typing.Union[PATH_TYPE, None] = None,\n):\n    \"\"\"Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.\n\n    Args:\n        detection_file_or_folder (PATH_TYPE):\n            Path to the CSV file with detections or a folder thereof\n        image_shape (tuple):\n            The (height, width) shape of the image in pixels.\n        label_key (str, optional):\n            The column that corresponds to the class. Defaults to \"label\".\n        image_path_key (str, optional):\n            The column that has the image filename. Defaults to \"image_path\".\n        imin_key (str, optional):\n            Column of the minimum i dimension. Defaults to \"ymin\".\n        imax_key (str, optional):\n            Column of the max i dimension. Defaults to \"ymax\".\n        jmin_key (str, optional):\n            Column of the min j dimension. Defaults to \"xmin\".\n        jmax_key (str, optional):\n            Column of the max j dimension. Defaults to \"xmax\".\n        detection_file_extension (str, optional):\n            File extension of the detection files. Defaults to \"csv\".\n        strip_image_extension (bool, optional):\n            Remove the extension from the image filenames. Defaults to True.\n        use_absolute_filepaths (bool, optional):\n            Add the absolute path from the image folder to the filenames. Defaults to False.\n        split_bbox (bool, optional):\n            Split the bounding box from one column rather than having seperate columns for imin,\n            imax, jmin, jmax. Defaults to True.\n        image_folder (PATH_TYPE, optional): Path to the image folder. Defaults to None.\n    \"\"\"\n    self.image_shape = image_shape\n\n    self.label_key = label_key\n    self.image_path_key = image_path_key\n    self.imin_key = imin_key\n    self.imax_key = imax_key\n    self.jmin_key = jmin_key\n    self.jmax_key = jmax_key\n    self.split_bbox = split_bbox\n\n    # Load the detections\n    self.labels_df = self.load_detection_files(\n        detection_file_or_folder=detection_file_or_folder,\n        detection_file_extension=detection_file_extension,\n        image_folder=image_folder,\n        use_absolute_filepaths=use_absolute_filepaths,\n        strip_image_extension=strip_image_extension,\n        image_path_key=image_path_key,\n    )\n\n    # Group the predictions\n    self.grouped_labels_df = self.labels_df.groupby(by=self.image_path_key)\n\n    # List the images\n    self.image_names = list(self.grouped_labels_df.groups.keys())\n    # Record the class names and number of classes\n    self.class_names = np.unique(self.labels_df[self.label_key]).tolist()\n    self.num_classes = len(self.class_names)\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.get_all_detections","title":"<code>get_all_detections()</code>","text":"<p>Return the aggregated detections dataframe</p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def get_all_detections(self) -&gt; pd.DataFrame:\n    \"\"\"Return the aggregated detections dataframe\"\"\"\n    return self.labels_df\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.get_detection_centers","title":"<code>get_detection_centers(filename)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>(n,2) array for (i,j) centers for each detection</p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def get_detection_centers(self, filename):\n    \"\"\"_summary_\n\n    Args:\n        filename (_type_): _description_\n\n    Returns:\n        _type_: (n,2) array for (i,j) centers for each detection\n    \"\"\"\n    if filename not in self.image_names:\n        # Empty array of detection centers\n        return np.zeros((0, 2))\n\n    # Extract the corresponding dataframe\n    df = self.grouped_labels_df.get_group(filename)\n\n    all_corners = []\n    for _, row in df.iterrows():\n        corners = self.get_corners(row, as_int=False)\n        all_corners.append(corners)\n\n    all_corners = zip(*all_corners)\n    all_corners = [np.array(x) for x in all_corners]\n\n    imin, jmin, imax, jmax = all_corners\n\n    # Average the left-right, top-bottom pairs\n    centers = np.vstack([(imin + imax) / 2, (jmin + jmax) / 2]).T\n    return centers\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.save_detection_data","title":"<code>save_detection_data(output_csv_file)</code>","text":"<p>Save the aggregated detections to a file</p> <p>Parameters:</p> Name Type Description Default <code>output_csv_file</code> <code>PATH_TYPE</code> <p>A path to a CSV file to save the detections to. The containing folder will be created if needed.</p> required Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def save_detection_data(self, output_csv_file: PATH_TYPE):\n    \"\"\"Save the aggregated detections to a file\n\n    Args:\n        output_csv_file (PATH_TYPE):\n            A path to a CSV file to save the detections to. The containing folder will be\n            created if needed.\n    \"\"\"\n    ensure_containing_folder(output_csv_file)\n    self.labels_df.to_csv(output_csv_file)\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/","title":"Ortho segmentor","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor","title":"<code>ortho_segmentor</code>","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.assemble_tiled_predictions","title":"<code>assemble_tiled_predictions(raster_file, pred_folder, class_savefile, num_classes, counts_savefile=None, downweight_edge_frac=0.25, nodataval=NULL_TEXTURE_INT_VALUE, count_dtype=np.uint8, max_overlapping_tiles=4)</code>","text":"<p>Take tiled predictions on disk and aggregate them into a raster</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>Path to the raster file used to generate chips. This is required only to understand the geospatial reference.</p> required <code>pred_folder</code> <code>PATH_TYPE</code> <p>A folder where every file is a prediction for a tile. The filename must encode the bounds of the windowed crop.</p> required <code>class_savefile</code> <code>PATH_TYPE</code> <p>Where to save the merged raster.</p> required <code>counts_savefile</code> <code>Union[PATH_TYPE, NoneType]</code> <p>Where to save the counts for the merged predictions raster. A tempfile will be created and then deleted if not specified. Defaults to None.</p> <code>None</code> <code>downweight_edge_frac</code> <code>float</code> <p>Downweight this fraction of predictions at the edge of each tile using a linear ramp. Defaults to 0.25.</p> <code>0.25</code> <code>nodataval</code> <code>Union[int, None]</code> <p>(typing.Union[int, None]): Value for unassigned pixels. If None, will be set to num_classes, the first unused class. Defaults to None.</p> <code>NULL_TEXTURE_INT_VALUE</code> <code>count_dtype</code> <code>type</code> <p>What type to use for aggregation. Float uses more space but is more accurate. Defaults to np.uint8</p> <code>uint8</code> <code>max_overlapping_tiles</code> <code>int</code> <p>The max number of prediction tiles that may overlap at a given point. This is used to upper bound the valud in the count matrix, because we use scaled np.uint8 values rather than floats for efficiency. Setting a lower value enables slightly more accuracy in the aggregation process, but too low can lead to overflow. Defaults to 4</p> <code>4</code> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def assemble_tiled_predictions(\n    raster_file: PATH_TYPE,\n    pred_folder: PATH_TYPE,\n    class_savefile: PATH_TYPE,\n    num_classes: int,\n    counts_savefile: Union[PATH_TYPE, None] = None,\n    downweight_edge_frac: float = 0.25,\n    nodataval: Union[int, None] = NULL_TEXTURE_INT_VALUE,\n    count_dtype: type = np.uint8,\n    max_overlapping_tiles: int = 4,\n):\n    \"\"\"Take tiled predictions on disk and aggregate them into a raster\n\n    Args:\n        raster_file (PATH_TYPE):\n            Path to the raster file used to generate chips. This is required only to understand the\n            geospatial reference.\n        pred_folder (PATH_TYPE):\n            A folder where every file is a prediction for a tile. The filename must encode the\n            bounds of the windowed crop.\n        class_savefile (PATH_TYPE):\n            Where to save the merged raster.\n        counts_savefile (typing.Union[PATH_TYPE, NoneType], optional):\n            Where to save the counts for the merged predictions raster.\n            A tempfile will be created and then deleted if not specified. Defaults to None.\n        downweight_edge_frac (float, optional):\n            Downweight this fraction of predictions at the edge of each tile using a linear ramp. Defaults to 0.25.\n        nodataval: (typing.Union[int, None]):\n            Value for unassigned pixels. If None, will be set to num_classes, the first unused class. Defaults to None.\n        count_dtype (type, optional):\n            What type to use for aggregation. Float uses more space but is more accurate. Defaults to np.uint8\n        max_overlapping_tiles (int):\n            The max number of prediction tiles that may overlap at a given point. This is used to upper bound the valud in the count matrix,\n            because we use scaled np.uint8 values rather than floats for efficiency. Setting a lower value enables slightly more accuracy in the\n            aggregation process, but too low can lead to overflow. Defaults to 4\n    \"\"\"\n    # Find the filenames of tiled predictions\n    pred_files = [f for f in pred_folder.glob(\"*\") if f.is_file()]\n\n    # Set nodataval to the first unused class ID\n    if nodataval is None:\n        nodataval = num_classes\n\n    # If the user didn't specify where to write the counts, create a tempfile that will be deleted\n    if counts_savefile is None:\n        # Create the containing folder if required\n        ensure_containing_folder(class_savefile)\n        counts_savefile_manager = tempfile.NamedTemporaryFile(\n            mode=\"w+\", suffix=\".tif\", dir=class_savefile.parent\n        )\n        counts_savefile = counts_savefile_manager.name\n\n    # Parse the filenames to get the windows\n    # TODO consider using the extent to only write a file for the minimum encolsing rectangle\n    windows, extent = parse_windows_from_files(pred_files, return_in_extent_coords=True)\n\n    # Aggregate predictions\n    with rio.open(raster_file) as src:\n        # Create file to store counts that is the same as the input raster except it has num_classes number of bands\n        # TODO make this only the size of the extent computed by parse_windows_from_files\n        extent_transform = src.window_transform(extent)\n\n        with rio.open(\n            counts_savefile,\n            \"w+\",\n            driver=\"GTiff\",\n            height=extent.height,\n            width=extent.width,\n            count=num_classes,\n            dtype=count_dtype,\n            crs=src.crs,\n            transform=extent_transform,\n        ) as dst:\n            # Create\n            pred_weighting_dict = {}\n            for pred_file, window in tqdm(\n                zip(pred_files, windows),\n                desc=\"Aggregating raster predictions\",\n                total=len(pred_files),\n            ):\n                # Read the prediction from disk\n                pred = read_image_or_numpy(pred_file)\n\n                if pred.shape != (window.height, window.width):\n                    raise ValueError(\"Size of pred does not match window\")\n\n                # We want to downweight portions at the edge so we create a ramped weighting mask\n                # but we don't want to duplicate this computation because it's the same for each same sized chip\n                if pred.shape not in pred_weighting_dict:\n                    # We want to keep this as a uint8\n                    pred_weighting = create_ramped_weighting(\n                        pred.shape, downweight_edge_frac\n                    )\n\n                    # Allow us to get as much granularity as possible given the datatype\n                    if count_dtype is not float:\n                        pred_weighting = pred_weighting * (\n                            np.iinfo(count_dtype).max / max_overlapping_tiles\n                        )\n                    # Convert weighting to desired type\n                    pred_weighting_dict[pred.shape] = pred_weighting.astype(count_dtype)\n\n                # Get weighting\n                pred_weighting = pred_weighting_dict[pred.shape]\n\n                # Update each band in the counts file within the window\n                for i in range(num_classes):\n                    # Bands in rasterio are 1-indexed\n                    band_ind = i + 1\n                    class_i_window_counts = dst.read(band_ind, window=window)\n                    class_i_preds = pred == i\n                    # If nothing matches this class, don't waste computation\n                    if not np.any(class_i_preds):\n                        continue\n                    # Weight the predictions to downweight the ones at the edge\n                    weighted_preds = (class_i_preds * pred_weighting).astype(\n                        count_dtype\n                    )\n                    # Add the new predictions to the previous counts\n                    class_i_window_counts += weighted_preds\n                    # Write out the updated results for this window\n                    dst.write(class_i_window_counts, band_ind, window=window)\n\n    ## Convert counts file to max-class file\n\n    with rio.open(counts_savefile, \"r\") as src:\n        # Create a one-band file to store the index of the most predicted class\n        with rio.open(\n            class_savefile,\n            \"w\",\n            driver=\"GTiff\",\n            height=src.shape[0],\n            width=src.shape[1],\n            count=1,\n            dtype=np.uint8,\n            crs=src.crs,\n            transform=src.transform,\n            nodata=nodataval,\n        ) as dst:\n            # Iterate over the blocks corresponding to the tiff driver in the dataset\n            # to compute the max class and write it out\n            for _, window in tqdm(\n                list(src.block_windows()), desc=\"Writing out max class\"\n            ):\n                # Read in the counts\n                counts_array = src.read(window=window)\n                # Compute which pixels have no recorded predictions and mask them out\n                nodata_mask = np.sum(counts_array, axis=0) == 0\n\n                # If it's all nodata, don't write it out\n                # TODO make sure this works as expected\n                if np.all(nodata_mask):\n                    continue\n\n                # Compute which class had the highest counts\n                max_class = np.argmax(counts_array, axis=0)\n                max_class[nodata_mask] = nodataval\n                # TODO, it would be good to check if it's all nodata and skip the write because that's unneeded\n                dst.write(max_class, 1, window=window)\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.parse_windows_from_files","title":"<code>parse_windows_from_files(files, sep=':', return_in_extent_coords=True)</code>","text":"<p>Return the boxes and extent from a list of filenames</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[Path]</code> <p>List of filenames</p> required <code>sep</code> <code>str</code> <p>Seperator between elements</p> <code>':'</code> <code>return_in_extent_coords</code> <code>bool</code> <p>Return in the coordinate frame of the extent</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[list[Window], Window]</code> <p>tuple[list[Window], Window]: List of windows for each file and extent</p> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def parse_windows_from_files(\n    files: list[Path], sep: str = \":\", return_in_extent_coords: bool = True\n) -&gt; tuple[list[Window], Window]:\n    \"\"\"Return the boxes and extent from a list of filenames\n\n    Args:\n        files (list[Path]): List of filenames\n        sep (str): Seperator between elements\n        return_in_extent_coords (bool): Return in the coordinate frame of the extent\n\n    Returns:\n        tuple[list[Window], Window]: List of windows for each file and extent\n    \"\"\"\n    # Split the coords out, currently ignorign the filename as the first element\n    coords = [file.stem.split(sep)[1:] for file in files]\n\n    # Compute the extents as the min/max of the boxes\n    coords_array = np.array(coords).astype(int)\n\n    xmin = np.min(coords_array[:, 0])\n    ymin = np.min(coords_array[:, 1])\n    xmax = np.max(coords_array[:, 2] + coords_array[:, 0])\n    ymax = np.max(coords_array[:, 3] + coords_array[:, 1])\n    extent = Window(row_off=ymin, col_off=xmin, width=xmax - xmin, height=ymax - ymin)\n\n    if return_in_extent_coords:\n        # Subtract out x and y min so it's w.r.t. the extent coordinates\n        coords_array[:, 0] = coords_array[:, 0] - xmin\n        coords_array[:, 1] = coords_array[:, 1] - ymin\n\n    # Create windows from coords\n    windows = [\n        Window(\n            col_off=coord[0],\n            row_off=coord[1],\n            width=coord[2],\n            height=coord[3],\n        )\n        for coord in coords_array.astype(int)\n    ]\n\n    return windows, extent\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.write_chips","title":"<code>write_chips(raster_file, output_folder, chip_size, chip_stride, label_vector_file=None, label_column=None, label_remap=None, write_empty_tiles=False, drop_transparency=True, remove_old=True, output_suffix='.JPG', ROI_file=None, background_ind=NULL_TEXTURE_INT_VALUE)</code>","text":"<p>Take raster data and tile it for machine learning training or inference</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>Path to the raster file to tile.</p> required <code>output_folder</code> <code>PATH_TYPE</code> <p>Where to write the tiled outputs.</p> required <code>chip_size</code> <code>int</code> <p>Size of the square chip in pixels.</p> required <code>chip_stride</code> <code>int</code> <p>The stride in pixels between sliding window tiles.</p> required <code>label_vector_file</code> <code>Optional[PATH_TYPE]</code> <p>A path to a vector geofile for the same region as the raster file. If provided, a parellel folder structure will be written to the chipped images that contains the corresponding rasterized data from the vector file. This is primarily useful for generating training data for ML. Defaults to None.</p> <code>None</code> <code>label_column</code> <code>Optional[str]</code> <p>Which column to use within the provided file. If not provided, the index will be used. Defaults to None.</p> <code>None</code> <code>label_remap</code> <code>Optional[dict]</code> <p>A dictionary mapping from the values in the <code>label_column</code> to integers that will be used for rasterization. Defaults to None.</p> <code>None</code> <code>write_empty_tiles</code> <code>bool</code> <p>Should tiles with no vector data be written. Defaults to False.</p> <code>False</code> <code>drop_transparency</code> <code>bool</code> <p>Should the forth channel be dropped if present. Defaults to True.</p> <code>True</code> <code>remove_old</code> <code>bool</code> <p>Remove <code>output_folder</code> if present. Defaults to True.</p> <code>True</code> <code>output_suffix</code> <code>str</code> <p>Suffix for written imagery files. Defaults to \".JPG\".</p> <code>'.JPG'</code> <code>ROI_file</code> <code>Optional[PATH_TYPE]</code> <p>Path to a geospatial region of interest to restrict tile generation to. Defaults to None.</p> <code>None</code> <code>background_ind</code> <code>int</code> <p>If labels are written, any un-labeled region will have this value. Defaults to <code>NULL_TEXTURE_INT_VALUE</code>.</p> <code>NULL_TEXTURE_INT_VALUE</code> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def write_chips(\n    raster_file: PATH_TYPE,\n    output_folder: PATH_TYPE,\n    chip_size: int,\n    chip_stride: int,\n    label_vector_file: Optional[PATH_TYPE] = None,\n    label_column: Optional[str] = None,\n    label_remap: Optional[dict] = None,\n    write_empty_tiles: bool = False,\n    drop_transparency: bool = True,\n    remove_old: bool = True,\n    output_suffix: str = \".JPG\",\n    ROI_file: Optional[PATH_TYPE] = None,\n    background_ind: int = NULL_TEXTURE_INT_VALUE,\n):\n    \"\"\"Take raster data and tile it for machine learning training or inference\n\n    Args:\n        raster_file (PATH_TYPE):\n            Path to the raster file to tile.\n        output_folder (PATH_TYPE):\n            Where to write the tiled outputs.\n        chip_size (int):\n            Size of the square chip in pixels.\n        chip_stride (int):\n            The stride in pixels between sliding window tiles.\n        label_vector_file (Optional[PATH_TYPE], optional):\n            A path to a vector geofile for the same region as the raster file. If provided, a\n            parellel folder structure will be written to the chipped images that contains the\n            corresponding rasterized data from the vector file. This is primarily useful for\n            generating training data for ML. Defaults to None.\n        label_column (Optional[str], optional):\n            Which column to use within the provided file. If not provided, the index will be used.\n            Defaults to None.\n        label_remap (Optional[dict], optional):\n            A dictionary mapping from the values in the `label_column` to integers that will be used\n            for rasterization. Defaults to None.\n        write_empty_tiles (bool, optional):\n            Should tiles with no vector data be written. Defaults to False.\n        drop_transparency (bool, optional):\n            Should the forth channel be dropped if present. Defaults to True.\n        remove_old (bool, optional):\n            Remove `output_folder` if present. Defaults to True.\n        output_suffix (str, optional):\n            Suffix for written imagery files. Defaults to \".JPG\".\n        ROI_file (Optional[PATH_TYPE], optional):\n            Path to a geospatial region of interest to restrict tile generation to. Defaults to None.\n        background_ind (int, optional):\n            If labels are written, any un-labeled region will have this value.\n            Defaults to `NULL_TEXTURE_INT_VALUE`.\n    \"\"\"\n    # Remove the existing directory\n    if remove_old and os.path.isdir(output_folder):\n        shutil.rmtree(output_folder)\n\n    # Read the labels if provided\n    if label_vector_file is not None:\n        label_gdf = gpd.read_file(label_vector_file)\n    else:\n        label_gdf = None\n\n    # Open the raster file\n    with rio.open(raster_file, \"r\") as dataset:\n        working_CRS = dataset.crs\n        # Create a list of windows for reading\n        windows = create_windows(\n            dataset_h_w=(dataset.height, dataset.width),\n            window_size=chip_size,\n            window_stride=chip_stride,\n        )\n\n        desc = f\"Writing image chips to {output_folder}\"\n        if label_gdf is not None:\n            desc = f\"Writing image chips and labels to {output_folder}\"\n            label_gdf.to_crs(working_CRS, inplace=True)\n\n            if label_column is not None:\n                label_values = label_gdf[label_column].tolist()\n            else:\n                label_values = label_gdf.index.tolist()\n\n            if label_remap is not None:\n                label_values = [label_remap[old_label] for old_label in label_values]\n\n            label_shapes = list(zip(label_gdf.geometry.values, label_values))\n            labels_folder = Path(output_folder, \"anns\")\n            output_folder = Path(output_folder, \"imgs\")\n\n            ensure_folder(labels_folder)\n        ensure_folder(output_folder)\n\n        # Set up the ROI now that we have the working CRS\n        if ROI_file is not None:\n            ROI_gdf = gpd.read_file(ROI_file).to_crs(working_CRS)\n            ROI_geometry = ROI_gdf.dissolve().geometry.values[0]\n            if label_gdf is not None:\n                # Crop the labels dataframe to the ROI\n                label_gdf = label_gdf.intersection(ROI_geometry)\n        else:\n            ROI_geometry = None\n\n        for window in tqdm(windows, desc=desc):\n            if ROI_geometry is not None:\n                window_transformer = AffineTransformer(dataset.window_transform(window))\n                pixel_corners = (\n                    (0, 0),\n                    (0, chip_size),\n                    (chip_size, chip_size),\n                    (chip_size, 0),\n                )\n                geospatial_corners = [\n                    window_transformer.xy(pc[0], pc[1], offset=\"ul\")\n                    for pc in pixel_corners\n                ]\n                geospatial_corners.append(geospatial_corners[0])\n                window_polygon = Polygon(geospatial_corners)\n\n                if not ROI_geometry.intersects(window_polygon):\n                    # Skip writing this chip if it doesn't intersect the ROI\n                    continue\n\n            if label_gdf is not None:\n                window_transform = dataset.window_transform(window)\n                window_transformer = AffineTransformer(window_transform)\n                labels_raster = rasterize(\n                    label_shapes,\n                    out_shape=(chip_size, chip_size),\n                    transform=window_transform,\n                    fill=background_ind,\n                )\n                labels_raster = labels_raster.astype(np.uint8)\n                # See if we should skip this tile since it's only background data\n                if not write_empty_tiles and np.all(\n                    labels_raster == NULL_TEXTURE_INT_VALUE\n                ):\n                    continue\n\n                # Write out the label\n                output_file_name = Path(\n                    labels_folder,\n                    get_str_from_window(\n                        raster_file=raster_file, window=window, suffix=\".png\"\n                    ),\n                )\n                imwrite(\n                    output_file_name,\n                    pad_to_full_size(labels_raster, (chip_size, chip_size)),\n                )\n\n            windowed_raster = dataset.read(window=window)\n            windowed_img = reshape_as_image(windowed_raster)\n\n            if drop_transparency and windowed_img.shape[2] == 4:\n                transparency = windowed_img[..., 3]\n                windowed_img = windowed_img[..., :3]\n                # Set transperent regions to black\n                mask = transparency == 0\n                if np.all(mask):\n                    continue\n\n                windowed_img[mask, :] = 0\n\n            output_file_name = Path(\n                output_folder,\n                get_str_from_window(\n                    raster_file=raster_file, window=window, suffix=output_suffix\n                ),\n            )\n            imwrite(\n                output_file_name,\n                pad_to_full_size(\n                    windowed_img,\n                    (chip_size, chip_size),\n                ),\n            )\n</code></pre>"},{"location":"API_reference/predictors/segmentor/","title":"Segmentor","text":""},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor","title":"<code>Segmentor</code>","text":"Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>class Segmentor:\n    def __init__(self, num_classes=None):\n        self.num_classes = num_classes\n\n    def setup(self, **kwargs) -&gt; None:\n        \"\"\"This is for things like loading a model. It's fine to not override it if there's no setup\"\"\"\n        pass\n\n    def segment_image(self, image: np.ndarray, **kwargs):\n        \"\"\"Produce a segmentation mask for an image\n\n        Args:\n            image (np.ndarray): np\n        \"\"\"\n        raise NotImplementedError(\"Abstract base class\")\n\n    def segment_image_batch(self, images: typing.List[np.ndarray], **kwargs):\n        \"\"\"\n        Segment a batch of images, to potentially use full compute capacity. The current implementation\n        should be overriden when there is a way to get improvements\n\n        Args:\n            images (typing.List[np.ndarray]): The list of images\n        \"\"\"\n        segmentations = []\n\n        for image in images:\n            segmentation = self.segment_image(image, **kwargs)\n            segmentations.append(segmentation)\n        return segmentations\n\n    @staticmethod\n    def inds_to_one_hot(\n        inds_image: np.ndarray,\n        num_classes: typing.Union[int, None] = None,\n        ignore_ind: int = 255,\n    ) -&gt; np.ndarray:\n        \"\"\"Convert an image of indices to a one-hot, one-per-channel encoding\n\n        Args:\n            inds_image (np.ndarray): Image of integer indices. (m, n)\n            num_classes (int, None): The number of classes. If None, computed as the max index provided. Default None\n            ignore_ind (inte, optional): This index is an ignored class\n\n        Returns:\n            np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False\n        \"\"\"\n        if num_classes is None:\n            inds_image_copy = inds_image.copy()\n            # Mask out ignore ind so it's not used in computation\n            inds_image_copy[inds_image_copy == ignore_ind] == 0\n            num_classes = np.max(inds_image_copy) + 1\n\n        one_hot_array = np.zeros(\n            (inds_image.shape[0], inds_image.shape[1], num_classes), dtype=bool\n        )\n        # Iterate up to max ind, not num_classes to avoid wasted computation when there won't be matches\n        for i in range(num_classes):\n            # TODO determine if there are any more efficient ways to do this\n            # Maybe create all these slices and then concatenate\n            # Or test equality with an array that has all the values in it\n            one_hot_array[..., i] = inds_image == i\n\n        return one_hot_array\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.inds_to_one_hot","title":"<code>inds_to_one_hot(inds_image, num_classes=None, ignore_ind=255)</code>  <code>staticmethod</code>","text":"<p>Convert an image of indices to a one-hot, one-per-channel encoding</p> <p>Parameters:</p> Name Type Description Default <code>inds_image</code> <code>ndarray</code> <p>Image of integer indices. (m, n)</p> required <code>num_classes</code> <code>(int, None)</code> <p>The number of classes. If None, computed as the max index provided. Default None</p> <code>None</code> <code>ignore_ind</code> <code>inte</code> <p>This index is an ignored class</p> <code>255</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False</p> Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>@staticmethod\ndef inds_to_one_hot(\n    inds_image: np.ndarray,\n    num_classes: typing.Union[int, None] = None,\n    ignore_ind: int = 255,\n) -&gt; np.ndarray:\n    \"\"\"Convert an image of indices to a one-hot, one-per-channel encoding\n\n    Args:\n        inds_image (np.ndarray): Image of integer indices. (m, n)\n        num_classes (int, None): The number of classes. If None, computed as the max index provided. Default None\n        ignore_ind (inte, optional): This index is an ignored class\n\n    Returns:\n        np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False\n    \"\"\"\n    if num_classes is None:\n        inds_image_copy = inds_image.copy()\n        # Mask out ignore ind so it's not used in computation\n        inds_image_copy[inds_image_copy == ignore_ind] == 0\n        num_classes = np.max(inds_image_copy) + 1\n\n    one_hot_array = np.zeros(\n        (inds_image.shape[0], inds_image.shape[1], num_classes), dtype=bool\n    )\n    # Iterate up to max ind, not num_classes to avoid wasted computation when there won't be matches\n    for i in range(num_classes):\n        # TODO determine if there are any more efficient ways to do this\n        # Maybe create all these slices and then concatenate\n        # Or test equality with an array that has all the values in it\n        one_hot_array[..., i] = inds_image == i\n\n    return one_hot_array\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.segment_image","title":"<code>segment_image(image, **kwargs)</code>","text":"<p>Produce a segmentation mask for an image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>np</p> required Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def segment_image(self, image: np.ndarray, **kwargs):\n    \"\"\"Produce a segmentation mask for an image\n\n    Args:\n        image (np.ndarray): np\n    \"\"\"\n    raise NotImplementedError(\"Abstract base class\")\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.segment_image_batch","title":"<code>segment_image_batch(images, **kwargs)</code>","text":"<p>Segment a batch of images, to potentially use full compute capacity. The current implementation should be overriden when there is a way to get improvements</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[ndarray]</code> <p>The list of images</p> required Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def segment_image_batch(self, images: typing.List[np.ndarray], **kwargs):\n    \"\"\"\n    Segment a batch of images, to potentially use full compute capacity. The current implementation\n    should be overriden when there is a way to get improvements\n\n    Args:\n        images (typing.List[np.ndarray]): The list of images\n    \"\"\"\n    segmentations = []\n\n    for image in images:\n        segmentation = self.segment_image(image, **kwargs)\n        segmentations.append(segmentation)\n    return segmentations\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.setup","title":"<code>setup(**kwargs)</code>","text":"<p>This is for things like loading a model. It's fine to not override it if there's no setup</p> Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def setup(self, **kwargs) -&gt; None:\n    \"\"\"This is for things like loading a model. It's fine to not override it if there's no setup\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/utils/example_data/","title":"Example Data","text":""},{"location":"API_reference/utils/example_data/#geograypher.utils.example_data","title":"<code>example_data</code>","text":""},{"location":"API_reference/utils/files/","title":"Files","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files","title":"<code>files</code>","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files-functions","title":"Functions","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files.ensure_containing_folder","title":"<code>ensure_containing_folder(filename)</code>","text":"<p>Ensure the folder containing this file exists. Nothing happens if already present.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>PATH_TYPE</code> <p>The path to the file for which the containing folder should be created</p> required Source code in <code>geograypher/utils/files.py</code> <pre><code>def ensure_containing_folder(filename: PATH_TYPE):\n    \"\"\"Ensure the folder containing this file exists. Nothing happens if already present.\n\n    Args:\n        filename (PATH_TYPE): The path to the file for which the containing folder should be created\n    \"\"\"\n    # Cast the file to a pathlib Path\n    filename = Path(filename)\n    # Get the folder above it\n    containing_folder = filename.parent\n    # Create this folder and all parent folders if needed. Nothing happens if it already exists\n    ensure_folder(containing_folder)\n</code></pre>"},{"location":"API_reference/utils/files/#geograypher.utils.files.ensure_folder","title":"<code>ensure_folder(folder)</code>","text":"<p>Ensure this folder, and parent folders, exist. Nothing happens if already present</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PATH_TYPE</code> <p>Path to folder to ensure exists</p> required Source code in <code>geograypher/utils/files.py</code> <pre><code>def ensure_folder(folder: PATH_TYPE):\n    \"\"\"Ensure this folder, and parent folders, exist. Nothing happens if already present\n\n    Args:\n        folder (PATH_TYPE): Path to folder to ensure exists\n    \"\"\"\n    folder = Path(folder)\n    # Create this folder and all parent folders if needed. Nothing happens if it already exists\n    folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"API_reference/utils/geometric/","title":"Geometric","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric","title":"<code>geometric</code>","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric-functions","title":"Functions","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.angle_between","title":"<code>angle_between(v1, v2)</code>","text":"<p>Returns the angle in radians between vectors 'v1' and 'v2'::</p> <p>angle_between((1, 0, 0), (0, 1, 0)) 1.5707963267948966 angle_between((1, 0, 0), (1, 0, 0)) 0.0 angle_between((1, 0, 0), (-1, 0, 0)) 3.141592653589793</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def angle_between(v1, v2):\n    \"\"\"Returns the angle in radians between vectors 'v1' and 'v2'::\n\n    &gt;&gt;&gt; angle_between((1, 0, 0), (0, 1, 0))\n    1.5707963267948966\n    &gt;&gt;&gt; angle_between((1, 0, 0), (1, 0, 0))\n    0.0\n    &gt;&gt;&gt; angle_between((1, 0, 0), (-1, 0, 0))\n    3.141592653589793\n    \"\"\"\n    v1_u = unit_vector(v1)\n    v2_u = unit_vector(v2)\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n</code></pre>"},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.batched_unary_union","title":"<code>batched_unary_union(geometries, batch_size, grid_size=None, subsequent_batch_size=4, sort_by_loc=False, simplify_tol=0, verbose=False)</code>","text":"<p>Roughly replicate the functionality of shapely.unary_union using a batched implementation</p> <p>Parameters:</p> Name Type Description Default <code>geometries</code> <code>List[Geometry]</code> <p>Geometries to aggregate</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the first aggregation</p> required <code>grid_size</code> <code>Union[None, float]</code> <p>grid size passed to unary_union</p> <code>None</code> <code>subsequent_batch_size</code> <code>int</code> <p>The batch size for subsequent (recursive) batches. Defaults to 4.</p> <code>4</code> <code>sort_by_loc</code> <code>bool</code> <p>Should the polygons be sorted by location to have a higher likelihood of merging. Defaults to False.</p> <code>False</code> <code>simplify_tol</code> <code>float</code> <p>How much to simplify in intermediate steps</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>Should additional print outs be provided</p> <code>False</code> <p>Returns:</p> Type Description <code>MultiPolygon</code> <p>shapely.MultiPolygon: The merged multipolygon</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def batched_unary_union(\n    geometries: typing.List[shapely.Geometry],\n    batch_size: int,\n    grid_size: typing.Union[None, float] = None,\n    subsequent_batch_size: int = 4,\n    sort_by_loc: bool = False,\n    simplify_tol: float = 0,\n    verbose: bool = False,\n) -&gt; shapely.MultiPolygon:\n    \"\"\"Roughly replicate the functionality of shapely.unary_union using a batched implementation\n\n    Args:\n        geometries (typing.List[shapely.Geometry]): Geometries to aggregate\n        batch_size (int): The batch size for the first aggregation\n        grid_size (typing.Union[None, float]): grid size passed to unary_union\n        subsequent_batch_size (int, optional): The batch size for subsequent (recursive) batches. Defaults to 4.\n        sort_by_loc (bool, optional): Should the polygons be sorted by location to have a higher likelihood of merging. Defaults to False.\n        simplify_tol (float, optional): How much to simplify in intermediate steps\n        verbose (bool, optional): Should additional print outs be provided\n\n    Returns:\n        shapely.MultiPolygon: The merged multipolygon\n    \"\"\"\n\n    # If the geoemtry is already one entry or empty\n    if len(geometries) &lt;= 1:\n        # Run unary_union to ensure that edge cases such as empty geometries are handled in the expected way\n        return unary_union(geometries)\n\n    # Sort the polygons by their least x coordinates (any bound could be used)\n    # The goal of this is to have a higher likelihood of grouping objects together and removing interior coordinates\n    if sort_by_loc and batch_size &lt; len(geometries):\n        logger.error(f\"Sorting the geometries with {len(geometries)} entries\")\n        geometries = sorted(geometries, key=lambda x: x.bounds[0])\n        logger.error(\"Done sorting geometries\")\n\n    # TODO you could consider requesting a give number of points in the batch,\n    # rather than number of objects.\n    # TODO you could consider multiprocessing this since it's embarassingly parallel\n\n    # Wrap the iteration in tqdm if requested, else just return it\n    iteration_decorator = lambda x: (\n        tqdm(x, desc=f\"Computing batched unary union with batch size {batch_size}\")\n        if verbose\n        else x\n    )\n\n    # Compute batched version\n    batched_unions = []\n    for i in iteration_decorator(\n        range(0, len(geometries), batch_size),\n    ):\n        batch = geometries[i : i + batch_size]\n        batched_unions.append(unary_union(batch, grid_size=grid_size))\n\n    # Simplify the geometry to reduce the number of points\n    # Don't do this if it would otherwise be returned as-is\n    if simplify_tol &gt; 0.0 and len(batched_unions) &gt; 1:\n        # Simplify then buffer, to make sure we don't have holes\n        logger.info(\n            f\"Lengths before simplification {[len(bu.geoms) for bu in batched_unions]}\"\n        )\n        batched_unions = [\n            simplify(bu, simplify_tol).buffer(simplify_tol)\n            for bu in tqdm(batched_unions, desc=\"simplifying polygons\")\n        ]\n        logger.info(\n            f\"Lengths after simplification {[len(bu.geoms) for bu in batched_unions]}\"\n        )\n    # Recurse this process until there's only one merged geometry\n    # All future calls will use the subsequent_batch_size\n    # TODO this batch size could be computed more inteligently, or sidestepped by requesting a number of points\n    # Don't sort because this should already be sorted\n    # Don't simplify because we don't want to repeatedly degrade the geometry\n    return batched_unary_union(\n        batched_unions,\n        batch_size=subsequent_batch_size,\n        grid_size=grid_size,\n        subsequent_batch_size=subsequent_batch_size,\n        sort_by_loc=False,\n        simplify_tol=0.0,\n    )\n</code></pre>"},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.clip_line_segments","title":"<code>clip_line_segments(boundaries, origins, directions, image_indices, ray_limit=None)</code>","text":"<p>Clips line segments between two boundary surfaces using ray tracing, keeping only the segments that intersect both surfaces.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>Tuple[PolyData, PolyData]</code> <p>A tuple containing two PyVista PolyData surfaces representing the boundaries.</p> required <code>origins</code> <code>ndarray</code> <p>Array of ray origin points (shape: [N, 3]).</p> required <code>directions</code> <code>ndarray</code> <p>Array of ray direction vectors (shape: [N, 3]).</p> required <code>image_indices</code> <code>List[int]</code> <p>List of indices associated with each ray, used for identification. Tracks which image each ray corresponded to.</p> required <code>ray_limit</code> <code>Optional[float]</code> <p>If provided, segments longer than this value are dropped as invalid. Note that the filtered distance is from the original ray start to the second boundary. This is to mimic measuring from a camera (hypothetical ray source) to the ground (assuming the boundaries are given as [ceiling, floor]).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: - (N, 3) array of start points of clipped segments. This will be the point     that intersected boundaries[0] - (N, 3) array of end points of clipped segments. This will be the point     that intersected boundaries[1] - (N, 3) array of direction vectors for the clipped segments. - (N,) array of image indices corresponding to the clipped segments.</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def clip_line_segments(\n    boundaries: typing.Tuple[pv.PolyData, pv.PolyData],\n    origins: np.ndarray,\n    directions: np.ndarray,\n    image_indices: typing.Union[typing.List[int], np.ndarray],\n    ray_limit: typing.Optional[float] = None,\n) -&gt; typing.Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Clips line segments between two boundary surfaces using ray tracing,\n    keeping only the segments that intersect both surfaces.\n\n    Args:\n        boundaries (Tuple[pv.PolyData, pv.PolyData]):\n            A tuple containing two PyVista PolyData surfaces representing the boundaries.\n        origins (np.ndarray):\n            Array of ray origin points (shape: [N, 3]).\n        directions (np.ndarray):\n            Array of ray direction vectors (shape: [N, 3]).\n        image_indices (List[int]):\n            List of indices associated with each ray, used for identification.\n            Tracks which image each ray corresponded to.\n        ray_limit (Optional[float], optional):\n            If provided, segments longer than this value are dropped as invalid. Note\n            that the filtered distance is from the **original ray start to the second\n            boundary**. This is to mimic measuring from a camera (hypothetical ray\n            source) to the ground (assuming the boundaries are given as [ceiling, floor]).\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n            - (N, 3) array of start points of clipped segments. This will be the point\n                that intersected boundaries[0]\n            - (N, 3) array of end points of clipped segments. This will be the point\n                that intersected boundaries[1]\n            - (N, 3) array of direction vectors for the clipped segments.\n            - (N,) array of image indices corresponding to the clipped segments.\n\n    Raises:\n        ValueError if various input requirements are not met.\n    \"\"\"\n\n    # Input checking\n    if len(boundaries) != 2:\n        raise ValueError(f\"2 boundaries required, not {len(boundaries)}\")\n    if any([not isinstance(b, pv.PolyData) for b in boundaries]):\n        raise ValueError(f\"pv.PolyData required, found {[type(b) for b in boundaries]}\")\n    if origins.shape != directions.shape:\n        raise ValueError(\n            f\"origins and directions mismatched ({origins.shape} != {directions.shape})\"\n        )\n    if origins.shape[1] != 3:\n        raise ValueError(f\"(N, 3) input arrays required, found {origins.shape}\")\n    if len(origins) != len(image_indices):\n        raise ValueError(\n            f\"origins and image indices mismatched ({len(origins)} !=\"\n            f\" {len(image_indices)})\"\n        )\n\n    # Handle the empty case\n    if len(origins) == 0:\n        return (\n            origins.copy(),\n            origins.copy(),\n            directions.copy(),\n            np.array(image_indices),\n        )\n\n    # Calculate ray/boundary intersections\n    pts0, idx0, _ = boundaries[0].multi_ray_trace(\n        origins=origins,\n        directions=directions,\n        first_point=True,\n        retry=True,\n    )\n    pts1, idx1, _ = boundaries[1].multi_ray_trace(\n        origins=origins,\n        directions=directions,\n        first_point=True,\n        retry=True,\n    )\n\n    # Find the ray indices that were found to intersect with both surfaces\n    matched_set = list(set(idx0).intersection(set(idx1)))\n\n    # Iterate through the rays with intersections and update the starts and ends\n    new_starts = []\n    new_ends = []\n    new_directions = []\n    new_indices = []\n\n    for ray_index in matched_set:\n        pt0 = pts0[np.where(idx0 == ray_index)[0][0]]\n        pt1 = pts1[np.where(idx1 == ray_index)[0][0]]\n\n        # Limit the length of the origin \u2192 boundary[1] vector if requested\n        if ray_limit is not None:\n            vector = origins[ray_index] - pt1\n            if np.linalg.norm(vector) &gt; ray_limit:\n                continue\n\n        # Keep track of the outputs\n        new_starts.append(pt0)\n        new_ends.append(pt1)\n        new_directions.append((pt1 - pt0) / np.linalg.norm(pt1 - pt0))\n        new_indices.append(image_indices[ray_index])\n\n    return (\n        np.array(new_starts),\n        np.array(new_ends),\n        np.array(new_directions),\n        np.array(new_indices),\n    )\n</code></pre>"},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.unit_vector","title":"<code>unit_vector(vector)</code>","text":"<p>Returns the unit vector of the vector.</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def unit_vector(vector):\n    \"\"\"Returns the unit vector of the vector.\"\"\"\n    return vector / np.linalg.norm(vector)\n</code></pre>"},{"location":"API_reference/utils/geospatial/","title":"Geospatial","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial","title":"<code>geospatial</code>","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial-functions","title":"Functions","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.ensure_projected_CRS","title":"<code>ensure_projected_CRS(geodata)</code>","text":"<p>Returns a projected geodataframe from the provided geodataframe by converting it to ESPG:4326 (if not already) and determining the projected CRS from the point coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geodata</code> <code>GeoDataGrame</code> <p>Original geodataframe that is potentially unprojected</p> required <p>Returns:     gpd.GeoDataGrame: projected geodataframe</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def ensure_projected_CRS(geodata: gpd.GeoDataFrame):\n    \"\"\"Returns a projected geodataframe from the provided geodataframe by converting it to\n    ESPG:4326 (if not already) and determining the projected CRS from the point\n    coordinates.\n\n    Args:\n        geodata (gpd.GeoDataGrame): Original geodataframe that is potentially unprojected\n    Returns:\n        gpd.GeoDataGrame: projected geodataframe\n    \"\"\"\n    # If CRS is projected return immediately\n    if geodata.crs.is_projected:\n        return geodata\n\n    # If CRS is geographic and not long-lat, convert it to long-lat\n    if geodata.crs.is_geographic and geodata.crs != LAT_LON_CRS:\n        geodata = geodata.to_crs(LAT_LON_CRS)\n\n    # Convert geographic long-lat CRS to projected CRS\n    point = geodata[\"geometry\"][0].centroid\n    geometric_crs = get_projected_CRS(lon=point.x, lat=point.y)\n    return geodata.to_crs(geometric_crs)\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.get_overlap_raster","title":"<code>get_overlap_raster(unlabeled_df, classes_raster, num_classes=None, normalize=False)</code>","text":"<p>Get the overlap for each polygon in the unlabeled DF with each class in the raster</p> <p>Parameters:</p> Name Type Description Default <code>unlabeled_df</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Dataframe or path to dataframe containing geometries per object</p> required <code>classes_raster</code> <code>PATH_TYPE</code> <p>Path to a categorical raster</p> required <code>num_classes</code> <code>Union[None, int]</code> <p>Number of classes, if None defaults to the highest overlapping class. Defaults to None.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Normalize counts matrix from pixels to fraction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid, n_classes) counts per polygon per class</p> <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def get_overlap_raster(\n    unlabeled_df: typing.Union[PATH_TYPE, GeoDataFrame],\n    classes_raster: PATH_TYPE,\n    num_classes: typing.Union[None, int] = None,\n    normalize: bool = False,\n) -&gt; (np.ndarray, np.ndarray):\n    \"\"\"Get the overlap for each polygon in the unlabeled DF with each class in the raster\n\n    Args:\n        unlabeled_df (typing.Union[PATH_TYPE, GeoDataFrame]):\n            Dataframe or path to dataframe containing geometries per object\n        classes_raster (PATH_TYPE): Path to a categorical raster\n        num_classes (typing.Union[None, int], optional):\n            Number of classes, if None defaults to the highest overlapping class. Defaults to None.\n        normalize (bool, optional): Normalize counts matrix from pixels to fraction. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_valid, n_classes) counts per polygon per class\n        np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions\n    \"\"\"\n    unlabeled_df = coerce_to_geoframe(unlabeled_df)\n\n    with rio.open(classes_raster, \"r\") as src:\n        raster_crs = src.crs\n        # Note this is a shapely object with no CRS, but we ensure\n        # the vectors are converted to the same CRS\n        raster_bounds = box(*src.bounds)\n\n    # Ensure that the vector data is in the same CRS as the raster\n    if raster_crs != unlabeled_df.crs:\n        # Avoid doing this in place because we don't want to modify the input dataframe\n        # This should properly create a copy\n        unlabeled_df = unlabeled_df.to_crs(raster_crs)\n\n    # Compute which polygons intersect the raster region\n    within_bounds_IDs = intersects_union_of_polygons(unlabeled_df, raster_bounds)\n\n    # Compute the stats\n    stats = zonal_stats(\n        unlabeled_df.iloc[within_bounds_IDs], str(classes_raster), categorical=True\n    )\n\n    # Find which polygons have non-null class predictions\n    # Due to nondata regions, some polygons within the region may not have class information\n    valid_prediction_IDs = np.where([x != {} for x in stats])[0]\n\n    # Determine the number of classes if not set\n    if num_classes is None:\n        # Find the max value that show up in valid predictions\n        num_classes = 1 + np.max(\n            [np.max(list(stats[i].keys())) for i in valid_prediction_IDs]\n        )\n\n    # Build the counts matrix for non-null predictions\n    counts_matrix = np.zeros((len(valid_prediction_IDs), num_classes))\n\n    # Fill the counts matrix\n    for i in valid_prediction_IDs:\n        for j, count in stats[i].items():\n            counts_matrix[i, j] = count\n\n    # Bookkeeping to find the IDs that were both within the raster and non-null\n    valid_IDs_in_original = within_bounds_IDs[valid_prediction_IDs]\n\n    if normalize:\n        counts_matrix = counts_matrix / np.sum(counts_matrix, axis=1, keepdims=True)\n\n    return counts_matrix, valid_IDs_in_original\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.get_overlap_vector","title":"<code>get_overlap_vector(unlabeled_df, classes_df, class_column, normalize=False)</code>","text":"<p>For each element in unlabeled df, return the fractional overlap with each class in classes_df</p> <p>Parameters:</p> Name Type Description Default <code>unlabeled_df</code> <code>GeoDataFrame</code> <p>A dataframe of geometries</p> required <code>classes_df</code> <code>GeoDataFrame</code> <p>A dataframe of classes</p> required <code>class_column</code> <code>str</code> <p>Which column in the classes_df to use. Defaults to \"names\".</p> required <code>normalize</code> <code>bool</code> <p>Normalize counts matrix from area to fraction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid, n_classes) counts per polygon per class</p> <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def get_overlap_vector(\n    unlabeled_df: GeoDataFrame,\n    classes_df: GeoDataFrame,\n    class_column: str,\n    normalize: bool = False,\n) -&gt; (np.ndarray, np.ndarray):\n    \"\"\"\n    For each element in unlabeled df, return the fractional overlap with each class in\n    classes_df\n\n\n    Args:\n        unlabeled_df (GeoDataFrame): A dataframe of geometries\n        classes_df (GeoDataFrame): A dataframe of classes\n        class_column (str, optional): Which column in the classes_df to use. Defaults to \"names\".\n        normalize (bool, optional): Normalize counts matrix from area to fraction. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_valid, n_classes) counts per polygon per class\n        np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions\n    \"\"\"\n\n    ## Preprocessing\n    # Ensure that both a geodataframes\n    unlabeled_df = coerce_to_geoframe(unlabeled_df)\n    classes_df = coerce_to_geoframe(classes_df)\n\n    unlabeled_df = ensure_projected_CRS(unlabeled_df)\n    if classes_df.crs != unlabeled_df.crs:\n        classes_df = classes_df.to_crs(unlabeled_df.crs)\n\n    unlabeled_df.geometry = unlabeled_df.geometry.simplify(0.01)\n    classes_df.geometry = classes_df.geometry.simplify(0.01)\n\n    if class_column not in classes_df.columns:\n        raise ValueError(f\"Class column `{class_column}` not in {classes_df.columns}\")\n\n    logging.info(\n        \"Computing the intersection of the unlabeled polygons with the labeled region\"\n    )\n    # Find which unlabeled polygons intersect with the labeled region\n    intersection_IDs = intersects_union_of_polygons(unlabeled_df, classes_df)\n    logging.info(\"Finished computing intersection\")\n    # Extract only these polygons\n    unlabeled_df_intersecting_classes = unlabeled_df.iloc[intersection_IDs]\n    unlabeled_df_intersecting_classes[\"index\"] = unlabeled_df_intersecting_classes.index\n\n    # Add area field to each\n    unlabeled_df_intersecting_classes[\"unlabeled_area\"] = (\n        unlabeled_df_intersecting_classes.area\n    )\n\n    # Find the intersecting geometries\n    # We want only the ones that have some overlap with the unlabeled geometry, but I don't think that can be specified\n    logging.info(\"computing overlay\")\n    overlay = gpd.overlay(\n        classes_df,\n        unlabeled_df_intersecting_classes,\n        how=\"union\",\n        keep_geom_type=False,\n    )\n    # Drop the rows that only contain information from the class_labels\n    overlay = overlay.dropna(subset=\"index\")\n\n    # TODO look more into this part, something seems wrong\n    overlay[\"overlapping_area\"] = overlay.area\n    # overlay[\"per_class_area_fraction\"] = (\n    #    overlay[\"overlapping_area\"] / overlay[\"unlabeled_area\"]\n    # )\n    # Aggregating the results\n    # WARNING Make sure that this is a list and not a tuple or it gets considered one key\n    logging.info(\"computing groupby\")\n\n    # If the two dataframes have a column with the same name, they will be renamed\n    if (\n        f\"{class_column}_1\" in overlay.columns\n        and f\"{class_column}_2\" in overlay.columns\n    ):\n        aggregatation_class_column = f\"{class_column}_1\"\n    else:\n        aggregatation_class_column = class_column\n\n    # Groupby and aggregate\n    grouped_by = overlay.groupby(by=[\"index\", aggregatation_class_column])\n    aggregated = grouped_by.agg({\"overlapping_area\": \"sum\"})\n\n    # Extract the original class names\n    unique_class_names = sorted(classes_df[class_column].unique().tolist())\n    # And the indices from the original dataframe. This is relavent if the input\n    # dataframe was a subset of an original one\n    unique_index_values = sorted(unlabeled_df_intersecting_classes.index.tolist())\n\n    counts_matrix = np.zeros(\n        (len(unlabeled_df_intersecting_classes), len(unique_class_names))\n    )\n\n    for r in aggregated.iterrows():\n        (index, class_name), area = r\n        # The index is the index from the original unlabeled dataset, but we need the index into the subset\n        unlabled_object_ind = unique_index_values.index(index)\n        # Transform the class name into a class index\n        class_ind = unique_class_names.index(class_name)\n        # Set the value to the area of the overlap between that unlabeled object and given class\n        counts_matrix[unlabled_object_ind, class_ind] = float(area.iloc[0])\n\n    if normalize:\n        counts_matrix = counts_matrix / np.sum(counts_matrix, axis=1, keepdims=True)\n\n    return counts_matrix, intersection_IDs, unique_class_names\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.load_downsampled_raster_data","title":"<code>load_downsampled_raster_data(dataset_filename, downsample_factor)</code>","text":"<p>Load a raster file spatially downsampled</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PATH_TYPE</code> <p>Path to the raster</p> required <code>downsample_factor</code> <code>float</code> <p>Downsample factor of 10 means that pixels are 10 times larger</p> required <p>Returns:</p> Type Description <p>np.array: The downsampled array in the rasterio (c, h, w) convention</p> <p>rio.DatasetReader: The reader with the transform updated</p> <p>rio.Transform: The updated transform</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def load_downsampled_raster_data(dataset_filename: PATH_TYPE, downsample_factor: float):\n    \"\"\"Load a raster file spatially downsampled\n\n    Args:\n        dataset (PATH_TYPE): Path to the raster\n        downsample_factor (float): Downsample factor of 10 means that pixels are 10 times larger\n\n    Returns:\n        np.array: The downsampled array in the rasterio (c, h, w) convention\n        rio.DatasetReader: The reader with the transform updated\n        rio.Transform: The updated transform\n    \"\"\"\n    # Open the dataset handler. Note that this doesn't read into memory.\n    dataset = rio.open(dataset_filename)\n\n    # resample data to target shape\n    data = dataset.read(\n        out_shape=(\n            dataset.count,\n            int(dataset.height / downsample_factor),\n            int(dataset.width / downsample_factor),\n        ),\n        resampling=rio.enums.Resampling.bilinear,\n    )\n\n    # scale image transform\n    updated_transform = dataset.transform * dataset.transform.scale(\n        (dataset.width / data.shape[-1]), (dataset.height / data.shape[-2])\n    )\n    # Return the data and the transform\n    return data, dataset, updated_transform\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.reproject_raster","title":"<code>reproject_raster(in_path, out_path, out_crs=pyproj.CRS.from_epsg(4326))</code>","text":"Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def reproject_raster(in_path, out_path, out_crs=pyproj.CRS.from_epsg(4326)):\n    \"\"\" \"\"\"\n    logging.warn(\"Starting to reproject raster\")\n    # reproject raster to project crs\n    with rio.open(in_path) as src:\n        src_crs = src.crs\n        transform, width, height = rio.warp.calculate_default_transform(\n            src_crs, out_crs, src.width, src.height, *src.bounds\n        )\n        kwargs = src.meta.copy()\n\n        kwargs.update(\n            {\"crs\": out_crs, \"transform\": transform, \"width\": width, \"height\": height}\n        )\n\n        with rio.open(out_path, \"w\", **kwargs) as dst:\n            for i in tqdm(range(1, src.count + 1), desc=\"Reprojecting bands\"):\n                rio.warp.reproject(\n                    source=rio.band(src, i),\n                    destination=rio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=out_crs,\n                    resampling=rio.warp.Resampling.nearest,\n                )\n    logging.warn(\"Done reprojecting raster\")\n</code></pre>"},{"location":"API_reference/utils/image/","title":"Image","text":""},{"location":"API_reference/utils/image/#geograypher.utils.image","title":"<code>image</code>","text":""},{"location":"API_reference/utils/image/#geograypher.utils.image-functions","title":"Functions","text":""},{"location":"API_reference/utils/image/#geograypher.utils.image.flexible_inputs_warp","title":"<code>flexible_inputs_warp(input_image, inverse_map, interpolation_order=None, fill_value=0.0)</code>","text":"<p>Extends the functionality of skimage.transform.warp to handle arbitrary datatypes and multi-channel images</p> Source code in <code>geograypher/utils/image.py</code> <pre><code>def flexible_inputs_warp(\n    input_image: np.ndarray,\n    inverse_map: np.ndarray,\n    interpolation_order: int = None,\n    fill_value: float = 0.0,\n) -&gt; np.ndarray:\n    \"\"\"\n    Extends the functionality of skimage.transform.warp to handle arbitrary datatypes and\n    multi-channel images\n    \"\"\"\n    # Temporarily expand grayscale images to be (N, M, 1)\n    input_image = np.atleast_3d(input_image)\n\n    # Note if the fill_value is not actually used and is in fact outside the range of the input\n    # values it will result in an artificially small range of data for the real values. But it won't\n    # affect the results aside from the possible loss of precision.\n    # Compute the min and max of values, including both the inputs and the fill value\n    input_min = min(np.min(input_image), fill_value)\n    input_max = max(np.max(input_image), fill_value)\n    # Compute the range of values\n    min_max_range = input_max - input_min\n\n    # If there's no variation, we can avoid division by 0 issues and save computation by just\n    # returning an array of one value\n    if min_max_range == 0:\n        return np.full_like(np.squeeze(input_image), fill_value=fill_value)\n\n    # Record the original datatype\n    initial_dtype = input_image.dtype\n    # Rescale to 0-1\n    input_image = (input_image.astype(float) - input_min) / min_max_range\n    # The fill value will be applied to the rescaled image, so it must be rescaled similarly\n    rescaled_fill_value = (float(fill_value) - input_min) / min_max_range\n\n    # Create an output image that's the shape of inverse map (minus first dimension which is 2 [i,j])\n    # and the number of channels from the input image\n    output_image = np.zeros(inverse_map.shape[1:] + (input_image.shape[2],))\n    # For each color channel, do the warping\n    for channel in range(input_image.shape[2]):\n        warped = warp(\n            image=input_image[:, :, channel],\n            inverse_map=inverse_map,\n            order=interpolation_order,  # interpolation strategy for fractional pixels\n            mode=\"constant\",  # fill unseen areas with cval\n            cval=rescaled_fill_value,\n            clip=True,  # clip to [0,1]\n            preserve_range=True,  # keep original range without rescaling\n        )\n        output_image[:, :, channel] = warped\n\n    # Convert to the original range and datatype\n    output_image = ((output_image * min_max_range) + input_min).astype(initial_dtype)\n\n    # If the original image was grayscale, remove the added singleton dimension\n    return np.squeeze(output_image)\n</code></pre>"},{"location":"API_reference/utils/image/#geograypher.utils.image.perspective_from_equirectangular","title":"<code>perspective_from_equirectangular(equi_img, fov_deg, output_size=(1440, 1440), yaw_deg=0, pitch_deg=0, roll_deg=0, warp_order=1, oversample_factor=1, return_mask=False)</code>","text":"<p>Sample a perspective image from an equirectangular (360) image. Different parameters of the \"virtual camera\" can be controlled, including the field of view, orientation (yaw, pitch, roll), and output image size.</p> <p>The rotations are defined using the roll-pitch-yaw convention. The roll axis corresponds to the camera Z (forward) axis, pitch corresponds to the X axis, and yaw corresponds to the Y axis.</p> <p>With RPY all set to zero, the camera is looking toward the center of the equirectangular image.</p> <p>Parameters:</p> Name Type Description Default <code>equi_img</code> <code>ndarray</code> <p>Equirectangular image in (H, W, C) format.</p> required <code>fov_deg float</code> <p>camera horizontal field of view.</p> required <code>output_size Tuple[int, int]</code> <p>Shape of the output image to sample (i, j).</p> required <code>yaw_deg</code> <code>float</code> <p>yaw camera orientation. Defaults to 0.</p> <code>0</code> <code>pitch_deg</code> <code>float</code> <p>pitch camera orientation. Defaults to 0.</p> <code>0</code> <code>roll_deg</code> <code>float</code> <p>roll camera orientation. Defaults to 0.</p> <code>0</code> <code>warp_order</code> <code>int</code> <p>Interpolation order to use when sampling pixels. Defaults to 1.</p> <code>1</code> <code>oversample_factor</code> <code>int</code> <p>Sample this number times more pixels in each dimension. This helps avoid aliasing but increases runtime. Defaults to 1.</p> <code>1</code> <code>return_mask</code> <code>bool</code> <p>Return a mask showing what pixels from the original image were  sampled. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The sampled perspective image</p> <code>Optional[ndarray]</code> <p>Optional[np.ndarray]]: The mask of sampled locations, if requested</p> Source code in <code>geograypher/utils/image.py</code> <pre><code>def perspective_from_equirectangular(\n    equi_img: np.ndarray,\n    fov_deg: float,\n    output_size: Tuple[int, int] = (1440, 1440),\n    yaw_deg: float = 0,\n    pitch_deg: float = 0,\n    roll_deg: float = 0,\n    warp_order: int = 1,\n    oversample_factor: int = 1,\n    return_mask: bool = False,\n) -&gt; Tuple[np.ndarray, Optional[np.ndarray]]:\n    \"\"\"\n    Sample a perspective image from an equirectangular (360) image. Different parameters of the\n    \"virtual camera\" can be controlled, including the field of view, orientation (yaw, pitch, roll),\n    and output image size.\n\n    The rotations are defined using the roll-pitch-yaw convention. The roll axis corresponds to the\n    camera Z (forward) axis, pitch corresponds to the X axis, and yaw corresponds to the Y axis.\n\n    With RPY all set to zero, the camera is looking toward the center of the equirectangular image.\n\n    Args:\n        equi_img (np.ndarray):\n            Equirectangular image in (H, W, C) format.\n        fov_deg float: camera horizontal field of view.\n        output_size Tuple[int, int]: Shape of the output image to sample (i, j).\n        yaw_deg (float, optional): yaw camera orientation. Defaults to 0.\n        pitch_deg (float, optional): pitch camera orientation. Defaults to 0.\n        roll_deg (float, optional): roll camera orientation. Defaults to 0.\n        warp_order (int, optional): Interpolation order to use when sampling pixels. Defaults to 1.\n        oversample_factor (int, optional):\n            Sample this number times more pixels in each dimension. This helps avoid aliasing but\n            increases runtime. Defaults to 1.\n        return_mask (bool, optional):\n           Return a mask showing what pixels from the original image were\n             sampled. Defaults to False.\n\n    Returns:\n        np.ndarray: The sampled perspective image\n        Optional[np.ndarray]]: The mask of sampled locations, if requested\n    \"\"\"\n    # Convert an equirectangular (360) image to a perspective view.\n    H, W = equi_img.shape[:2]\n\n    # Output image dimensions\n    out_h, out_w = output_size\n\n    # Sample a larger image and then downsample to reduce aliasing\n    out_w = int(out_w * oversample_factor)\n    out_h = int(out_h * oversample_factor)\n\n    # FOV and angles in radians\n    fov = np.deg2rad(fov_deg)\n\n    # Compute the aspect ratio of the requested image dimensions\n    aspect_ratio = out_h / out_w\n\n    # Compute the distance from the center of the image to the edge in the x dimension\n    x_dist = np.tan(fov / 2)\n    y_dist = x_dist * aspect_ratio\n\n    # Compute the size of each pixel to center the rays within the pixel rather than using the\n    # top-left corner\n    pixel_width = (2 * x_dist) / out_w\n\n    # Create homogenous image coordinates for the output image\n    x = np.arange(-x_dist + pixel_width / 2, x_dist, pixel_width)\n    y = np.arange(-y_dist + pixel_width / 2, y_dist, pixel_width)\n\n    # invert y for image coordinates to account for array indexing notation\n    xv, yv = np.meshgrid(x, -y)\n\n    # The z direction\n    zv = np.ones_like(xv)\n\n    pixel_directions = np.stack((xv, yv, zv), axis=-1)\n    # normalize to unit\n    pixel_directions /= np.linalg.norm(pixel_directions, axis=-1, keepdims=True)\n\n    rotation_matrix = rotate_by_roll_pitch_yaw(roll_deg, pitch_deg, yaw_deg)\n\n    # Rotate the pixel directions by the rotation matrix\n    # The strange convention here is to deal with the fact that pixel_directions is (w, h, 3)\n    # so this allows the dimensions to align for the matrix multiplication.\n    pixel_directions = pixel_directions @ rotation_matrix.T\n\n    # Convert 3D directions to spherical coordinates\n    # horizontal angle\n    horizontal = np.arctan2(pixel_directions[..., 0], pixel_directions[..., 2])\n    # vertical angle. Clip to avoid floating point errors which extend beyond the valid domain\n    # of arcsin\n    altitude = np.arcsin(np.clip(pixel_directions[..., 1], -1.0, 1.0))\n\n    # Map to equirectangular image coordinates\n    i = (0.5 - altitude / np.pi) * H\n    j = (horizontal / (2 * np.pi) + 0.5) * W\n    # The range of these coordinates would be i in [0, H], j in [0, W]\n    # To account for the wraparound effects if the sampling goes of the right edge, add the left row\n    # of pixels to the right side.\n    # TODO think more about sub-pixel considerations and how these coordinates interact with skimage\n    # conventions on sampling and interpolation.\n    equi_img = np.concatenate([equi_img, equi_img[:, 0:1, :]], axis=1)\n\n    # Ensure that the coordinates are within the image. Note that the cropping in j is more generous\n    # because of the wrap-around pixel.\n    i = np.clip(i, 0, H - 1)\n    j = np.clip(j, 0, W)\n\n    # Stack the coordinates\n    ij = np.stack((i, j), axis=0)\n\n    # Sample pixels from the specified coordinates to obtain the perspective image\n    sampled_perspective = flexible_inputs_warp(\n        input_image=equi_img, inverse_map=ij, interpolation_order=warp_order\n    )\n\n    if oversample_factor &gt; 1:\n        sampled_perspective = downscale_local_mean(\n            sampled_perspective, (oversample_factor, oversample_factor, 1)\n        )\n\n    # If only the resampled image is needed, return it\n    if not return_mask:\n        return sampled_perspective\n    # Otherwise return a mask of the pixels which were sampled\n\n    # Also save a mask of the pixels being sampled.\n    mask = np.zeros((equi_img.shape[0], equi_img.shape[1]), dtype=bool)\n    # Set pixels which were sampled to True. Out of bound errors are not possible because the values\n    # have been clipped to the appropriate range.\n    mask[np.round(i).astype(int), np.round(j).astype(int)] = True\n\n    # In some cases the right edge might have been sampled, so report this as a sampled pixel on the\n    # left edge\n    mask[:, 0] = np.logical_or(mask[:, 0], mask[:, -1])\n    # Remove the right edge pixels\n    mask = mask[:, :-1]\n\n    return sampled_perspective, mask\n</code></pre>"},{"location":"API_reference/utils/indexing/","title":"Indexing","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing","title":"<code>indexing</code>","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing-functions","title":"Functions","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing.find_argmax_nonzero_value","title":"<code>find_argmax_nonzero_value(array, keepdims=False, axis=1)</code>","text":"<p>Find the argmax of an array, setting entires with zero sum or finite values to nan</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The input array</p> required <code>keepdims</code> <code>bool</code> <p>Should the dimensions be kept. Defaults to False.</p> <code>False</code> <code>axis</code> <code>int</code> <p>Which axis to perform the argmax along. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>array</code> <p>np.array: The argmax, with nans for invalid or all-zero entries</p> Source code in <code>geograypher/utils/indexing.py</code> <pre><code>def find_argmax_nonzero_value(\n    array: np.ndarray, keepdims: bool = False, axis: int = 1\n) -&gt; np.array:\n    \"\"\"Find the argmax of an array, setting entires with zero sum or finite values to nan\n\n    Args:\n        array (np.ndarray): The input array\n        keepdims (bool, optional): Should the dimensions be kept. Defaults to False.\n        axis (int, optional): Which axis to perform the argmax along. Defaults to 1.\n\n    Returns:\n        np.array: The argmax, with nans for invalid or all-zero entries\n    \"\"\"\n    # Find the column with the highest value per row\n    argmax = np.argmax(array, axis=axis, keepdims=keepdims).astype(float)\n\n    # Find rows with zero sum or any infinite values\n    zero_sum_mask = np.sum(array, axis=axis) == 0\n    infinite_mask = np.any(~np.isfinite(array), axis=axis)\n\n    # Set these rows in the argmax to nan\n    argmax[np.logical_or(zero_sum_mask, infinite_mask)] = np.nan\n\n    return argmax\n</code></pre>"},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing.inverse_map_interpolation","title":"<code>inverse_map_interpolation(ijmap, downsample=1, fill=-1)</code>","text":"<p>Inverts the type of map that can be fed to skimage.transform.warp.</p> <p>The basic construction is the pixel position of these maps is the position in the destination image, and the value of the map is the position in the source image. Therefore if location [20, 30] has value [22.2, 28.4], it means that the destination image pixel [20, 30] will be sampled from the source image at pixel [22.2, 28.4] (the sampler can choose to snap to the closest integer value or interpolate nearby pixels).</p> <p>By inverting, we seek to reverse this map through interpolation. In the example above, in the output we would now have the location [22, 28] have the value [20, 30] (or slightly different because it might interpolate with nearby values).</p> <p>Parameters:</p> Name Type Description Default <code>ijmap</code> <code>(2, H, W) numpy array</code> <p>a map of the structure discussed above</p> required <code>downsample</code> <code>int</code> <p>Inverting (particularly griddata) can be expensive for high-res image maps. You can downsample in integer steps to save computation, which will downsample both the (i, j) axes. Using downsample=2 will therefore interpolate on 1/4 of the pixels.</p> <code>1</code> <code>fill</code> <code>int</code> <p>Values outside of the interpolation convex hull will take this value.</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(2, H, W) numpy array of the same shape as ijmap</p> Source code in <code>geograypher/utils/indexing.py</code> <pre><code>def inverse_map_interpolation(\n    ijmap: np.ndarray, downsample: int = 1, fill: int = -1\n) -&gt; np.ndarray:\n    \"\"\"\n    Inverts the type of map that can be fed to skimage.transform.warp.\n\n    The basic construction is the pixel position of these maps is the position\n    in the destination image, and the value of the map is the position in the\n    source image. Therefore if location [20, 30] has value [22.2, 28.4], it\n    means that the destination image pixel [20, 30] will be sampled from the\n    source image at pixel [22.2, 28.4] (the sampler can choose to snap to the\n    closest integer value or interpolate nearby pixels).\n\n    By inverting, we seek to reverse this map through interpolation. In the\n    example above, in the output we would now have the location [22, 28]\n    have the value [20, 30] (or slightly different because it might\n    interpolate with nearby values).\n\n    Arguments:\n        ijmap ((2, H, W) numpy array): a map of the structure discussed above\n        downsample (int): Inverting (particularly griddata) can be expensive\n            for high-res image maps. You can downsample in integer steps to\n            save computation, which will downsample both the (i, j) axes. Using\n            downsample=2 will therefore interpolate on 1/4 of the pixels.\n        fill (int): Values outside of the interpolation convex hull will take\n            this value.\n\n    Returns:\n        (2, H, W) numpy array of the same shape as ijmap\n    \"\"\"\n\n    # (row, col) grid of pixels coordinates\n    H, W = ijmap.shape[1:]\n    igrid, jgrid = np.meshgrid(np.arange(H), np.arange(W), indexing=\"ij\")\n\n    # Get an (N, 2) array of the grid coordinates\n    grid_coords = np.stack([igrid.ravel(), jgrid.ravel()], axis=1)\n\n    # Get (N, 2) arrays of the source and goal coordinates we have data for\n    if downsample &gt; 1:\n        ds = slice(None, None, downsample)\n        sample_y = np.stack([igrid[ds, ds].ravel(), jgrid[ds, ds].ravel()], axis=1)\n        sample_x = np.stack(\n            [ijmap[0][ds, ds].ravel(), ijmap[1][ds, ds].ravel()], axis=1\n        )\n    else:\n        sample_y = grid_coords.copy()\n        sample_x = np.stack([ijmap[0].ravel(), ijmap[1].ravel()], axis=1)\n\n    # This is a little complicated, but griddata takes in\n    # 1) The samples you have data for (x)\n    # 2) The sample data (y)\n    # 3) The new x at which you want to resample\n    # In this case our sample x data is the mapped indices, the sample y data\n    # is the grid that mapping came from, and the resample x is also the grid\n    # because we are trying to invert.\n    inv_i = griddata(\n        sample_x, sample_y[:, 0], grid_coords, method=\"linear\", fill_value=fill\n    )\n    inv_j = griddata(\n        sample_x, sample_y[:, 1], grid_coords, method=\"linear\", fill_value=fill\n    )\n\n    return np.stack([inv_i.reshape(H, W), inv_j.reshape(H, W)], axis=0)\n</code></pre>"},{"location":"API_reference/utils/io/","title":"Io","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io","title":"<code>io</code>","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io-functions","title":"Functions","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io.read_image_or_numpy","title":"<code>read_image_or_numpy(filename)</code>","text":"<p>Read in a file that's either an image or numpy array</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>PATH_TYPE</code> <p>Filename to be read</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If file cannot be read</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Image, in format present on disk</p> Source code in <code>geograypher/utils/io.py</code> <pre><code>def read_image_or_numpy(filename: PATH_TYPE) -&gt; np.ndarray:\n    \"\"\"Read in a file that's either an image or numpy array\n\n    Args:\n        filename (PATH_TYPE): Filename to be read\n\n    Raises:\n        ValueError: If file cannot be read\n\n    Returns:\n        np.ndarray: Image, in format present on disk\n    \"\"\"\n    img = None\n    if img is None:\n        try:\n            img = imread(filename)\n        except:\n            print(\"Couldn't read as image\")\n\n    if img is None:\n        try:\n            img = np.load(filename)\n        except:\n            print(\"couldn't read as numpy\")\n\n    if img is None:\n        raise ValueError(\"Could not read image\")\n\n    return img\n</code></pre>"},{"location":"API_reference/utils/numeric/","title":"Numeric","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric","title":"<code>numeric</code>","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric-functions","title":"Functions","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.calc_communities","title":"<code>calc_communities(starts, ends, edge_weights, louvain_resolution=1.0, out_dir=None, transform_to_epsg_4978=None)</code>","text":"<p>Build a networkx graph from adjacency information. Each node represents a detection while the edges represent the quality of the matches between detections.</p> <p>Parameters:</p> Name Type Description Default <code>starts</code> <code>ndarray</code> <p>(N, 3) array of ray start points</p> required <code>ends</code> <code>ndarray</code> <p>(N, 3) array of ray end points</p> required <code>edge_weights</code> <code>List[Tuple[int, int, Dict[str, float]]]</code> <p>List of edges defining the graph connectivity. Each edge is (start_idx, end_idx, weight_dict) where weight_dict contains the edge weight information</p> required <code>louvain_resolution</code> <code>float</code> <p>Resolution hyperparameter for the Louvain community detection algorithm. Higher values lead to more communities</p> <code>1.0</code> <code>out_dir</code> <code>PATH_TYPE</code> <p>Directory to save the output NPZ file containing community information. If None, results are returned as a dictionary</p> <code>None</code> <code>transform_to_epsg_4978</code> <code>ndarray</code> <p>4x4 transformation matrix to convert points from local coordinates to EPSG:4978 (Earth-centered Earth-fixed)</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Path, Dict[str, ndarray]]</code> <p>Union[Path, Dict[str, np.ndarray]]: If out_dir is provided, returns the path to the NPZ file containing the community information. Otherwise returns a dictionary with keys: - 'ray_IDs': (N,) array mapping each ray to its community ID - 'community_points': (M, 3) array of 3D points representing each community - 'community_points_latlon': (M, 3) array of lat/lon points (only if    transform_to_epsg_4978 is provided)</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def calc_communities(\n    starts: np.ndarray,\n    ends: np.ndarray,\n    edge_weights: typing.List[typing.Tuple[int, int, typing.Dict[str, float]]],\n    louvain_resolution: float = 1.0,\n    out_dir: typing.Optional[PATH_TYPE] = None,\n    transform_to_epsg_4978: typing.Optional[np.ndarray] = None,\n) -&gt; typing.Union[Path, typing.Dict[str, np.ndarray]]:\n    \"\"\"\n    Build a networkx graph from adjacency information. Each node represents\n    a detection while the edges represent the quality of the matches between detections.\n\n    Args:\n        starts (np.ndarray): (N, 3) array of ray start points\n        ends (np.ndarray): (N, 3) array of ray end points\n        edge_weights (List[Tuple[int, int, Dict[str, float]]]): List of edges defining\n            the graph connectivity. Each edge is (start_idx, end_idx, weight_dict) where\n            weight_dict contains the edge weight information\n        louvain_resolution (float): Resolution hyperparameter for the Louvain community\n            detection algorithm. Higher values lead to more communities\n        out_dir (PATH_TYPE, optional): Directory to save the output NPZ file containing\n            community information. If None, results are returned as a dictionary\n        transform_to_epsg_4978 (np.ndarray, optional): 4x4 transformation matrix to\n            convert points from local coordinates to EPSG:4978 (Earth-centered Earth-fixed)\n\n    Returns:\n        Union[Path, Dict[str, np.ndarray]]: If out_dir is provided, returns the path to\n            the NPZ file containing the community information. Otherwise returns a\n            dictionary with keys:\n            - 'ray_IDs': (N,) array mapping each ray to its community ID\n            - 'community_points': (M, 3) array of 3D points representing each community\n            - 'community_points_latlon': (M, 3) array of lat/lon points (only if\n               transform_to_epsg_4978 is provided)\n    \"\"\"\n\n    # Build up the basic graph from edge weights\n    graph = networkx.Graph(edge_weights)\n\n    # Check that the graph is non empty\n    if len(graph) &gt; 0:\n\n        # Determine Louvain communities which are sets of nodes. Ideally, each\n        # community represents a set of detections that correspond to one 3D object\n        communities = networkx.community.louvain_communities(\n            graph, weight=\"weight\", resolution=louvain_resolution\n        )\n        # Sort the communities by size\n        communities = sorted(communities, key=len, reverse=True)\n\n        # Triangulate the rays for each community to identify the 3D location\n        community_points = []\n        # Record the community IDs per ray\n        num_rays = starts.shape[0]\n        ray_IDs = np.full(num_rays, fill_value=np.nan)\n        # Iterate over communities\n        for community_ID, community in enumerate(\n            tqdm(communities, desc=\"Build community points\")\n        ):\n            # Get the ray indices that belong to that community\n            community_indices = np.array(list(community))\n            # Record the community ID for the corresponding rays\n            ray_IDs[community_indices] = community_ID\n            # Use the average of the closest points between rays as the\n            # representative point for the community\n            community_points.append(\n                intersection_average(\n                    starts=starts[community_indices],\n                    ends=ends[community_indices],\n                )\n            )\n\n        # Stack all of the points into one vector\n        community_points = np.vstack(community_points)\n\n        result = {\n            \"ray_IDs\": ray_IDs,\n            \"community_points\": community_points,\n        }\n\n        if transform_to_epsg_4978 is not None:\n            # Append a column of all ones to make the homogenous coordinates\n            homogenous = np.concatenate(\n                [community_points, np.ones_like(community_points[:, 0:1])],\n                axis=1,\n            )\n            # Use the transform matrix to transform the points into the earth\n            # centered, earth fixed frame (EPSG:4978)\n            community_points_epsg_4978 = (transform_to_epsg_4978 @ homogenous.T).T\n            # Convert the points from earth centered, earth fixed frame to lat lon\n            community_points_lat_lon = convert_CRS_3D_points(\n                community_points_epsg_4978,\n                input_CRS=EARTH_CENTERED_EARTH_FIXED_CRS,\n                output_CRS=LAT_LON_CRS,\n            )\n            result[\"community_points_latlon\"] = community_points_lat_lon\n\n    else:\n        # Handle the empty case\n        result = {\n            \"ray_IDs\": np.zeros((0,), dtype=int),\n            \"community_points\": np.zeros((0, 3)),\n        }\n        if transform_to_epsg_4978 is not None:\n            result[\"community_points_latlon\"] = np.zeros((0, 3))\n\n    if out_dir is not None:\n        path = Path(out_dir) / \"communities.npz\"\n        np.savez(path, **result)\n        return path\n    else:\n        return result\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.calc_graph_weights","title":"<code>calc_graph_weights(starts, ends, ray_IDs, similarity_threshold, out_dir=None, min_dist=1e-06, step=5000, transform=None)</code>","text":"<p>This function processes sets of ray segments to build a graph where edges represent ray intersections. The weight of each edge is inversely proportional to the intersection distance between rays. For memory efficiency with large numbers of segments, the computation is done in chunks.</p> <p>Parameters:</p> Name Type Description Default <code>starts</code> <code>ndarray</code> <p>(N, 3) array of ray start points</p> required <code>ends</code> <code>ndarray</code> <p>(N, 3) array of ray end points</p> required <code>ray_IDs</code> <code>ndarray</code> <p>(N,) array of integers identifying which image each ray comes from</p> required <code>similarity_threshold</code> <code>float</code> <p>Maximum intersection distance to consider when creating graph edges. Greater distances will be dropped.</p> required <code>out_dir</code> <code>PATH_TYPE</code> <p>Directory to save the output JSON file containing edge information. If None, no file is saved and a list of edge weights is returned instead.</p> <code>None</code> <code>min_dist</code> <code>float</code> <p>Minimum intersection distance to allow, used to avoid division by zero when calculating weights. Defaults to 1e-6</p> <code>1e-06</code> <code>step</code> <code>int</code> <p>Number of rays to process in each chunk to manage memory usage. Defaults to 5000</p> <code>5000</code> <code>transform</code> <code>callable</code> <p>Function to apply to distances before inversion for weight calculation. For example, if you want graph weights to be distance^3, use lambda x: x**3. Defaults to None</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Path, List[Tuple]]</code> <p>Union[Path, List[Tuple]]: If out_dir is provided, returns the path to the JSON</p> <code>Union[Path, List[Tuple]]</code> <p>file containing the edge information. Otherwise returns a list of tuples, each</p> <code>Union[Path, List[Tuple]]</code> <p>containing (start_idx, end_idx, weight_dict) where weight_dict contains the</p> <code>Union[Path, List[Tuple]]</code> <p>edge weight information</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def calc_graph_weights(\n    starts: np.ndarray,\n    ends: np.ndarray,\n    ray_IDs: np.ndarray,\n    similarity_threshold: float,\n    out_dir: typing.Optional[PATH_TYPE] = None,\n    min_dist: float = 1e-6,\n    step: int = 5000,\n    transform: typing.Optional[typing.Callable[[np.ndarray], np.ndarray]] = None,\n) -&gt; typing.Union[Path, typing.List[typing.Tuple]]:\n    \"\"\"\n    This function processes sets of ray segments to build a graph where edges represent\n    ray intersections. The weight of each edge is inversely proportional to the\n    intersection distance between rays. For memory efficiency with large numbers of\n    segments, the computation is done in chunks.\n\n    Args:\n        starts (np.ndarray): (N, 3) array of ray start points\n        ends (np.ndarray): (N, 3) array of ray end points\n        ray_IDs (np.ndarray): (N,) array of integers identifying which image\n            each ray comes from\n        similarity_threshold (float): Maximum intersection distance to consider when\n            creating graph edges. Greater distances will be dropped.\n        out_dir (PATH_TYPE, optional): Directory to save the output JSON file containing\n            edge information. If None, no file is saved and a list of edge weights\n            is returned instead.\n        min_dist (float, optional): Minimum intersection distance to allow, used to\n            avoid division by zero when calculating weights. Defaults to 1e-6\n        step (int, optional): Number of rays to process in each chunk to manage memory\n            usage. Defaults to 5000\n        transform (callable, optional): Function to apply to distances before inversion\n            for weight calculation. For example, if you want graph weights to be\n            distance^3, use lambda x: x**3. Defaults to None\n\n    Returns:\n        Union[Path, List[Tuple]]: If out_dir is provided, returns the path to the JSON\n        file containing the edge information. Otherwise returns a list of tuples, each\n        containing (start_idx, end_idx, weight_dict) where weight_dict contains the\n        edge weight information\n    \"\"\"\n\n    # Calculate and filter intersection distances\n    # For memory reasons, we need to iterate over blocks. When the number of segments starts\n    # getting very large, the matrices for calculating ray intersections take a great\n    # deal of RAM\n    edge_weights = []\n    num_steps = np.ceil(len(starts) / step)\n    total = int(num_steps * (num_steps + 1) / 2)\n    for islice, jslice, diagonal in tqdm(\n        chunk_slices(N=len(starts), step=step),\n        total=total,\n        desc=\"Calculating graph weights\",\n    ):\n        _, _, dist = compute_approximate_ray_intersections(\n            a0=starts[islice],\n            a1=ends[islice],\n            b0=starts[jslice],\n            b1=ends[jslice],\n            clamp=True,\n        )\n        if diagonal:\n            np.fill_diagonal(dist, np.nan)\n        dist[dist &gt; similarity_threshold] = np.nan\n        dist[dist &lt; min_dist] = min_dist\n\n        # Apply transform if provided\n        if transform is not None:\n            dist = transform(dist)\n\n        # Create edge weights for valid intersections\n        edge_weights.extend(format_graph_edges(islice, jslice, dist, ray_IDs))\n\n    if out_dir is None:\n        return edge_weights\n    else:\n        path = Path(out_dir) / \"edge_weights.json\"\n        with path.open(\"w\") as file:\n            json.dump(edge_weights, file)\n        return path\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.chunk_slices","title":"<code>chunk_slices(N, step)</code>","text":"<p>Yield slices for (step, step) chunks of the upper triangular area of an (N, N) square matrix.</p> <p>For example, if N=5 and step=2, the slices would grab:     1 1 2 2 3     1 1 2 2 3     - - 4 4 5     - - 4 4 5     - - - - 6 And in the (1, 4, 6) cases, is_diag would be True</p> <p>Yields:</p> Type Description <code>slice</code> <p>Each yielded value is (islice, jslice, is_diag), where:</p> <code>slice</code> <ul> <li>islice: slice along the first axis</li> </ul> <code>bool</code> <ul> <li>jslice: slice along the second axis</li> </ul> <code>Tuple[slice, slice, bool]</code> <ul> <li>is_diag: True if the chunk is on the diagonal (i == j)</li> </ul> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def chunk_slices(\n    N: int, step: int\n) -&gt; typing.Iterator[typing.Tuple[slice, slice, bool]]:\n    \"\"\"\n    Yield slices for (step, step) chunks of the upper triangular area\n    of an (N, N) square matrix.\n\n    For example, if N=5 and step=2, the slices would grab:\n        1 1 2 2 3\n        1 1 2 2 3\n        - - 4 4 5\n        - - 4 4 5\n        - - - - 6\n    And in the (1, 4, 6) cases, is_diag would be True\n\n    Yields:\n        Each yielded value is (islice, jslice, is_diag), where:\n        - islice: slice along the first axis\n        - jslice: slice along the second axis\n        - is_diag: True if the chunk is on the diagonal (i == j)\n    \"\"\"\n    ranges = range(0, N, step)\n    for i, j in product(ranges, repeat=2):\n        if j &gt;= i:  # upper triangle including diagonal\n            islice = slice(i, min(i + step, N))\n            jslice = slice(j, min(j + step, N))\n            yield islice, jslice, i == j\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.compute_3D_triangle_area_vectorized","title":"<code>compute_3D_triangle_area_vectorized(corners, return_z_proj_area=True)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>corners</code> <code>ndarray</code> <p>(n_faces, n)</p> required <code>return_z_proj_area</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def compute_3D_triangle_area_vectorized(corners: np.ndarray, return_z_proj_area=True):\n    \"\"\"_summary_\n\n    Args:\n        corners (np.ndarray): (n_faces, n)\n        return_z_proj_area (bool, optional): _description_. Defaults to True.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    A, B, C = corners\n    # https://math.stackexchange.com/questions/2152754/calculate-3d-triangle-area-by-determinant\n    u = B - A\n    v = C - A\n\n    # Save for future computation\n    u0v1_min_u1v0 = u[0] * v[1] - u[1] * v[0]\n    area = (\n        1\n        / 2\n        * np.sqrt(\n            np.power(u[1] * v[2] - u[2] * v[1], 2)\n            + np.power(u[2] * v[0] - u[0] * v[2], 2)\n            + np.power(u0v1_min_u1v0, 2)\n        )\n    )\n\n    if return_z_proj_area:\n        area_z_proj = np.abs(u0v1_min_u1v0) / 2\n        return area, area_z_proj\n\n    return area\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.compute_approximate_ray_intersections","title":"<code>compute_approximate_ray_intersections(a0, a1, b0, b1, clamp=False)</code>","text":"<p>Compute closest points and distances between N line segments a0-&gt;a1 and b0-&gt;b1. Returns (N, N, 3), (N, N, 3), (N, N). If clamp is True, then respect the line segment ends. If clamp is False, then use the infinite rays.</p> <p>Based on https://stackoverflow.com/questions/2824478/shortest-distance-between-two-line-segments</p> <p>Parameters:</p> Name Type Description Default <code>a0</code> <code>ndarray</code> <p>Start points of the first segments (N, 3).</p> required <code>a1</code> <code>ndarray</code> <p>End points of the first segments (N, 3).</p> required <code>b0</code> <code>ndarray</code> <p>Start points of the second segments (N, 3).</p> required <code>b1</code> <code>ndarray</code> <p>End points of the second segments (N, 3).</p> required <code>clamp</code> <code>bool</code> <p>If True, the closest points are clamped to the segment endpoints. If False, the closest points may be anywhere along the infinite rays.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pA</code> <code>ndarray</code> <p>Closest point on the A segments (axis 0) compared to each of the B segments (axis 1) (N, N, 3). For example, the closest point between A[5] and B[2] at pA[5, 2]</p> <code>pB</code> <code>ndarray</code> <p>Closest point on the B segments (axis 1) compared to each of the A segments (axis 1) (N, N, 3)</p> <code>dist</code> <code>ndarray</code> <p>The minimum distance between the A (axis 0) and B (axis 1) segments.</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def compute_approximate_ray_intersections(\n    a0: np.ndarray, a1: np.ndarray, b0: np.ndarray, b1: np.ndarray, clamp: bool = False\n) -&gt; typing.Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute closest points and distances between N line segments a0-&gt;a1 and\n    b0-&gt;b1. Returns (N, N, 3), (N, N, 3), (N, N). If clamp is True, then respect\n    the line segment ends. If clamp is False, then use the infinite rays.\n\n    Based on https://stackoverflow.com/questions/2824478/shortest-distance-between-two-line-segments\n\n    Args:\n        a0 (np.ndarray): Start points of the first segments (N, 3).\n        a1 (np.ndarray): End points of the first segments (N, 3).\n        b0 (np.ndarray): Start points of the second segments (N, 3).\n        b1 (np.ndarray): End points of the second segments (N, 3).\n        clamp (bool, optional): If True, the closest points are clamped to the\n            segment endpoints. If False, the closest points may be anywhere\n            along the infinite rays.\n\n    Returns:\n        pA (np.ndarray): Closest point on the A segments (axis 0) compared to\n            each of the B segments (axis 1) (N, N, 3). For example, the closest\n            point between A[5] and B[2] at pA[5, 2]\n        pB (np.ndarray): Closest point on the B segments (axis 1) compared to\n            each of the A segments (axis 1) (N, N, 3)\n        dist (np.ndarray): The minimum distance between the A (axis 0) and B\n            (axis 1) segments.\n    \"\"\"\n\n    # (N, 3) vectors, representing the A and B line segments\n    A = a1 - a0\n    B = b1 - b0\n    # (N,) distances, representing the length of each A and B vector\n    magA = np.linalg.norm(A, axis=1)\n    magB = np.linalg.norm(B, axis=1)\n    # (N, 3) vectors, representing the A and B unit vectors\n    _A = A / magA[:, None]\n    _B = B / magB[:, None]\n\n    # Expand the A vectors to (N, 1, 3) and the B vectors to (1, N, 3) so that\n    # they project together to an (N, N, 3) matrix later\n    a0_exp = a0[:, None, :]\n    b0_exp = b0[None, :, :]\n    _A_exp = _A[:, None, :]\n    _B_exp = _B[None, :, :]\n\n    # (N, N, 3) cross product. The cross product of unit vectors calculates the\n    # area of the parallelogram formed by the unit vectors, a larger value\n    # indicates lines that are more orthogonal\n    cross = np.cross(_A_exp, _B_exp)\n    # (N, N) representing the squared area of the unit vector parallelograms\n    denom = np.linalg.norm(cross, axis=2) ** 2\n\n    # (N, N, 3) matrix, representing the vector from the start of each A\n    # segment to the start of each B segment\n    t = b0_exp - a0_exp\n\n    # (N, N) matrix, calculate the determinant by multiplying all axes (ijk)\n    # and then sum across k, leaving only an ij matrix. The determinant\n    # represents how a given transform scales space\n    detA = np.einsum(\"ijk,ijk-&gt;ij\", np.cross(t, _B_exp), cross)\n    detB = np.einsum(\"ijk,ijk-&gt;ij\", np.cross(t, _A_exp), cross)\n\n    # Make the denom safe for division by replacing 0 values with 1. Later we\n    # check for parallel locations and fix them\n    parallel = denom == 0\n    denom[parallel] = 1\n\n    # t0 and t1 are (N, N) matrices representing how far along A the closest\n    # point is (t0) where 0 is at a0 and 1 is at a1. B and t1 are the same.\n    t0 = detA / denom\n    t1 = detB / denom\n\n    if clamp:\n        # (N, N) matrix where t0 and t1 are clipped to the vector length\n        t0_clamped = np.clip(t0, 0, magA[:, None])\n        t1_clamped = np.clip(t1, 0, magB[None, :])\n\n        # (N, N, 3) matrix where we start at each vector starting point (a0\n        # and b0 expanded) then travel along them to the closest clamped point\n        pA = a0_exp + t0_clamped[:, :, None] * _A_exp\n        pB = b0_exp + t1_clamped[:, :, None] * _B_exp\n\n        # Check whether the original scale values where out of bounds\n        oob_A = (t0 &lt; 0) | (t0 &gt; magA[:, None])\n        oob_B = (t1 &lt; 0) | (t1 &gt; magB[None, :])\n\n        # Broadcast a0 and _A to (N, N, 3), similar for b0/_B\n        a0_bcast = np.broadcast_to(a0[:, None, :], pA.shape)\n        _A_bcast = np.broadcast_to(_A[:, None, :], pA.shape)\n        b0_bcast = np.broadcast_to(b0[None, :, :], pB.shape)\n        _B_bcast = np.broadcast_to(_B[None, :, :], pB.shape)\n\n        # If any of the scale vectors were clipped, we may have to adjust the\n        # closest point on the opposite vector. For example if the closest\n        # point on an A vector was clipped, the corresponding B point may need\n        # to update\n        if np.any(oob_A):\n            # Get the projection of the new A point onto the B vectors\n            dot = np.einsum(\"ijk,ijk-&gt;ij\", pA - b0_bcast, _B_bcast)\n            # Get a new B point matching that projection\n            dot_clipped = np.clip(dot, 0, magB[None, :])\n            pB[oob_A] = b0_bcast[oob_A] + dot_clipped[oob_A, None] * _B_bcast[oob_A]\n        if np.any(oob_B):\n            # Get the projection of the new B point onto the A vectors\n            dot = np.einsum(\"ijk,ijk-&gt;ij\", pB - a0_bcast, _A_bcast)\n            # Get a new A point matching that projection\n            dot_clipped = np.clip(dot, 0, magA[:, None])\n            pA[oob_B] = a0_bcast[oob_B] + dot_clipped[oob_B, None] * _A_bcast[oob_B]\n    else:\n        # If there is no clamping, we use the scale value as-is to get the\n        # closest points between the rays\n        pA = a0_exp + t0[:, :, None] * _A_exp\n        pB = b0_exp + t1[:, :, None] * _B_exp\n\n    # Handle parallel case\n    if np.any(parallel):\n        # Get the (N, N) projection of each point in b0 onto each unit vector\n        # in A. Subtract the (N, 1) projection of each point in a0 along their\n        # unit vector. By subtracting them, we calculating whether b0 falls\n        # \"ahead\" or \"behind\" of a0, along the relevant axis (_A)\n        d0 = np.einsum(\"ij,kj-&gt;ik\", _A, b0) - np.einsum(\"ij,ij-&gt;i\", _A, a0).reshape(\n            -1, 1\n        )\n        if clamp:\n            # Same logic as d0, but with a1 and b1\n            d1 = np.einsum(\"ij,kj-&gt;ik\", _A, b1) - np.einsum(\"ij,ij-&gt;i\", _A, a0).reshape(\n                -1, 1\n            )\n\n            # Check in which of the (N, N) combinations b0 and b1 fall before\n            # a0, forming an (N, N) boolean mask\n            before = (d0 &lt;= 0) &amp; (d1 &lt;= 0) &amp; parallel\n            # Same logic for after\n            after = (d0 &gt;= magA[:, None]) &amp; (d1 &gt;= magA[:, None]) &amp; parallel\n            # If an A and B vector is parallel and partially overlapping,\n            # middle will be true (N, N) boolean mask\n            middle = parallel &amp; ~(before | after)\n\n            if np.any(before):\n                # Set pA to a0 where relevant\n                pA[before] = a0[:, None, :][before]\n                # Set pB to whichever endpoint is closer to a0\n                pB[before] = np.where(\n                    np.abs(d0[before])[:, None] &lt; np.abs(d1[before])[:, None],\n                    b0[None, :, :][before],\n                    b1[None, :, :][before],\n                )\n\n            if np.any(after):\n                # Set pA to a1 where relevant\n                pA[after] = a1[:, None, :][after]\n                # Set pB to whichever endpoint is closer to a1\n                pB[after] = np.where(\n                    np.abs(d0[after])[:, None] &lt; np.abs(d1[after])[:, None],\n                    b0[None, :, :][after],\n                    b1[None, :, :][after],\n                )\n\n            if np.any(middle):\n                # Clip d0 along the A vectors (from 0 to the magnitude of A)\n                t_mid = np.clip(\n                    d0[middle], 0, np.broadcast_to(magA[:, None], d0.shape)[middle]\n                )\n\n                # Get the broadcast components that align with the state we\n                # care about (middle)\n                a0_bcast = np.broadcast_to(a0[:, None, :], pA.shape)\n                _A_bcast = np.broadcast_to(_A[:, None, :], pA.shape)\n                a0_mid = a0_bcast[middle]\n                _A_mid = _A_bcast[middle]\n\n                # Reproject onto the A vectors\n                pA[middle] = a0_mid + t_mid[:, None] * _A_mid\n\n                # Get the vector from the new pA points to b0\n                b0_bcast = np.broadcast_to(b0[None, :, :], pB.shape)\n                a2b = b0_bcast[middle] - pA[middle]\n\n                # Then subtract the component that is along the A vectors, and\n                # that gives us the final pB location\n                alongA = np.einsum(\"ij,ij-&gt;i\", a2b, _A_mid)[:, None] * _A_mid\n                pB[middle] = pA[middle] + (a2b - alongA)\n\n        else:\n            # If we're not clamping, arbitrarily set the parallel \"closest\n            # points\" to b0 and the matching point on A\n            a0_bcast = np.broadcast_to(a0[:, None, :], pA.shape)\n            b0_bcast = np.broadcast_to(b0[None, :, :], pB.shape)\n            _A_bcast = np.broadcast_to(_A[:, None, :], pA.shape)\n            pA[parallel] = (\n                a0_bcast[parallel] + d0[:, :, None][parallel] * _A_bcast[parallel]\n            )\n            pB[parallel] = b0_bcast[parallel]\n\n    # pA and pB are existing (N, N, 3) matrices. In addition, calculate the\n    # (N, N) distances between each pair\n    return pA, pB, np.linalg.norm(pA - pB, axis=2)\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.create_ramped_weighting","title":"<code>create_ramped_weighting(rectangle_shape, ramp_dist_frac)</code>","text":"<p>Create a ramped weighting that is higher toward the center with a max value of 1 at a fraction from the edge</p> <p>Parameters:</p> Name Type Description Default <code>rectangle_shape</code> <code>Tuple[int, int]</code> <p>Size of rectangle to create a mask for</p> required <code>ramp_dist_frac</code> <code>float</code> <p>Portions at least this far from an edge will have full weight</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array representing the weighting from 0-1</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def create_ramped_weighting(\n    rectangle_shape: typing.Tuple[int, int], ramp_dist_frac: float\n) -&gt; np.ndarray:\n    \"\"\"Create a ramped weighting that is higher toward the center with a max value of 1 at a fraction from the edge\n\n    Args:\n        rectangle_shape (typing.Tuple[int, int]): Size of rectangle to create a mask for\n        ramp_dist_frac (float): Portions at least this far from an edge will have full weight\n\n    Returns:\n        np.ndarray: An array representing the weighting from 0-1\n    \"\"\"\n    i_ramp = np.clip(np.linspace(0, 1 / ramp_dist_frac, num=rectangle_shape[0]), 0, 1)\n    j_ramp = np.clip(np.linspace(0, 1 / ramp_dist_frac, num=rectangle_shape[1]), 0, 1)\n\n    i_ramp = np.minimum(i_ramp, np.flip(i_ramp))\n    j_ramp = np.minimum(j_ramp, np.flip(j_ramp))\n\n    i_ramp = np.expand_dims(i_ramp, 1)\n    j_ramp = np.expand_dims(j_ramp, 0)\n\n    ramped_weighting = np.minimum(i_ramp, j_ramp)\n    return ramped_weighting\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.fair_mode_non_nan","title":"<code>fair_mode_non_nan(values)</code>","text":"<p>Compute the most common value per row in an array of integers and nans. This behaves similarly to scipy.stats.mode(values, axis=1, nan_policy=\"omit\") except that for values with equal counts, one is chosen randomly, rather than taking the lower value.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>(n, m) The input values (float-typed), consisting of integers and nans</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (n,) the most common value per row</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def fair_mode_non_nan(values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute the most common value per row in an array of integers and nans. This behaves similarly to\n    scipy.stats.mode(values, axis=1, nan_policy=\"omit\") except that for values with equal counts,\n    one is chosen randomly, rather than taking the lower value.\n\n    Args:\n        values (np.ndarray): (n, m) The input values (float-typed), consisting of integers and nans\n\n    Returns:\n        np.ndarray: (n,) the most common value per row\n    \"\"\"\n    max_val = np.nanmax(values)\n    # All input values are nan, return all nans\n    if np.isnan(max_val):\n        return np.full((values.shape[0],), fill_value=np.nan)\n\n    max_val = int(max_val)\n    # TODO consider using unique if these indices are sparse\n    counts_per_value_per_row = np.array(\n        [np.sum(values == i, axis=1) for i in range(max_val + 1)]\n    ).T\n    # Check which entires had no classes reported and mask them out\n    # TODO consider removing these rows beforehand\n    zeros_mask = np.all(counts_per_value_per_row == 0, axis=1)\n    # We want to fairly tiebreak since np.argmax will always take th first index\n    # This is hard to do in a vectorized way, so we just add a small random value\n    # independently to each element\n    counts_per_value_per_row = (\n        counts_per_value_per_row\n        + np.random.random(counts_per_value_per_row.shape) * 0.5\n    )\n    most_common_value_per_row = np.argmax(counts_per_value_per_row, axis=1).astype(\n        float\n    )\n    # Set any faces with zero counts to nan\n    most_common_value_per_row[zeros_mask] = np.nan\n    return most_common_value_per_row\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.format_graph_edges","title":"<code>format_graph_edges(islice, jslice, dist, ray_IDs)</code>","text":"<p>This function generates edge definitions for a graph where nodes represent rays and edges represent valid intersections between rays. It applies three filtering criteria: 1. Only uses edges where (i, j) is finite (not NaN) 2. Only uses edges where i &lt; j (keeps it upper triangular) 3. Only uses edges between rays from different images (different ray_IDs)</p> <p>Parameters:</p> Name Type Description Default <code>islice</code> <code>slice</code> <p>Slice indicating the block of rows being processed in the chunked computation</p> required <code>jslice</code> <code>slice</code> <p>Slice indicating the block of columns being processed in the chunked computation</p> required <code>dist</code> <code>ndarray</code> <p>Distance matrix containing distances between rays (chunked) This is the [islice, jslice] section of a larger distance matrix.</p> required <code>ray_IDs</code> <code>ndarray</code> <p>Array of identifiers indicating which image each ray comes from (not chunked)</p> required <p>Returns:</p> Type Description <code>List[Tuple[int, int, Dict[str, float]]]</code> <p>List[Tuple[int, int, Dict[str, float]]]: List of edge definitions, where each edge</p> <code>List[Tuple[int, int, Dict[str, float]]]</code> <p>is a tuple of</p> <code>List[Tuple[int, int, Dict[str, float]]]</code> <ul> <li>i index in the adjacency matrix</li> </ul> <code>List[Tuple[int, int, Dict[str, float]]]</code> <ul> <li>j index in the adjacency matrix</li> </ul> <code>List[Tuple[int, int, Dict[str, float]]]</code> <ul> <li>weight_dict of the form {\"weight\": value}</li> </ul> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def format_graph_edges(\n    islice: slice,\n    jslice: slice,\n    dist: np.ndarray,\n    ray_IDs: np.ndarray,\n) -&gt; typing.List[typing.Tuple[int, int, typing.Dict[str, float]]]:\n    \"\"\"\n    This function generates edge definitions for a graph where nodes represent rays and\n    edges represent valid intersections between rays. It applies three filtering criteria:\n    1. Only uses edges where (i, j) is finite (not NaN)\n    2. Only uses edges where i &lt; j (keeps it upper triangular)\n    3. Only uses edges between rays from different images (different ray_IDs)\n\n    Args:\n        islice (slice): Slice indicating the block of rows being processed in the\n            chunked computation\n        jslice (slice): Slice indicating the block of columns being processed in\n            the chunked computation\n        dist (np.ndarray): Distance matrix containing distances between rays (chunked)\n            This is the [islice, jslice] section of a larger distance matrix.\n        ray_IDs (np.ndarray): Array of identifiers indicating which image each ray comes\n            from (not chunked)\n\n    Returns:\n        List[Tuple[int, int, Dict[str, float]]]: List of edge definitions, where each edge\n        is a tuple of\n        - i index in the adjacency matrix\n        - j index in the adjacency matrix\n        - weight_dict of the form {\"weight\": value}\n    \"\"\"\n\n    # The places where the array is finite are the valid graph distances\n    i_inds, j_inds = np.where(np.isfinite(dist))\n\n    # Pre-calculate the inverse distance\n    weights = 1 / dist\n\n    return [\n        (\n            int(i) + islice.start,\n            int(j) + jslice.start,\n            {\"weight\": float(weights[i, j])},\n        )\n        for i, j in zip(i_inds, j_inds)\n        if (i + islice.start &lt; j + jslice.start)\n        and (ray_IDs[i + islice.start] != ray_IDs[j + jslice.start])\n    ]\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.intersection_average","title":"<code>intersection_average(starts, ends)</code>","text":"<p>Given arrays of line segment start and end points, compute the average of the closest intersection points between all pairs of segments.</p> <p>Parameters:</p> Name Type Description Default <code>starts</code> <code>ndarray</code> <p>(N, 3) array of segment start points</p> required <code>ends</code> <code>ndarray</code> <p>(N, 3) array of segment end points</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (3,) array, the average intersection point</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def intersection_average(starts: np.ndarray, ends: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Given arrays of line segment start and end points, compute the average of the closest\n    intersection points between all pairs of segments.\n\n    Args:\n        starts (np.ndarray): (N, 3) array of segment start points\n        ends (np.ndarray): (N, 3) array of segment end points\n\n    Returns:\n        np.ndarray: (3,) array, the average intersection point\n    \"\"\"\n    closest_points = []\n    pA, pB, _ = compute_approximate_ray_intersections(\n        a0=starts, a1=ends, b0=starts, b1=ends, clamp=True\n    )\n    mask = ~np.eye(starts.shape[0], dtype=bool)\n    return np.mean(np.vstack([pA[mask], pB[mask]]), axis=0)\n</code></pre>"},{"location":"API_reference/utils/parsing/","title":"Parsing","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing","title":"<code>parsing</code>","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing-functions","title":"Functions","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing.make_4x4_transform","title":"<code>make_4x4_transform(rotation_str, translation_str, scale_str='1')</code>","text":"<p>Convenience function to make a 4x4 matrix from the string format used by Metashape</p> <p>Parameters:</p> Name Type Description Default <code>rotation_str</code> <code>str</code> <p>Row major with 9 entries</p> required <code>translation_str</code> <code>str</code> <p>3 entries</p> required <code>scale_str</code> <code>str</code> <p>single value. Defaults to \"1\".</p> <code>'1'</code> <p>Returns:</p> Type Description <p>np.ndarray: (4, 4) A homogenous transform mapping from cam to world</p> Source code in <code>geograypher/utils/parsing.py</code> <pre><code>def make_4x4_transform(rotation_str: str, translation_str: str, scale_str: str = \"1\"):\n    \"\"\"Convenience function to make a 4x4 matrix from the string format used by Metashape\n\n    Args:\n        rotation_str (str): Row major with 9 entries\n        translation_str (str): 3 entries\n        scale_str (str, optional): single value. Defaults to \"1\".\n\n    Returns:\n        np.ndarray: (4, 4) A homogenous transform mapping from cam to world\n    \"\"\"\n    rotation_np = np.fromstring(rotation_str, sep=\" \")\n    rotation_np = np.reshape(rotation_np, (3, 3))\n\n    if not np.isclose(np.linalg.det(rotation_np), 1.0, atol=1e-8, rtol=0):\n        raise ValueError(\n            f\"Inproper rotation matrix with determinant {np.linalg.det(rotation_np)}\"\n        )\n\n    translation_np = np.fromstring(translation_str, sep=\" \")\n    scale = float(scale_str)\n    transform = np.eye(4)\n    transform[:3, :3] = rotation_np * scale\n    transform[:3, 3] = translation_np\n    return transform\n</code></pre>"},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing.parse_metashape_mesh_metadata","title":"<code>parse_metashape_mesh_metadata(mesh_metadata_file)</code>","text":"<p>Parse the metadata file which is produced by Metashape when exporting a mesh to determine the coordinate reference frame and origin shift to use when interpreting the mesh.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_metadata_file</code> <code>Union[str, Path]</code> <p>Path to the metadata XML file.</p> required <p>Returns:</p> Type Description <code>Union[CRS, None]</code> <p>Union[pyproj.CRS, None]: The CRS to interpret the vertices (after the shift). If not present, None is returned.</p> <code>Union[ndarray, None]</code> <p>Union[np.ndarray, None]: The shift to be added to the mesh vertices. If not present, None is returned.</p> Source code in <code>geograypher/utils/parsing.py</code> <pre><code>def parse_metashape_mesh_metadata(\n    mesh_metadata_file: typing.Union[str, Path],\n) -&gt; typing.Tuple[typing.Union[pyproj.CRS, None], typing.Union[np.ndarray, None]]:\n    \"\"\"\n    Parse the metadata file which is produced by Metashape when exporting a mesh to determine the\n    coordinate reference frame and origin shift to use when interpreting the mesh.\n\n\n    Args:\n        mesh_metadata_file (typing.Union[str, Path]): Path to the metadata XML file.\n\n    Returns:\n        Union[pyproj.CRS, None]:\n            The CRS to interpret the vertices (after the shift). If not present, None is returned.\n        Union[np.ndarray, None]:\n            The shift to be added to the mesh vertices. If not present, None is returned.\n    \"\"\"\n    tree = ET.parse(mesh_metadata_file)\n    root = tree.getroot()\n\n    # Parse CRS and shift\n    CRS = root.find(\"SRS\")\n    shift = root.find(\"SRSOrigin\")\n\n    # If CRS is present, convert it to a pyproj type\n    if CRS is not None:\n        CRS = pyproj.CRS(CRS.text)\n\n    # If the shift is present, convert to a numpy array\n    if shift is not None:\n        shift = np.array(shift.text.split(\",\"), dtype=float)\n    return CRS, shift\n</code></pre>"},{"location":"API_reference/utils/prediction_metrics/","title":"Prediction metrics","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics","title":"<code>prediction_metrics</code>","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics-functions","title":"Functions","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics.compute_and_show_cf","title":"<code>compute_and_show_cf(pred_labels, gt_labels, labels=None, use_labels_from='both', vis=True, cf_plot_savefile=None, cf_np_savefile=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>pred_labels</code> <code>list</code> <p>description</p> required <code>gt_labels</code> <code>list</code> <p>description</p> required <code>labels</code> <code>Union[None, List[str]]</code> <p>description. Defaults to None.</p> <code>None</code> <code>use_labels_from</code> <code>str</code> <p>description. Defaults to \"both\".</p> <code>'both'</code> <code>vis</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>cf_plot_savefile</code> <code>Union[None, PATH_TYPE]</code> <p>description. Defaults to None.</p> <code>None</code> <code>cf_np_savefile</code> <code>Union[None, PATH_TYPE]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>geograypher/utils/prediction_metrics.py</code> <pre><code>def compute_and_show_cf(\n    pred_labels: list,\n    gt_labels: list,\n    labels: typing.Union[None, typing.List[str]] = None,\n    use_labels_from: str = \"both\",\n    vis: bool = True,\n    cf_plot_savefile: typing.Union[None, PATH_TYPE] = None,\n    cf_np_savefile: typing.Union[None, PATH_TYPE] = None,\n):\n    \"\"\"_summary_\n\n    Args:\n        pred_labels (list): _description_\n        gt_labels (list): _description_\n        labels (typing.Union[None, typing.List[str]], optional): _description_. Defaults to None.\n        use_labels_from (str, optional): _description_. Defaults to \"both\".\n        vis (bool, optional): _description_. Defaults to True.\n        cf_plot_savefile (typing.Union[None, PATH_TYPE], optional): _description_. Defaults to None.\n        cf_np_savefile (typing.Union[None, PATH_TYPE], optional): _description_. Defaults to None.\n\n    Raises:\n        ValueError: _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    if labels is None:\n        if use_labels_from == \"gt\":\n            labels = np.unique(list(gt_labels))\n        elif use_labels_from == \"pred\":\n            labels = np.unique(list(pred_labels))\n        elif use_labels_from == \"both\":\n            labels = np.unique(list(pred_labels) + list(gt_labels))\n        else:\n            raise ValueError(\n                f\"Must use labels from gt, pred, or both but instead was {use_labels_from}\"\n            )\n\n    cf_matrix = confusion_matrix(y_true=gt_labels, y_pred=pred_labels, labels=labels)\n\n    if vis:\n        cf_disp = ConfusionMatrixDisplay(\n            confusion_matrix=cf_matrix, display_labels=labels\n        )\n        cf_disp.plot()\n        if cf_plot_savefile is None:\n            plt.show()\n        else:\n            ensure_containing_folder(cf_plot_savefile)\n            plt.savefig(cf_plot_savefile)\n\n    if cf_np_savefile:\n        ensure_containing_folder(cf_np_savefile)\n        np.save(cf_np_savefile, cf_matrix)\n\n    # TODO compute more comprehensive metrics here\n    accuracy = np.sum(cf_matrix * np.eye(cf_matrix.shape[0])) / np.sum(cf_matrix)\n\n    return cf_matrix, labels, accuracy\n</code></pre>"},{"location":"API_reference/utils/visualization/","title":"Visualization","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization","title":"<code>visualization</code>","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization-functions","title":"Functions","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.create_composite","title":"<code>create_composite(RGB_image, label_image, label_blending_weight=0.5, IDs_to_labels=None, grayscale_RGB_overlay=True)</code>","text":"<p>Create a three-panel composite with an RGB image and a label</p> <p>Parameters:</p> Name Type Description Default <code>RGB_image</code> <code>ndarray</code> <p>(h, w, 3) rgb image to be used directly as one panel</p> required <code>label_image</code> <code>ndarray</code> <p>(h, w) image containing either integer labels or float scalars. Will be colormapped prior to display.</p> required <code>label_blending_weight</code> <code>float</code> <p>Opacity for the label in the blended composite. Defaults to 0.5.</p> <code>0.5</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping from integer IDs to string labels. Used to compute colormap. If None, a continous colormap is used. Defaults to None.</p> <code>None</code> <code>grayscale_RGB_overlay</code> <code>bool</code> <p>Convert the RGB image to grayscale in the overlay. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the RGB image cannot be interpreted as such</p> <p>Returns:</p> Type Description <p>np.ndarray: (h, 3*w, 3) horizontally composited image</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def create_composite(\n    RGB_image: np.ndarray,\n    label_image: np.ndarray,\n    label_blending_weight: float = 0.5,\n    IDs_to_labels: typing.Union[None, dict] = None,\n    grayscale_RGB_overlay: bool = True,\n):\n    \"\"\"Create a three-panel composite with an RGB image and a label\n\n    Args:\n        RGB_image (np.ndarray):\n            (h, w, 3) rgb image to be used directly as one panel\n        label_image (np.ndarray):\n            (h, w) image containing either integer labels or float scalars. Will be colormapped\n            prior to display.\n        label_blending_weight (float, optional):\n            Opacity for the label in the blended composite. Defaults to 0.5.\n        IDs_to_labels (typing.Union[None, dict], optional):\n            Mapping from integer IDs to string labels. Used to compute colormap. If None, a\n            continous colormap is used. Defaults to None.\n        grayscale_RGB_overlay (bool):\n            Convert the RGB image to grayscale in the overlay. Default is True.\n\n    Raises:\n        ValueError: If the RGB image cannot be interpreted as such\n\n    Returns:\n        np.ndarray: (h, 3*w, 3) horizontally composited image\n    \"\"\"\n    if RGB_image.ndim != 3 or RGB_image.shape[2] != 3:\n        raise ValueError(\"Invalid RGB error\")\n\n    if RGB_image.dtype == np.uint8:\n        # Rescale to float range and implicitly cast\n        RGB_image = RGB_image / 255\n\n    vis_options = get_vis_options_from_IDs_to_labels(IDs_to_labels)\n    if label_image.dtype == np.uint8 and not vis_options[\"discrete\"]:\n        # Rescale to float range and implicitly cast\n        label_image = label_image / 255\n\n    if not (label_image.ndim == 3 and label_image.shape[2] == 3):\n        # If it's a one channel image make it not have a channel dim\n        label_image = np.squeeze(label_image)\n\n        cmap = plt.get_cmap(vis_options[\"cmap\"])\n        null_mask = np.logical_or(\n            label_image == NULL_TEXTURE_INT_VALUE,\n            np.logical_not(np.isfinite(label_image)),\n        )\n        if not vis_options[\"discrete\"]:\n            # Shift\n            label_image = label_image - np.nanmin(label_image)\n            # Find the max value that's not the null vlaue\n            valid_pixels = label_image[np.logical_not(null_mask)]\n            if valid_pixels.size &gt; 0:\n                # TODO this might have to be changed to nanmax in the future\n                max_value = np.max(valid_pixels)\n                # Scale\n                label_image = label_image / max_value\n\n        # Perform the colormapping\n        label_image = cmap(label_image)[..., :3]\n        # Mask invalid values\n        label_image[null_mask] = 0\n\n    # Create a blended image\n    if grayscale_RGB_overlay:\n        RGB_for_composite = np.tile(\n            np.mean(RGB_image, axis=2, keepdims=True), (1, 1, 3)\n        )\n    else:\n        RGB_for_composite = RGB_image\n    overlay = ((1 - label_blending_weight) * RGB_for_composite) + (\n        label_blending_weight * label_image\n    )\n    # Concatenate the images horizonally\n    composite = np.concatenate((label_image, RGB_image, overlay), axis=1)\n    # Cast to np.uint8 for saving\n    composite = (composite * 255).astype(np.uint8)\n    return composite\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.create_pv_plotter","title":"<code>create_pv_plotter(off_screen, force_xvfb=False, plotter=None)</code>","text":"<p>Create a pyvista plotter while handling offscreen rendering</p> <p>Parameters:</p> Name Type Description Default <code>off_screen</code> <code>bool</code> <p>Whether the plotter should be offscreen</p> required <code>force_xvfb</code> <code>bool</code> <p>Should XVFB be used for rendering by default. Defaults to False.</p> <code>False</code> <code>plotter</code> <code>(None, Plotter)</code> <p>Existing plotter to use, will just return it if not None. Defaults to None</p> <code>None</code> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def create_pv_plotter(\n    off_screen: bool,\n    force_xvfb: bool = False,\n    plotter: typing.Union[None, pv.Plotter] = None,\n):\n    \"\"\"Create a pyvista plotter while handling offscreen rendering\n\n    Args:\n        off_screen (bool):\n            Whether the plotter should be offscreen\n        force_xvfb (bool, optional):\n            Should XVFB be used for rendering by default. Defaults to False.\n        plotter ((None, pv.Plotter), optional):\n            Existing plotter to use, will just return it if not None. Defaults to None\n    \"\"\"\n    # If a valid plotter has not been passed in create one\n    if not isinstance(plotter, pv.Plotter):\n        # Catch the warning that there is not xserver running\n        with warnings.catch_warnings(record=True) as w:\n            # Create the plotter which may be onscreen or off\n            plotter = pv.Plotter(off_screen=off_screen)\n\n        # Start xvfb if requested or the system is not running an xserver\n        if force_xvfb or (len(w) &gt; 0 and \"pyvista.start_xvfb()\" in str(w[0].message)):\n            # Start a headless renderer\n            safe_start_xvfb()\n    return plotter\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.get_vis_options_from_IDs_to_labels","title":"<code>get_vis_options_from_IDs_to_labels(IDs_to_labels, cmap_continous='viridis', cmap_10_classes='tab10', cmap_20_classes='tab20', cmap_many_classes='viridis')</code>","text":"<p>Determine vis options based on a given IDs_to_labels object</p> <p>Parameters:</p> Name Type Description Default <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>description</p> required <code>cmap_continous</code> <code>str</code> <p>Colormap to use if the values are continous. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>cmap_10_classes</code> <code>str</code> <p>Colormap to use if the values are discrete and there are 10 or fewer classes. Defaults to \"tab10\".</p> <code>'tab10'</code> <code>cmap_20_classes</code> <code>str</code> <p>Colormap to use if the values are discrete and there are 11-20 classes. Defaults to \"tab20\".</p> <code>'tab20'</code> <code>cmap_many_classes</code> <code>str</code> <p>Colormap to use if there are more than 20 classes. Defaults to \"viridis\".</p> <code>'viridis'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Containing the cmap, vmin/vmax, and whether the colormap is discrete</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def get_vis_options_from_IDs_to_labels(\n    IDs_to_labels: typing.Union[None, dict],\n    cmap_continous: str = \"viridis\",\n    cmap_10_classes: str = \"tab10\",\n    cmap_20_classes: str = \"tab20\",\n    cmap_many_classes: str = \"viridis\",\n):\n    \"\"\"Determine vis options based on a given IDs_to_labels object\n\n    Args:\n        IDs_to_labels (typing.Union[None, dict]): _description_\n        cmap_continous (str, optional):\n            Colormap to use if the values are continous. Defaults to \"viridis\".\n        cmap_10_classes (str, optional):\n            Colormap to use if the values are discrete and there are 10 or fewer classes. Defaults to \"tab10\".\n        cmap_20_classes (str, optional):\n            Colormap to use if the values are discrete and there are 11-20 classes. Defaults to \"tab20\".\n        cmap_many_classes (str, optional):\n            Colormap to use if there are more than 20 classes. Defaults to \"viridis\".\n\n    Returns:\n        dict: Containing the cmap, vmin/vmax, and whether the colormap is discrete\n    \"\"\"\n    # This could be written in fewer lines of code but I kept it intentionally explicit\n\n    if IDs_to_labels is None:\n        # No IDs_to_labels means it's continous\n        cmap = cmap_continous\n        vmin = None\n        vmax = None\n        discrete = False\n    else:\n        # Otherwise, we can determine the max class ID\n        max_ID = np.max(list(IDs_to_labels.keys()))\n\n        if max_ID &lt; 10:\n            # 10 or fewer discrete classes\n            cmap = cmap_10_classes\n            vmin = -0.5\n            vmax = 9.5\n            discrete = True\n        elif max_ID &lt; 20:\n            # 11-20 discrete classes\n            cmap = cmap_20_classes\n            vmin = -0.5\n            vmax = 19.5\n            discrete = True\n        else:\n            # More than 20 classes. There are no good discrete colormaps for this, so we generally\n            # fall back on displaying it with a continous colormap\n            cmap = cmap_many_classes\n            vmin = None\n            vmax = None\n            discrete = False\n\n    return {\"cmap\": cmap, \"vmin\": vmin, \"vmax\": vmax, \"discrete\": discrete}\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.merge_cylinders","title":"<code>merge_cylinders(starts, ends, community_IDs, cmap, norm)</code>","text":"<p>Create and merge a set of cylinders one by one.</p> <p>Parameters:</p> Name Type Description Default <code>ray_starts</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the starting points of N rays</p> required <code>ray_ends</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the ending points of N rays</p> required <code>community_IDs</code> <code>(N,) np.ndarray</code> <p>The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0 and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</p> required <code>cmap</code> <code>ListedColormap</code> <p>Colormap to use on the normalized community IDs</p> required <code>norm</code> <code>Normalize</code> <p>Normalization function that spans from min to max of the community IDs</p> required <p>Returns: pv.Polydata mesh representing the given rays with cylinders.</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def merge_cylinders(\n    starts: np.ndarray,\n    ends: np.ndarray,\n    community_IDs: np.ndarray,\n    cmap: ListedColormap,\n    norm: Normalize,\n) -&gt; pv.PolyData:\n    \"\"\"\n    Create and merge a set of cylinders one by one.\n\n    Arguments:\n        ray_starts ((N, 3) np.ndarray):\n            The 3D locations of the starting points of N rays\n        ray_ends ((N, 3) np.ndarray):\n            The 3D locations of the ending points of N rays\n        community_IDs ((N,) np.ndarray):\n            The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0\n            and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n        cmap (matplotlib.colors.ListedColormap):\n            Colormap to use on the normalized community IDs\n        norm (matplotlib.colors.Normalize):\n            Normalization function that spans from min to max of the community IDs\n\n    Returns: pv.Polydata mesh representing the given rays with cylinders.\n    \"\"\"\n\n    polydata = None\n    for start, end, community_ID in zip(starts, ends, community_IDs):\n\n        # Build a cylinder\n        center = (start + end) / 2\n        direction = end - start\n        height = np.linalg.norm(direction)\n        if height == 0:\n            continue\n        direction = direction / height\n        # Some of the chosen parameters (low resolution, no capping) are to reduce\n        # polygon faces when dealing with large numbers of cylinders\n        cyl = pv.Cylinder(\n            center=center,\n            direction=direction,\n            radius=0.05,\n            height=height,\n            resolution=3,\n            capping=False,\n        )\n\n        # Color the cylinder\n        color = (np.array(cmap(norm(community_ID)))[:3] * 255).astype(np.uint8)\n        cyl[\"scalars\"] = np.full(cyl.n_points, community_ID)\n        cyl.point_data[\"RGB\"] = np.tile(color, (cyl.n_points, 1))\n\n        # And merge it into the scene\n        if polydata is None:\n            polydata = cyl\n        else:\n            polydata = polydata.merge(cyl)\n\n    return polydata\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.show_segmentation_labels","title":"<code>show_segmentation_labels(label_folder, image_folder, savefolder=None, num_show=10, label_suffix='.png', image_suffix='.JPG', IDs_to_labels=None)</code>","text":"<p>Visualize and optionally save composite images showing segmentation labels overlaid on their corresponding images.</p> <p>Parameters:</p> Name Type Description Default <code>label_folder</code> <code>PATH_TYPE</code> <p>Path to the folder containing label images.</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder containing original images.</p> required <code>savefolder</code> <code>PATH_TYPE</code> <p>If provided, composites are saved here; otherwise, they are displayed using pyplot. Defaults to None.</p> <code>None</code> <code>num_show</code> <code>int</code> <p>Number of samples to show or save. Defaults to 10.</p> <code>10</code> <code>label_suffix</code> <code>str</code> <p>Suffix for label image files. Defaults to \".png\".</p> <code>'.png'</code> <code>image_suffix</code> <code>str</code> <p>Suffix for image files. Defaults to \".JPG\".</p> <code>'.JPG'</code> <code>IDs_to_labels</code> <code>dict</code> <p>Mapping from label IDs to class names. If None, will attempt to load from label_folder. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def show_segmentation_labels(\n    label_folder: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    savefolder: typing.Union[None, PATH_TYPE] = None,\n    num_show: int = 10,\n    label_suffix: str = \".png\",\n    image_suffix: str = \".JPG\",\n    IDs_to_labels: typing.Optional[dict] = None,\n) -&gt; None:\n    \"\"\"\n    Visualize and optionally save composite images showing segmentation labels overlaid\n    on their corresponding images.\n\n    Args:\n        label_folder (PATH_TYPE): Path to the folder containing label images.\n        image_folder (PATH_TYPE): Path to the folder containing original images.\n        savefolder (PATH_TYPE, optional): If provided, composites are saved here;\n            otherwise, they are displayed using pyplot. Defaults to None.\n        num_show (int): Number of samples to show or save. Defaults to 10.\n        label_suffix (str): Suffix for label image files. Defaults to \".png\".\n        image_suffix (str): Suffix for image files. Defaults to \".JPG\".\n        IDs_to_labels (dict, optional): Mapping from label IDs to class names.\n            If None, will attempt to load from label_folder. Defaults to None.\n    \"\"\"\n    # Find all label files in the label_folder and shuffle them\n    rendered_files = list(Path(label_folder).rglob(\"*\" + label_suffix))\n    np.random.shuffle(rendered_files)\n\n    # Ensure the save folder exists if saving output\n    if savefolder is not None:\n        ensure_folder(savefolder)\n\n    # Attempt to load IDs_to_labels from a JSON file if not provided\n    if (\n        IDs_to_labels is None\n        and (IDs_to_labels_file := Path(label_folder, \"IDs_to_labels.json\")).exists()\n    ):\n        with open(IDs_to_labels_file, \"r\") as infile:\n            IDs_to_labels = json.load(infile)\n            IDs_to_labels = {int(k): v for k, v in IDs_to_labels.items()}\n    # Iterate through a subset of label files and create composites\n    for i, rendered_file in tqdm(\n        enumerate(rendered_files[:num_show]),\n        desc=\"Showing segmentation labels\",\n        total=min(len(rendered_files), num_show),\n    ):\n        # Find the corresponding image file by matching the label path to an image path.\n        # Assumes that image_folder and label_folder have the same subdirectory structure\n        # and file stems for corresponding images/labels.\n        image_file = Path(\n            image_folder, rendered_file.relative_to(label_folder)\n        ).with_suffix(image_suffix)\n\n        # Read the image and label data\n        image = imread(image_file)\n        render = read_img_npy(rendered_file)\n        # Create a composite visualization\n        composite = create_composite(image, render, IDs_to_labels=IDs_to_labels)\n\n        if savefolder is None:\n            # Display the composite\n            plt.imshow(composite)\n            plt.show()\n        else:\n            # Save the composite to the output folder\n            output_file = Path(savefolder, f\"rendered_label_{i:03}.png\")\n            imwrite(output_file, composite)\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.visualize_intersections_as_mesh","title":"<code>visualize_intersections_as_mesh(ray_starts, ray_ends, community_IDs, community_points, out_dir, batch=250, cube_side_len=0.2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ray_starts</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the starting points of N rays</p> required <code>ray_ends</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the ending points of N rays</p> required <code>community_IDs</code> <code>(N,) np.ndarray</code> <p>The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0 and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</p> required <code>community_points</code> <code>(M, 3) np.ndarray</code> <p>One 3D point per community, indicating the center of the grouped rays. In the example above, this would be an (2, 3) array with 2 points.</p> required <code>out_dir</code> <code>PATH_TYPE</code> required <code>batch</code> <code>int</code> <p>Defaults to 250.</p> <code>250</code> <code>cube_side_len</code> <code>float</code> <p>Defaults to 0.2</p> <code>0.2</code> Saves <p>out_dir / rays.ply out_dir / points.ply</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def visualize_intersections_as_mesh(\n    ray_starts: np.ndarray,\n    ray_ends: np.ndarray,\n    community_IDs: np.ndarray,\n    community_points: np.ndarray,\n    out_dir: PATH_TYPE,\n    batch: int = 250,\n    cube_side_len: float = 0.2,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n        ray_starts ((N, 3) np.ndarray):\n            The 3D locations of the starting points of N rays\n        ray_ends ((N, 3) np.ndarray):\n            The 3D locations of the ending points of N rays\n        community_IDs ((N,) np.ndarray):\n            The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0\n            and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n        community_points ((M, 3) np.ndarray):\n            One 3D point per community, indicating the center of the grouped rays. In the\n            example above, this would be an (2, 3) array with 2 points.\n        out_dir (PATH_TYPE):\n        batch (int):\n            Defaults to 250.\n        cube_side_len (float):\n            Defaults to 0.2\n\n    Saves:\n        out_dir / rays.ply\n        out_dir / points.ply\n    \"\"\"\n\n    # Short-circuit on empty\n    if len(ray_starts) == 0:\n        return\n\n    # Enforce Path type\n    out_dir = Path(out_dir)\n\n    # Split the communities into an (unforunately limited) color set\n    norm = Normalize(vmin=np.nanmin(community_IDs), vmax=np.nanmax(community_IDs))\n    cmap = colormaps[\"tab20\"]\n\n    # Build up cylinders in batches\n    n_batches = int(np.ceil(len(community_IDs) / batch))\n    cylinder_polydata = None\n    for i in tqdm(range(n_batches), desc=\"Building cylinders\"):\n        islice = slice(i * batch, min((i + 1) * batch, len(community_IDs)))\n        batched = merge_cylinders(\n            starts=ray_starts[islice],\n            ends=ray_ends[islice],\n            community_IDs=community_IDs[islice],\n            cmap=cmap,\n            norm=norm,\n        )\n        # Merge with previous cylinders\n        if cylinder_polydata is None:\n            cylinder_polydata = batched\n        else:\n            cylinder_polydata = cylinder_polydata.merge(batched)\n\n    if cylinder_polydata is not None:\n        path = out_dir / \"rays.ply\"\n        print(f\"Saving visualized cylinders to {path}\")\n        cylinder_polydata.save(path, texture=\"RGB\")\n\n    # The cube merging is much less costly than the cylinders, and thus far hasn't\n    # required batching\n    cube_polydata = None\n    for community_ID, point in enumerate(\n        tqdm(community_points, desc=\"Building points\")\n    ):\n        # Build a cube and set the color\n        cube = pv.Cube(\n            center=point,\n            x_length=cube_side_len,\n            y_length=cube_side_len,\n            z_length=cube_side_len,\n        )\n        color = (np.array(cmap(norm(community_ID)))[:3] * 255).astype(np.uint8)\n        cube.point_data[\"RGB\"] = np.tile(color, (cube.n_points, 1))\n        # Merge with previous cubes\n        if cube_polydata is None:\n            cube_polydata = cube\n        else:\n            cube_polydata = cube_polydata.merge(cube)\n\n    if cube_polydata is not None:\n        path = out_dir / \"points.ply\"\n        print(f\"Saving visualized cubes to {path}\")\n        cube_polydata.save(path, texture=\"RGB\")\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.visualize_intersections_in_pyvista","title":"<code>visualize_intersections_in_pyvista(plotter, ray_starts, ray_ends, community_IDs, community_points)</code>","text":"<p>Visualize the given grouped rays and detected points in a pyvista plotter.</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Existing pyvista plotter to add intersection lines/points to</p> required <code>ray_starts</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the starting points of N rays</p> required <code>ray_ends</code> <code>(N, 3) np.ndarray</code> <p>The 3D locations of the ending points of N rays</p> required <code>community_IDs</code> <code>(N,) np.ndarray</code> <p>The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0 and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]</p> required <code>community_points</code> <code>(M, 3) np.ndarray</code> <p>One 3D point per community, indicating the center of the grouped rays. In the example above, this would be an (2, 3) array with 2 points.</p> required Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def visualize_intersections_in_pyvista(\n    plotter: pv.Plotter,\n    ray_starts: np.ndarray,\n    ray_ends: np.ndarray,\n    community_IDs: np.ndarray,\n    community_points: np.ndarray,\n) -&gt; None:\n    \"\"\"\n    Visualize the given grouped rays and detected points in a pyvista plotter.\n\n    Arguments:\n        plotter (pv.Plotter):\n            Existing pyvista plotter to add intersection lines/points to\n        ray_starts ((N, 3) np.ndarray):\n            The 3D locations of the starting points of N rays\n        ray_ends ((N, 3) np.ndarray):\n            The 3D locations of the ending points of N rays\n        community_IDs ((N,) np.ndarray):\n            The IDs for groups of rays. For example, if there are 10 rays with 5 in group 0\n            and 5 in group 1, this would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n        community_points ((M, 3) np.ndarray):\n            One 3D point per community, indicating the center of the grouped rays. In the\n            example above, this would be an (2, 3) array with 2 points.\n    \"\"\"\n\n    # Interweave the points as line_segments_from_points desires\n    interwoven = np.empty((2 * len(ray_starts), 3), dtype=ray_starts.dtype)\n    interwoven[0::2] = ray_starts\n    interwoven[1::2] = ray_ends\n\n    # Show the line segments\n    lines_mesh = pv.line_segments_from_points(interwoven)\n    plotter.add_mesh(\n        lines_mesh,\n        scalars=community_IDs,\n        label=\"Rays, colored by community ID\",\n    )\n\n    # Show the triangulated communtities as red spheres\n    detected_points = pv.PolyData(community_points)\n    plotter.add_points(\n        detected_points,\n        color=\"r\",\n        render_points_as_spheres=True,\n        point_size=10,\n        label=\"Triangulated locations\",\n    )\n    plotter.add_legend()\n</code></pre>"},{"location":"examples_and_applications/applications/","title":"Projects using geograypher","text":""},{"location":"examples_and_applications/applications/#cross-site-tree-species-classification-for-western-conifers","title":"Cross-site tree species classification for Western Conifers","text":"<p>The goal of this project is to identify the species of western conifers in regions which have been burned by severe fire in the past decade. Data was collected at four sites and consisted of both drone surveys and manual field surveys. This work showed geograypher's multiview workflow enabled 75% prediction accuracy on a leave-on-site-out prediction task. Details can be found in this ArXiv paper.</p>"},{"location":"examples_and_applications/examples/","title":"Examples","text":""},{"location":"examples_and_applications/examples/#projecting-image-based-segmentation-predictions-to-geospatial-coordinates","title":"Projecting image-based segmentation predictions to geospatial coordinates","text":"<p>For tasks like vegetation cover segmentation, it is common to use a semantic segmentation network that produces a class label for each pixel in an image. This workflow shows how to take these per-image predictions and project them to a 3D mesh representation. From there, the most commonly-predicted class is identified for each face of the mesh. Then, this information can be exported in a geospatial format showing the boundaries of each class. This information can also be post-processed to determine the most common class for a pre-determined region, such as a single tree or a management region. This workflow is fairly well-developed and works for a variety of applications.</p> <ul> <li>geograypher/examples/aggregate_predictions.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#projecting-object-detections-to-geospatial-coordinates","title":"Projecting object detections to geospatial coordinates","text":"<p>Other tasks consist of identifying individual objects such as trees or birds. Given object detections in individual images, this workflow can determine the corresponding geospatial boundaries. The quality of this approach is heavily dependent on the quality of the mesh model of the scene. In cases where the scale of the reconstruction errors is significant compared to the size of individual objects, the localization may be poor. An alternative workflow is to triangulate multiple detections from different images to localize the objects without using a mesh. This works in some cases, but often under-predicts the true number of unique objects. Both workflows are highly experimental.</p> <ul> <li>geograypher/examples/project_detections.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#generating-per-image-labels-from-geospatial-ground-truth","title":"Generating per-image labels from geospatial ground truth","text":"<p>The prior examples have assumed that per-image predictions are available. In many practical applications, the user must train their own machine learning model to generate these predictions. Since the predictions are generated on individual raw images, it is important that the labeled data to train the model also consists of individual images, and not another representation such as an orthomosaic. The user can hand-annotate individual images, but this process is laborious and ecological classes (e.g. plant species) cannot always be reliably determined without in-field observations. This workflow takes geospatial ground truth data collected in-field and projects it to the viewpoint of each individual image. These per-pixel labels corresponding to each real image can be used to train a machine learning model.</p> <ul> <li>geograypher/examples/render_labels.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#workflow-using-simulated-data","title":"Workflow using simulated data","text":"<p>All the other examples use real data. To support conceptual understanding and rapid debugging, we developed an end-to-end workflow using only simulated data. The scene consists of various geometric objects arranged on a flat plane. Users can configure the objects in the scene and the locations of the virtual cameras observing them.</p> <ul> <li>geograypher/examples/concept_figure.ipynb</li> </ul>"},{"location":"getting_started/conceptual_workflow/","title":"Conceptual workflow","text":"<p>Imagine you are trying to map the objects in a hypothetical region. Your world consists of three types of objects: cones, cubes, and cylinders. Cones are different shades of blue, cubes are difference shades of orange, and cylinders are different shades of green. Your landscape consists of a variety of these objects arranged randomly on a flat gray surface. You fly a drone survey and collect images of your scene, some of which are shown below.</p> <p> </p> <p>While you are there, you also do some field work and survey a small subset of your region. Field work is labor-intensive, so you can't survey the entire region your drone flew. You note down the class of the object and their location and shape in geospatial coordinates. This results in the following geospatial map.</p> <p> </p> <p>You use structure from motion to build a 3D model of your scene and also estimate the locations that each image was taken from.</p> <p> </p> <p>Up to this point, you have been following a fairly standard workflow. A common practice at this point would be to generate a top-down, 2D orthomosaic of the scene and do any prediction tasks, such as deep learning model training or inference, using this data. Instead, you decide it's important to maintain the high quality of the raw images and be able to see the sides of your objects when you are generating predictions. This is where geograypher comes in.</p> <p>Using your field reference map and the 3D model from photogrammetry, you determine which portions of your 3D scene correspond to each object. This is shown below, with the colors now representing the classification label.</p> <p> </p> <p>Your end goal is to generate predictions on the entire region. For this, you need a machine learning model that can generate automatic predictions on your data. No one else has developed a model for your cone-cube-cylinder classification task, so you need to train your own using labeled example data. Using the mesh that is textured with the classification information from the field survey, and the pose of the camera, you can \"render\" the labels onto the images. They are shown below, color-coded by class.</p> <p> </p> <p>These labels correspond to the images shown below.</p> <p> </p> <p>Now that you have pairs of real images and rendered labels, you can train a machine learning model to predict the class of the objects from the images. This model can be now used to generate predictions on un-labeled images. An example prediction is shown below.</p> <p> </p> <p>To make these predictions useful, you need the information in geospatial coordinates. We again use the mesh model as an intermediate step between the image coordinates and 2D geospatial coordinates. The predictions are projected or \"splatted\" onto the mesh from each viewpoint.</p> <p> </p> <p> </p> <p>As seen above, each prediction only captures a small region of the mesh, and cannot make any predictions about parts of the object that were occluded in the original viewpoint. Therefore, we need to aggregate the predictions from all viewpoints to have an understanding of the entire scene. This gives us added robustness, because we can tolerate some prediction errors for a single viewpoint, by choosing the most common prediction across all viewpoints of a single location. The aggregated prediction is shown below.</p> <p> </p> <p>Now, the final step is to transform these predictions back into geospatial coordinates.</p> <p> </p>"},{"location":"getting_started/data/","title":"Data","text":""},{"location":"getting_started/data/#example-data","title":"Example data","text":"<p>The public example data is in <code>data/example_Emerald_Point_data</code> . You can run notebooks in the <code>examples</code> folder to see how to interact with this data. You can download this data using Google Drive from this folder. Once you've downloaded it, extract it into the <code>data</code> folder.</p>"},{"location":"getting_started/data/#using-your-own-data","title":"Using your own data","text":"<p>If you have a Metashape scene with the location of cameras, a mesh, and geospatial information, you can likely use geograypher. If you are using the Metashape GUI, you must do an important step before exporting the mesh model. Metashape stores the mesh in an arbitrary coordinate system that's optimized for viewing and will export it as such. To fix this, in the Metashape GUI you need to do <code>Model-&gt;Transform Object-&gt;Reset Transform</code> , then save the mesh with the local coordinates option. The cameras can be exported without any special considerations.</p> <p>You can also use our scripted workflow for running Metashape, automate-metashape. The cameras and the <code>local</code> mesh export will be properly formatted for use with geograypher.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>If you only need to use the exisiting functionality of Geograypher and not make changes to the toolkit code or dependencies, follow the <code>Basic Installation</code> instructions. </p> <p>If you want to do development work, please see <code>Advanced/Developer Installation</code>.</p> <p>Internal collaborators please navigate here for more instructions. </p>"},{"location":"getting_started/installation/#basic-installation","title":"Basic Installation","text":"<p>Create and activate a conda environment:</p> <pre><code>conda create -n geograypher -c conda-forge python=3.9 -y\nconda activate geograypher\n</code></pre> <p>Install Geograypher: <pre><code>pip install geograypher\n</code></pre></p>"},{"location":"getting_started/installation/#advanceddeveloper-installation","title":"Advanced/Developer Installation","text":"<p>Create and activate a conda environment:</p> <pre><code>conda create -n geograypher -c conda-forge python=3.9 -y\nconda activate geograypher\n</code></pre> <p>Install poetry:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Now use this to install the majority of dependencies. First cd to the directory containing the <code>geograypher</code> repo. Then run:</p> <pre><code>poetry install\n</code></pre> <p>You may get the following error when running <code>pyvista</code> visualization:</p>"},{"location":"getting_started/installation/#common-errors","title":"Common errors","text":""},{"location":"getting_started/installation/#poetry-hanging","title":"Poetry hanging","text":"<p>If <code>poetry install</code> hangs for a long time with no printouts, you can trying disabling the keyring process. poetry by default is trying to access credentials stored in your system keyring (e.g., for private package repositories) and if that isn't set up it can hang.</p> <pre><code>export PYTHON_KEYRING_BACKEND=keyring.backends.null.Keyring\n</code></pre> <p>Then re-run <code>poetry install</code>.</p>"},{"location":"getting_started/installation/#libgl-error","title":"libGL error","text":"<pre><code>libGL error: MESA-LOADER: failed to open swrast: &lt;CONDA ENV LOCATION&gt;/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /lib/x86_64-linux-gnu/libLLVM-15.so.1) (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n</code></pre> <p>If this happens, you can fix it by symlinking to the system version. I don't know why this is required.</p> <pre><code>ln -sf /usr/lib/x86_64-linux-gnu/libstdc++.so.6 &lt;CONDA ENV LOCATION&gt;/lib/libstdc++.so.6\n</code></pre>"},{"location":"getting_started/installation/#working-on-headless-machine","title":"Working on Headless Machine","text":"<p>If you are working on a headless machine, such as a remote server, you will need the XVFB package to provide a virtual frame buffer. This can be installed at the system level using the package manager, for example: <pre><code>sudo apt install xvfb\n</code></pre> If you do not have root access on your machine, it may not be possible to install xvfb.</p>"},{"location":"getting_started/installation/#optional-install-pytorch3d","title":"Optional: Install <code>pytorch3d</code>","text":"<p>If you are working on a headless machine and are unable to install xvfb, <code>pytorch3d</code> can be a viable alternative since installing it does not require admin privileges.</p> <p>Install the pytorch3d dependencies:</p> <pre><code>conda install pytorch=1.13.0 torchvision pytorch-cuda=11.6 -c pytorch -c nvidia -y\nconda install -c fvcore -c iopath -c conda-forge fvcore iopath -y\nconda install -c bottler nvidiacub -y\nconda install pytorch3d -c pytorch3d -y\n</code></pre> <p>Validate the installation</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\npython -c \"import pytorch3d; print(pytorch3d.__version__)\"\n</code></pre>"}]}