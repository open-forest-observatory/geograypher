{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Geograypher: Multiview Semantic Reasoning with Geospatial Data","text":"<p>Geograypher is a tool developed by the Open Forest Observatory to help answer ecological questions using images from aerial surveys. Land managers and research ecologists are increasingly using small uncrewed aerial systems (sUAS or \"drones\") to survey large regions with overlapping high-resolution aerial images. This data can be used to generate predictions of some attribute, for example identifying the location of individual trees or deliniating the boundaries of different classes of vegetation. A common workflow is to take the raw images from the drone survey and generate a stiched top-down \"orthomosaic\" using photogrammetry software. Then, this orthomosaic is used as input to a machine learning model that predicts the attribute in question.</p> <p>Geograypher was developed to support an alternative workflow where machine learning predictions are generated on individual images and then these predictions are projected into geospatial coordinates. This is beneficial because there are artifacts and errors in the orthomosaic because it is synthesized from many individual images. Additionally, there are multiple raw images of a given location which can each be used to make independent predictions. While geograypher primarily supports this \"multiview\" workflow, it also supports more conventional orthomosaic workflows which can be used as baseline or alternative approach.</p>"},{"location":"software_design/","title":"Software design","text":"<p>This page is a work in progress. Some main topics I'd like to cover are the following:</p> <ul> <li>What is in scope for geograyper. Mapping content between images, meshes, and geospatial coordinates is the main focus. Geograypher is designed to be used in workflows that use machine learning, but geograypher is explicitly designed to not perform machine learning itself.</li> <li>Software architecture: Geograypher is primarily an object oriented library.</li> <li>Main mathematical/algorithmic concepts. Geograypher leverages concepts from multiple fields such as geometric computer vision, graphics, and geospatial informatics.</li> <li>Roadmap: what features do we want to implement in the future. Primarily around object detection. And supporting more types of data and/or pluggable modules for rendering.</li> </ul>"},{"location":"API_reference/entrypoints/","title":"Entrypoints","text":"<p>Entrypoints are functions can be directly called to perform a high-level task. Each of the entrypoints listed below can be called as a command line script. These scripts accept arguments that are directly passed to these functions.</p>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.render_labels.render_labels","title":"<code>render_labels(mesh_file, cameras_file, image_folder, texture, render_savefolder, transform_file=None, subset_images_savefolder=None, texture_column_name=None, DTM_file=None, ground_height_threshold=None, render_ground_class=False, textured_mesh_savefile=None, ROI=None, mesh_ROI_buffer_radius_meters=50, cameras_ROI_buffer_radius_meters=150, IDs_to_labels=None, render_image_scale=1, mesh_downsample=1, n_render_clusters=None, vis=False, mesh_vis_file=None, labels_vis_folder=None)</code>","text":"<p>Renders image-based labels using geospatial ground truth data</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the MetaShape-exported .xml cameras file</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to create the mesh</p> required <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>See TexturedPhotogrammetryMesh.load_texture</p> required <code>render_savefolder</code> <code>PATH_TYPE</code> <p>Where to save the rendered labels</p> required <code>transform_file</code> <code>Union[PATH_TYPE, None]</code> <p>File containing the transform from local coordinates to EPSG:4978. Defaults to None.</p> <code>None</code> <code>subset_images_savefolder</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the subset of images for which labels are generated. Defaults to None.</p> <code>None</code> <code>texture_column_name</code> <code>Union[str, None]</code> <p>Column to use in vector file for texture information\". Defaults to None.</p> <code>None</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a DTM file to use for ground thresholding. Defaults to None.</p> <code>None</code> <code>ground_height_threshold</code> <code>Union[float, None]</code> <p>Set points under this height to ground. Only applicable if DTM_file is provided. Defaults to None.</p> <code>None</code> <code>render_ground_class</code> <code>bool</code> <p>Should the ground class be included in the renders or deleted.. Defaults to False.</p> <code>False</code> <code>textured_mesh_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the textured and subsetted mesh, if needed in the future. Defaults to None.</p> <code>None</code> <code>ROI</code> <code>Union[PATH_TYPE, GeoDataFrame, MultiPolygon, None]</code> <p>The region of interest to render labels for. Defaults to None.</p> <code>None</code> <code>mesh_ROI_buffer_radius_meters</code> <code>float</code> <p>The distance in meters to include around the ROI for the mesh. Defaults to 50.</p> <code>50</code> <code>cameras_ROI_buffer_radius_meters</code> <code>float</code> <p>The distance in meters to include around the ROI for the cameras. Defaults to 150.</p> <code>150</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping between the integer labels and string values for the classes. Defaults to None.</p> <code>None</code> <code>render_image_scale</code> <code>float</code> <p>Downsample the images to this fraction of the size for increased performance but lower quality. Defaults to 1.</p> <code>1</code> <code>mesh_downsample</code> <code>float</code> <p>Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.</p> <code>1</code> <code>n_render_clusters</code> <code>Union[int, None]</code> <p>If set, break the camera set and mesh into this many clusters before rendering. This is useful for large meshes that are otherwise very slow. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/entrypoints/render_labels.py</code> <pre><code>def render_labels(\n    mesh_file: PATH_TYPE,\n    cameras_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    texture: typing.Union[PATH_TYPE, np.ndarray, None],\n    render_savefolder: PATH_TYPE,\n    transform_file: typing.Union[PATH_TYPE, None] = None,\n    subset_images_savefolder: typing.Union[PATH_TYPE, None] = None,\n    texture_column_name: typing.Union[str, None] = None,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    ground_height_threshold: typing.Union[float, None] = None,\n    render_ground_class: bool = False,\n    textured_mesh_savefile: typing.Union[PATH_TYPE, None] = None,\n    ROI: typing.Union[PATH_TYPE, gpd.GeoDataFrame, shapely.MultiPolygon, None] = None,\n    mesh_ROI_buffer_radius_meters: float = 50,\n    cameras_ROI_buffer_radius_meters: float = 150,\n    IDs_to_labels: typing.Union[dict, None] = None,\n    render_image_scale: float = 1,\n    mesh_downsample: float = 1,\n    n_render_clusters: typing.Union[int, None] = None,\n    vis: bool = False,\n    mesh_vis_file: typing.Union[PATH_TYPE, None] = None,\n    labels_vis_folder: typing.Union[PATH_TYPE, None] = None,\n):\n    \"\"\"Renders image-based labels using geospatial ground truth data\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        cameras_file (PATH_TYPE):\n            Path to the MetaShape-exported .xml cameras file\n        image_folder (PATH_TYPE):\n            Path to the folder of images used to create the mesh\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]):\n            See TexturedPhotogrammetryMesh.load_texture\n        render_savefolder (PATH_TYPE):\n            Where to save the rendered labels\n        transform_file (typing.Union[PATH_TYPE, None], optional):\n            File containing the transform from local coordinates to EPSG:4978. Defaults to None.\n        subset_images_savefolder (typing.Union[PATH_TYPE, None], optional):\n            Where to save the subset of images for which labels are generated. Defaults to None.\n        texture_column_name (typing.Union[str, None], optional):\n            Column to use in vector file for texture information\". Defaults to None.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a DTM file to use for ground thresholding. Defaults to None.\n        ground_height_threshold (typing.Union[float, None], optional):\n            Set points under this height to ground. Only applicable if DTM_file is provided. Defaults to None.\n        render_ground_class (bool, optional):\n            Should the ground class be included in the renders or deleted.. Defaults to False.\n        textured_mesh_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the textured and subsetted mesh, if needed in the future. Defaults to None.\n        ROI (typing.Union[PATH_TYPE, gpd.GeoDataFrame, shapely.MultiPolygon, None], optional):\n            The region of interest to render labels for. Defaults to None.\n        mesh_ROI_buffer_radius_meters (float, optional):\n            The distance in meters to include around the ROI for the mesh. Defaults to 50.\n        cameras_ROI_buffer_radius_meters (float, optional):\n            The distance in meters to include around the ROI for the cameras. Defaults to 150.\n        IDs_to_labels (typing.Union[None, dict], optional):\n            Mapping between the integer labels and string values for the classes. Defaults to None.\n        render_image_scale (float, optional):\n            Downsample the images to this fraction of the size for increased performance but lower quality. Defaults to 1.\n        mesh_downsample (float, optional):\n            Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.\n        n_render_clusters (typing.Union[int, None]):\n            If set, break the camera set and mesh into this many clusters before rendering. This is\n            useful for large meshes that are otherwise very slow. Defaults to None.\n        mesh_vis (typing.Union[PATH_TYPE, None])\n            Path to save the visualized mesh instead of showing it interactively. Only applicable if vis=True. Defaults to None.\n        labels_vis (typing.Union[PATH_TYPE, None])\n            Defaults to None.\n    \"\"\"\n    ## Determine the ROI\n    # If the ROI is unset and the texture is a geodataframe, set the ROI to that\n    if ROI is None and isinstance(texture, gpd.GeoDataFrame):\n        ROI = texture\n    elif ROI is None and isinstance(texture, (str, Path)):\n        try:\n            ROI = gpd.read_file(texture)\n        except fiona.errors.DriverError:\n            pass\n\n    # If the transform filename is None, use the cameras filename instead\n    # since this contains the transform information\n    if transform_file is None:\n        transform_file = cameras_file\n\n    ## Create the camera set\n    # This is done first because it's often faster than mesh operations which\n    # makes it a good place to check for failures\n    camera_set = MetashapeCameraSet(cameras_file, image_folder)\n\n    if ROI is not None:\n        # Extract cameras near the training data\n        camera_set = camera_set.get_subset_ROI(\n            ROI=ROI, buffer_radius=cameras_ROI_buffer_radius_meters, is_geospatial=True\n        )\n    # If requested, save out the images corresponding to this subset of cameras.\n    # This is useful for model training.\n    if subset_images_savefolder is not None:\n        camera_set.save_images(subset_images_savefolder)\n\n    # Select whether to use a class that renders by chunks or not\n    MeshClass = (\n        TexturedPhotogrammetryMesh\n        if n_render_clusters is None\n        else TexturedPhotogrammetryMeshChunked\n    )\n\n    ## Create the textured mesh\n    mesh = MeshClass(\n        mesh_file,\n        downsample_target=mesh_downsample,\n        texture=texture,\n        texture_column_name=texture_column_name,\n        transform_filename=transform_file,\n        ROI=ROI,\n        ROI_buffer_meters=mesh_ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n    )\n\n    ## Set the ground class if applicable\n    if DTM_file is not None and ground_height_threshold is not None:\n        # The ground ID will be set to the next value if None, or np.nan if np.nan\n        ground_ID = None if render_ground_class else np.nan\n        mesh.label_ground_class(\n            DTM_file=DTM_file,\n            height_above_ground_threshold=ground_height_threshold,\n            only_label_existing_labels=True,\n            ground_class_name=\"GROUND\",\n            ground_ID=ground_ID,\n            set_mesh_texture=True,\n        )\n\n    # Save the textured and subsetted mesh, if applicable\n    if textured_mesh_savefile is not None:\n        mesh.save_mesh(textured_mesh_savefile)\n\n    # Show the cameras and mesh if requested\n    if vis or mesh_vis_file is not None:\n        mesh.vis(camera_set=camera_set, screenshot_filename=mesh_vis_file)\n\n    # Include n_render_clusters as an optional keyword argument, if provided. This is only applicable\n    # if this mesh is a TexturedPhotogrammetryMeshChunked object\n    render_kwargs = (\n        {} if n_render_clusters is None else {\"n_clusters\": n_render_clusters}\n    )\n    # Render the labels and save them. This is the slow step.\n    mesh.save_renders(\n        camera_set=camera_set,\n        render_image_scale=render_image_scale,\n        save_native_resolution=True,\n        output_folder=render_savefolder,\n        make_composites=False,\n        **render_kwargs,\n    )\n\n    if vis or labels_vis_folder is not None:\n        # Show some examples of the rendered labels side-by-side with the real images\n        show_segmentation_labels(\n            label_folder=render_savefolder,\n            image_folder=image_folder,\n            savefolder=labels_vis_folder,\n            num_show=10,\n        )\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.aggregate_images.aggregate_images","title":"<code>aggregate_images(mesh_file, cameras_file, image_folder, label_folder, subset_images_folder=None, filename_regex=None, take_every_nth_camera=100, mesh_transform_file=None, DTM_file=None, height_above_ground_threshold=2.0, ROI=None, ROI_buffer_radius_meters=50, IDs_to_labels=None, mesh_downsample=1.0, n_aggregation_clusters=None, n_cameras_per_aggregation_cluster=None, aggregate_image_scale=1.0, aggregated_face_values_savefile=None, predicted_face_classes_savefile=None, top_down_vector_projection_savefile=None, vis=False)</code>","text":"<p>Aggregate labels from multiple viewpoints onto the surface of the mesh</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the MetaShape-exported .xml cameras file</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to create the mesh</p> required <code>filename_regex</code> <code>str</code> <p>Use only images with paths matching this regex</p> <code>None</code> <code>label_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of labels to be aggregated onto the mesh. Must be in the same structure as the images</p> required <code>subset_images_folder</code> <code>Union[PATH_TYPE, None]</code> <p>Use only images from this subset. Defaults to None.</p> <code>None</code> <code>take_every_nth_camera</code> <code>Union[int, None]</code> <p>Downsample the camera set to only every nth camera if set. Defaults to None.</p> <code>100</code> <code>mesh_transform_file</code> <code>Union[PATH_TYPE, None]</code> <p>Transform from the mesh coordinates to the earth-centered, earth-fixed frame. Can be a 4x4 matrix represented as a .csv, or a Metashape cameras file containing the information. Defaults to None.</p> <code>None</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a digital terrain model file to remove ground points. Defaults to None.</p> <code>None</code> <code>height_above_ground_threshold</code> <code>float</code> <p>Height in meters above the DTM to consider ground. Only used if DTM_file is set. Defaults to 2.0.</p> <code>2.0</code> <code>ROI</code> <code>Union[PATH_TYPE, None]</code> <p>Geofile region of interest to crop the mesh to. Defaults to None.</p> <code>None</code> <code>ROI_buffer_radius_meters</code> <code>float</code> <p>Keep points within this distance of the provided ROI object, if unset, everything will be kept. Defaults to 50.</p> <code>50</code> <code>IDs_to_labels</code> <code>Union[dict, None]</code> <p>Maps from integer IDs to human-readable class name labels. Defaults to None.</p> <code>None</code> <code>mesh_downsample</code> <code>float</code> <p>Downsample the mesh to this fraction of vertices for increased performance but lower quality. Defaults to 1.0.</p> <code>1.0</code> <code>n_aggregation_clusters</code> <code>Union[int, None]</code> <p>If set, aggregate with this many clusters. Defaults to None.</p> <code>None</code> <code>n_cameras_per_aggregation_cluster</code> <code>Union[int, None]</code> <p>If set, and n_aggregation_clusters is not, use to compute a number of clusters such that each cluster has this many cameras. Defaults to None.</p> <code>None</code> <code>aggregate_image_scale</code> <code>float</code> <p>Downsample the labels before aggregation for faster runtime but lower quality. Defaults to 1.0.</p> <code>1.0</code> <code>aggregated_face_values_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the aggregated image values as a numpy array. Defaults to None.</p> <code>None</code> <code>predicted_face_classes_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the most common label per face texture as a numpy array. Defaults to None.</p> <code>None</code> <code>top_down_vector_projection_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to export the predicted map. Defaults to None.</p> <code>None</code> <code>vis</code> <code>bool</code> <p>Show the mesh model and predicted results. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/entrypoints/aggregate_images.py</code> <pre><code>def aggregate_images(\n    mesh_file: PATH_TYPE,\n    cameras_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    label_folder: PATH_TYPE,\n    subset_images_folder: typing.Union[PATH_TYPE, None] = None,\n    filename_regex: typing.Optional[str] = None,\n    take_every_nth_camera: typing.Union[int, None] = 100,\n    mesh_transform_file: typing.Union[PATH_TYPE, None] = None,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    height_above_ground_threshold: float = 2.0,\n    ROI: typing.Union[PATH_TYPE, None] = None,\n    ROI_buffer_radius_meters: float = 50,\n    IDs_to_labels: typing.Union[dict, None] = None,\n    mesh_downsample: float = 1.0,\n    n_aggregation_clusters: typing.Union[int, None] = None,\n    n_cameras_per_aggregation_cluster: typing.Union[int, None] = None,\n    aggregate_image_scale: float = 1.0,\n    aggregated_face_values_savefile: typing.Union[PATH_TYPE, None] = None,\n    predicted_face_classes_savefile: typing.Union[PATH_TYPE, None] = None,\n    top_down_vector_projection_savefile: typing.Union[PATH_TYPE, None] = None,\n    vis: bool = False,\n):\n    \"\"\"Aggregate labels from multiple viewpoints onto the surface of the mesh\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        cameras_file (PATH_TYPE):\n            Path to the MetaShape-exported .xml cameras file\n        image_folder (PATH_TYPE):\n            Path to the folder of images used to create the mesh\n        filename_regex (str, optional):\n            Use only images with paths matching this regex\n        label_folder (PATH_TYPE):\n            Path to the folder of labels to be aggregated onto the mesh. Must be in the same\n            structure as the images\n        subset_images_folder (typing.Union[PATH_TYPE, None], optional):\n            Use only images from this subset. Defaults to None.\n        take_every_nth_camera (typing.Union[int, None], optional):\n            Downsample the camera set to only every nth camera if set. Defaults to None.\n        mesh_transform_file (typing.Union[PATH_TYPE, None], optional):\n            Transform from the mesh coordinates to the earth-centered, earth-fixed frame. Can be a\n            4x4 matrix represented as a .csv, or a Metashape cameras file containing the\n            information. Defaults to None.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a digital terrain model file to remove ground points. Defaults to None.\n        height_above_ground_threshold (float, optional):\n            Height in meters above the DTM to consider ground. Only used if DTM_file is set.\n            Defaults to 2.0.\n        ROI (typing.Union[PATH_TYPE, None], optional):\n            Geofile region of interest to crop the mesh to. Defaults to None.\n        ROI_buffer_radius_meters (float, optional):\n            Keep points within this distance of the provided ROI object, if unset, everything will\n            be kept. Defaults to 50.\n        IDs_to_labels (typing.Union[dict, None], optional):\n            Maps from integer IDs to human-readable class name labels. Defaults to None.\n        mesh_downsample (float, optional):\n            Downsample the mesh to this fraction of vertices for increased performance but lower\n            quality. Defaults to 1.0.\n        n_aggregation_clusters (typing.Union[int, None]):\n            If set, aggregate with this many clusters. Defaults to None.\n        n_cameras_per_aggregation_cluster (typing.Union[int, None]):\n            If set, and n_aggregation_clusters is not, use to compute a number of clusters such that\n            each cluster has this many cameras. Defaults to None.\n        aggregate_image_scale (float, optional):\n            Downsample the labels before aggregation for faster runtime but lower quality. Defaults\n            to 1.0.\n        aggregated_face_values_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the aggregated image values as a numpy array. Defaults to None.\n        predicted_face_classes_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the most common label per face texture as a numpy array. Defaults to None.\n        top_down_vector_projection_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to export the predicted map. Defaults to None.\n        vis (bool, optional):\n            Show the mesh model and predicted results. Defaults to False.\n    \"\"\"\n    ## Create the camera set\n    # Do the camera operations first because they are fast and good initial error checking\n    camera_set = MetashapeCameraSet(cameras_file, image_folder, validate_images=True)\n\n    # If the ROI is not None, subset to cameras within a buffer distance of the ROI\n    # TODO let get_subset_ROI accept a None ROI and return the full camera set\n    if subset_images_folder is not None:\n        camera_set = camera_set.get_cameras_in_folder(subset_images_folder)\n\n    # Subset based on regex if requested\n    if filename_regex is not None:\n        camera_set = camera_set.get_cameras_matching_filename_regex(\n            filename_regex=filename_regex\n        )\n\n    # If you only want to take every nth camera, helpful for initial testing\n    if take_every_nth_camera is not None:\n        camera_set = camera_set.get_subset_cameras(\n            range(0, len(camera_set), take_every_nth_camera)\n        )\n\n    if ROI is not None and ROI_buffer_radius_meters is not None:\n        # Extract cameras near the training data\n        camera_set = camera_set.get_subset_ROI(\n            ROI=ROI, buffer_radius=ROI_buffer_radius_meters\n        )\n\n    if mesh_transform_file is None:\n        mesh_transform_file = cameras_file\n\n    # If the number of aggregation clusters is not set but the number of cameras per cluster is,\n    # then compute it\n    if n_aggregation_clusters is None and n_cameras_per_aggregation_cluster is not None:\n        n_aggregation_clusters = int(\n            math.ceil(len(camera_set) / n_cameras_per_aggregation_cluster)\n        )\n\n    # Choose whether to use a mesh class that aggregates by clusters of cameras and chunks of the mesh\n    MeshClass = (\n        TexturedPhotogrammetryMesh\n        if n_aggregation_clusters is None\n        else TexturedPhotogrammetryMeshChunked\n    )\n    ## Create the mesh\n    mesh = MeshClass(\n        mesh_file,\n        transform_filename=mesh_transform_file,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n        downsample_target=mesh_downsample,\n    )\n\n    # Show the mesh if requested\n    if vis:\n        mesh.vis(camera_set=camera_set)\n\n    # Create a segmentor object to load in the predictions\n    segmentor = LookUpSegmentor(\n        base_folder=image_folder,\n        lookup_folder=label_folder,\n        num_classes=np.max(list(mesh.get_IDs_to_labels().keys())) + 1,\n    )\n    # Create a camera set that returns the segmented images instead of the original ones\n    segmentor_camera_set = SegmentorPhotogrammetryCameraSet(\n        camera_set, segmentor=segmentor\n    )\n\n    # Create the potentially-empty dict of kwargs to match what this class expects\n    n_clusters_kwargs = (\n        {} if n_aggregation_clusters is None else {\"n_clusters\": n_aggregation_clusters}\n    )\n\n    ## Perform aggregation, this is the slow step\n    aggregated_face_labels, _ = mesh.aggregate_projected_images(\n        segmentor_camera_set,\n        aggregate_img_scale=aggregate_image_scale,\n        **n_clusters_kwargs,\n    )\n\n    # If requested, save this data\n    if aggregated_face_values_savefile is not None:\n        ensure_containing_folder(aggregated_face_values_savefile)\n        np.save(aggregated_face_values_savefile, aggregated_face_labels)\n\n    # Find the index of the most common class per face, with faces with no predictions set to nan\n    predicted_face_classes = find_argmax_nonzero_value(\n        aggregated_face_labels, keepdims=True\n    )\n\n    # If requested, label the ground faces\n    if DTM_file is not None and height_above_ground_threshold is not None:\n        predicted_face_classes = mesh.label_ground_class(\n            labels=predicted_face_classes,\n            height_above_ground_threshold=height_above_ground_threshold,\n            DTM_file=DTM_file,\n            ground_ID=np.nan,\n            set_mesh_texture=False,\n        )\n\n    if predicted_face_classes_savefile is not None:\n        ensure_containing_folder(predicted_face_classes_savefile)\n        np.save(predicted_face_classes_savefile, predicted_face_classes)\n\n    if vis:\n        # Show the mesh with predicted classes\n        mesh.vis(vis_scalars=predicted_face_classes)\n\n    # If the vector file should be exported\n    if top_down_vector_projection_savefile is not None:\n        # Compute the label names\n        if IDs_to_labels is not None:\n            # This ensures that any missing keys are replaced with None so proper indexing is retained\n            label_names = [\n                IDs_to_labels.get(i, None)\n                for i in range(max(list(IDs_to_labels.keys())) + 1)\n            ]\n        else:\n            label_names = None\n        # Export the 2D top down projection\n        mesh.export_face_labels_vector(\n            face_labels=np.squeeze(predicted_face_classes),\n            export_file=top_down_vector_projection_savefile,\n            vis=vis,\n            label_names=label_names,\n        )\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.project_detections.project_detections","title":"<code>project_detections(mesh_filename, cameras_filename, project_to_mesh=False, convert_to_geospatial=False, image_folder=None, detections_folder=None, projections_to_mesh_filename=None, projections_to_geospatial_savefilename=None, default_focal_length=None, image_shape=None, segmentor_kwargs={}, vis_mesh=False, vis_geodata=False)</code>","text":"<p>Project per-image detections to geospatial coordinates</p> <p>Parameters:</p> Name Type Description Default <code>mesh_filename</code> <code>PATH_TYPE</code> <p>Path to mesh file, in local coordinates from Metashape</p> required <code>cameras_filename</code> <code>PATH_TYPE</code> <p>Path to cameras file. This also contains local-to-global coordinate transform to convert the mesh to geospatial units.</p> required <code>project_to_mesh</code> <code>bool</code> <p>Execute the projection to mesh step. Defaults to False.</p> <code>False</code> <code>convert_to_geospatial</code> <code>bool</code> <p>Execute the conversion to geospatial step. Defaults to False.</p> <code>False</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the folder of images used to generate the detections. TODO, see if this can be removed since none of this information is actually used. Defaults to None.</p> <code>None</code> <code>detections_folder</code> <code>PATH_TYPE</code> <p>Folder of detections in the DeepForest format, one per image. Defaults to None.</p> <code>None</code> <code>projections_to_mesh_filename</code> <code>PATH_TYPE</code> <p>Where to save and/or load from the data for the detections projected to the mesh faces. Defaults to None.</p> <code>None</code> <code>projections_to_geospatial_savefilename</code> <code>PATH_TYPE</code> <p>Where to export the geospatial detections. Defaults to None.</p> <code>None</code> <code>default_focal_length</code> <code>float</code> <p>Since the focal length is not provided in many cameras files, it can be specified. The units are in pixels. TODO, figure out where this information can be reliably obtained from. Defaults to None.</p> <code>None</code> <code>segmentor_kwargs</code> <code>dict</code> <p>Dict of keyword arguments to pass to the segmentor. Defaults to {}.</p> <code>{}</code> <code>vis_mesh</code> <code>bool</code> <p>Show the mesh with detections projected onto it. Defaults to False.</p> <code>False</code> <code>vis_geodata</code> <code>bool</code> <p>Show the geospatial projection. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If convert_to_geospatial but no projections to mesh are available</p> <code>FileNotFoundError</code> <p>If the projections_to_mesh_filename is set and needed but not present</p> Source code in <code>geograypher/entrypoints/project_detections.py</code> <pre><code>def project_detections(\n    mesh_filename: PATH_TYPE,\n    cameras_filename: PATH_TYPE,\n    project_to_mesh: bool = False,\n    convert_to_geospatial: bool = False,\n    image_folder: PATH_TYPE = None,\n    detections_folder: PATH_TYPE = None,\n    projections_to_mesh_filename: PATH_TYPE = None,\n    projections_to_geospatial_savefilename: PATH_TYPE = None,\n    default_focal_length: float = None,\n    image_shape: tuple = None,\n    segmentor_kwargs: dict = {},\n    vis_mesh: bool = False,\n    vis_geodata: bool = False,\n):\n    \"\"\"Project per-image detections to geospatial coordinates\n\n    Args:\n        mesh_filename (PATH_TYPE):\n            Path to mesh file, in local coordinates from Metashape\n        cameras_filename (PATH_TYPE):\n            Path to cameras file. This also contains local-to-global coordinate transform to convert\n            the mesh to geospatial units.\n        project_to_mesh (bool, optional):\n            Execute the projection to mesh step. Defaults to False.\n        convert_to_geospatial (bool, optional):\n            Execute the conversion to geospatial step. Defaults to False.\n        image_folder (PATH_TYPE, optional):\n            Path to the folder of images used to generate the detections. TODO, see if this can be\n            removed since none of this information is actually used. Defaults to None.\n        detections_folder (PATH_TYPE, optional):\n            Folder of detections in the DeepForest format, one per image. Defaults to None.\n        projections_to_mesh_filename (PATH_TYPE, optional):\n            Where to save and/or load from the data for the detections projected to the mesh faces.\n            Defaults to None.\n        projections_to_geospatial_savefilename (PATH_TYPE, optional):\n            Where to export the geospatial detections. Defaults to None.\n        default_focal_length (float, optional):\n            Since the focal length is not provided in many cameras files, it can be specified.\n            The units are in pixels. TODO, figure out where this information can be reliably obtained\n            from. Defaults to None.\n        segmentor_kwargs (dict, optional):\n            Dict of keyword arguments to pass to the segmentor. Defaults to {}.\n        vis_mesh (bool, optional):\n            Show the mesh with detections projected onto it. Defaults to False.\n        vis_geodata (bool, optional):\n            Show the geospatial projection. Defaults to False.\n\n    Raises:\n        ValueError: If convert_to_geospatial but no projections to mesh are available\n        FileNotFoundError: If the projections_to_mesh_filename is set and needed but not present\n    \"\"\"\n    # Create the mesh object, which will be used for either workflow\n    mesh = TexturedPhotogrammetryMeshIndexPredictions(\n        mesh_filename, transform_filename=cameras_filename\n    )\n\n    # Project per-image detections to the mesh\n    if project_to_mesh:\n        # Create a camera set associated with the images that have detections\n        camera_set = MetashapeCameraSet(\n            cameras_filename,\n            image_folder,\n            default_sensor_params={\"f\": default_focal_length, \"cx\": 0, \"cy\": 0},\n        )\n        # Infer the image shape from the first image in the folder\n        if image_shape is None:\n            image_filename_list = sorted(list(Path(image_folder).glob(\"*.*\")))\n            if len(image_filename_list) &gt; 0:\n                first_file = image_filename_list[0]\n                logging.info(f\"loading image shape from {first_file}\")\n                first_image = imread(first_file)\n                image_shape = first_image.shape[:2]\n            else:\n                raise ValueError(\n                    f\"No image_shape provided and folder of images {image_folder} was empty\"\n                )\n        # Create an object that looks up the detections from a folder of CSVs or one individual one.\n        # Using this, it can generate \"predictions\" for a given image.\n        detections_predictor = TabularRectangleSegmentor(\n            detection_file_or_folder=detections_folder,\n            image_folder=image_folder,\n            image_shape=image_shape,\n            **segmentor_kwargs,\n        )\n\n        # If a file is provided for the projections, save the detection info alongside it\n        if projections_to_mesh_filename is not None:\n            # Export the per-image detection information as one standardized file\n            detection_info_file = Path(\n                projections_to_mesh_filename.parent,\n                projections_to_mesh_filename.stem + \"_detection_info.csv\",\n            )\n            logging.info(f\"Saving detection info to {detection_info_file}\")\n            detections_predictor.save_detection_data(detection_info_file)\n\n        # Wrap the camera set so that it returns the detections rather than the original images\n        detections_camera_set = SegmentorPhotogrammetryCameraSet(\n            camera_set, segmentor=detections_predictor\n        )\n        # Project the detections to the mesh\n        aggregated_prejected_images_returns = mesh.aggregate_projected_images(\n            cameras=detections_camera_set, n_classes=detections_predictor.num_classes\n        )\n        # Get the summed (not averaged) projections\n        aggregated_projections = aggregated_prejected_images_returns[1][\n            \"summed_projections\"\n        ]\n\n        if projections_to_mesh_filename is not None:\n            # Export the per-face texture to an npz file, since it's a sparse array\n            ensure_containing_folder(projections_to_mesh_filename)\n            save_npz(projections_to_mesh_filename, aggregated_projections)\n\n        if vis_mesh:\n            # Determine which detection is predicted for each face, if any. In cases where multiple\n            # detections project to the same face, the one with the lower index will be reported\n            detection_ID_per_face = np.argmax(aggregated_projections, axis=1).astype(\n                float\n            )\n            # Mask out locations for which there are no predictions\n            detection_ID_per_face[np.sum(aggregated_projections, axis=1) == 0] = np.nan\n            # Show the mesh\n            mesh.vis(vis_scalars=detection_ID_per_face)\n\n    # Convert per-face projections to geospatial ones\n    if convert_to_geospatial:\n        # Determine if the mesh texture was computed in the last step or otherwise if it can be loaded\n        if not project_to_mesh:\n            if projections_to_mesh_filename is None:\n                raise ValueError(\"No projections_to_mesh_savefilename provided\")\n            elif os.path.isfile(projections_to_mesh_filename):\n                aggregated_projections = load_npz(projections_to_mesh_filename)\n                detection_info_file = Path(\n                    projections_to_mesh_filename.parent,\n                    projections_to_mesh_filename.stem + \"_detection_info.csv\",\n                )\n                detection_info = pd.read_csv(detection_info_file)\n            else:\n                raise FileNotFoundError(\n                    f\"projections_to_mesh_filename {projections_to_mesh_filename} not found\"\n                )\n        else:\n            detection_info = detections_predictor.get_all_detections()\n\n        # Convert the per-face labels to geospatial coordinates. Optionally vis and/or export\n        mesh.export_face_labels_vector(\n            face_labels=aggregated_projections,\n            export_file=projections_to_geospatial_savefilename,\n            vis=vis_geodata,\n        )\n\n        projected_geo_data = gpd.read_file(projections_to_geospatial_savefilename)\n        # Merge the two dataframes so the left df's \"class_ID\" field aligns with the right df's\n        # \"instance_ID\". This will add back the original data assocaited with each per-image detection\n        # to the projected data.\n        # Add the \"_right\" suffix to any of the original fields that share a name with the ones in the\n        # projected data\n        merged = projected_geo_data.merge(\n            detection_info,\n            left_on=CLASS_ID_KEY,\n            right_on=INSTANCE_ID_KEY,\n            suffixes=(None, \"_right\"),\n        )\n        # Drop the columns that are just an integer ID, except for \"instance_ID\"\n        # TODO determine why \"Unnamed: 0\" appears\n        merged.drop(columns=[CLASS_ID_KEY, \"Unnamed: 0\"], inplace=True)\n\n        # Save the data back out with the updated information\n        merged.to_file(projections_to_geospatial_savefilename)\n</code></pre>"},{"location":"API_reference/entrypoints/#geograypher.entrypoints.label_polygons.label_polygons","title":"<code>label_polygons(mesh_file, mesh_transform_file, aggregated_face_values_file, geospatial_polygons_to_label, geospatial_polygons_labeled_savefile, mesh_downsample=1.0, DTM_file=None, height_above_ground_threshold=2.0, ground_voting_weight=0.01, ROI=None, ROI_buffer_radius_meters=50, n_polygons_per_cluster=1000, IDs_to_labels=None, vis_mesh=False)</code>","text":"<p>Label each polygon with the most commonly predicted class as computed by the weighted sum of 3D face areas</p> <p>Parameters:</p> Name Type Description Default <code>mesh_file</code> <code>PATH_TYPE</code> <p>Path to the Metashape-exported mesh file</p> required <code>mesh_transform_file</code> <code>PATH_TYPE</code> <p>Transform from the mesh coordinates to the earth-centered, earth-fixed frame. Can be a 4x4 matrix represented as a .csv, or a Metashape cameras file containing the information.</p> required <code>aggregated_face_values_file</code> <code>PATH_TYPE</code> <p>Path to a (n_faces, n_classes) numpy array containing the frequency of each class prediction for each face</p> required <code>geospatial_polygons_to_label</code> <code>Union[PATH_TYPE, None]</code> <p>Each polygon/multipolygon will be labeled independently. Defaults to None.</p> required <code>geospatial_polygons_labeled_savefile</code> <code>Union[PATH_TYPE, None]</code> <p>Where to save the labeled results.</p> required <code>mesh_downsample</code> <code>float</code> <p>Fraction to downsample mesh. Should match what was used to generate the aggregated_face_values_file. Defaults to 1.0.</p> <code>1.0</code> <code>DTM_file</code> <code>Union[PATH_TYPE, None]</code> <p>Path to a digital terrain model file to remove ground points. Defaults to None.</p> <code>None</code> <code>height_above_ground_threshold</code> <code>float</code> <p>Height in meters above the DTM to consider ground. Only used if DTM_file is set. Defaults to 2.0.</p> <code>2.0</code> <code>ground_voting_weight</code> <code>float</code> <p>Faces identified as ground are given this weight during voting. Defaults to 0.01.</p> <code>0.01</code> <code>ROI</code> <code>Union[PATH_TYPE, None]</code> <p>Geofile region of interest to crop the mesh to. Should match what was used to generate aggregated_face_values_file. Defaults to None.</p> <code>None</code> <code>ROI_buffer_radius_meters</code> <code>float</code> <p>Keep points within this distance of the provided ROI object, if unset, everything will be kept. Should match what was used to generate aggregated_face_values_file. Defaults to 50.</p> <code>50</code> <code>n_polygons_per_cluster</code> <code>int</code> <p>The number of polygons to use in each cluster, when computing labeling by chunks. Defaults to 1000.</p> <code>1000</code> <code>IDs_to_labels</code> <code>Union[dict, None]</code> <p>Mapping from integer IDs to human readable labels. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/entrypoints/label_polygons.py</code> <pre><code>def label_polygons(\n    mesh_file: PATH_TYPE,\n    mesh_transform_file: PATH_TYPE,\n    aggregated_face_values_file: PATH_TYPE,\n    geospatial_polygons_to_label: typing.Union[PATH_TYPE, None],\n    geospatial_polygons_labeled_savefile: typing.Union[PATH_TYPE, None],\n    mesh_downsample: float = 1.0,\n    DTM_file: typing.Union[PATH_TYPE, None] = None,\n    height_above_ground_threshold: float = 2.0,\n    ground_voting_weight: float = 0.01,\n    ROI: typing.Union[PATH_TYPE, None] = None,\n    ROI_buffer_radius_meters: float = 50,\n    n_polygons_per_cluster: int = 1000,\n    IDs_to_labels: typing.Union[dict, None] = None,\n    vis_mesh: bool = False,\n):\n    \"\"\"\n    Label each polygon with the most commonly predicted class as computed by the weighted sum of 3D\n    face areas\n\n    Args:\n        mesh_file (PATH_TYPE):\n            Path to the Metashape-exported mesh file\n        mesh_transform_file (PATH_TYPE):\n            Transform from the mesh coordinates to the earth-centered, earth-fixed frame. Can be a\n            4x4 matrix represented as a .csv, or a Metashape cameras file containing the information.\n        aggregated_face_values_file (PATH_TYPE):\n            Path to a (n_faces, n_classes) numpy array containing the frequency of each class\n            prediction for each face\n        geospatial_polygons_to_label (typing.Union[PATH_TYPE, None], optional):\n            Each polygon/multipolygon will be labeled independently. Defaults to None.\n        geospatial_polygons_labeled_savefile (typing.Union[PATH_TYPE, None], optional):\n            Where to save the labeled results.\n        mesh_downsample (float, optional):\n            Fraction to downsample mesh. Should match what was used to generate the\n            aggregated_face_values_file. Defaults to 1.0.\n        DTM_file (typing.Union[PATH_TYPE, None], optional):\n            Path to a digital terrain model file to remove ground points. Defaults to None.\n        height_above_ground_threshold (float, optional):\n            Height in meters above the DTM to consider ground. Only used if DTM_file is set.\n            Defaults to 2.0.\n        ground_voting_weight (float, optional):\n            Faces identified as ground are given this weight during voting. Defaults to 0.01.\n        ROI (typing.Union[PATH_TYPE, None], optional):\n            Geofile region of interest to crop the mesh to. Should match what was used to generate\n            aggregated_face_values_file. Defaults to None.\n        ROI_buffer_radius_meters (float, optional):\n            Keep points within this distance of the provided ROI object, if unset, everything will\n            be kept. Should match what was used to generate aggregated_face_values_file. Defaults to 50.\n        n_polygons_per_cluster (int, optional):\n            The number of polygons to use in each cluster, when computing labeling by chunks.\n            Defaults to 1000.\n        IDs_to_labels (typing.Union[dict, None], optional):\n            Mapping from integer IDs to human readable labels. Defaults to None.\n    \"\"\"\n    # Load this first because it's quick\n    aggregated_face_values = np.load(aggregated_face_values_file)\n    predicted_face_classes = np.argmax(aggregated_face_values, axis=1).astype(float)\n    no_preds_mask = np.all(np.logical_not(np.isfinite(aggregated_face_values)), axis=1)\n    predicted_face_classes[no_preds_mask] = np.nan\n\n    ## Create the mesh\n    mesh = TexturedPhotogrammetryMeshChunked(\n        mesh_file,\n        transform_filename=mesh_transform_file,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_radius_meters,\n        IDs_to_labels=IDs_to_labels,\n        downsample_target=mesh_downsample,\n    )\n\n    if vis_mesh:\n        mesh.vis(vis_scalars=predicted_face_classes)\n\n    # Extract which vertices are labeled as ground\n    # TODO check that the types are correct here\n    ground_mask_verts = mesh.get_height_above_ground(\n        DTM_file=DTM_file,\n        threshold=height_above_ground_threshold,\n    )\n    # Convert that vertex labels into face labels\n    ground_mask_faces = mesh.vert_to_face_texture(ground_mask_verts)\n\n    # Ground points get a weighting of ground_voting_weight, others get 1\n    ground_weighting = 1 - (\n        (1 - ground_voting_weight) * ground_mask_faces.astype(float)\n    )\n    if vis_mesh:\n        ground_masked_predicted_face_classes = predicted_face_classes.copy()\n        ground_masked_predicted_face_classes[ground_mask_faces.astype(bool)] = np.nan\n        mesh.vis(vis_scalars=ground_masked_predicted_face_classes)\n\n    # Perform per-polygon labeling\n    polygon_labels = mesh.label_polygons(\n        face_labels=predicted_face_classes,\n        polygons=geospatial_polygons_to_label,\n        face_weighting=ground_weighting,\n        n_polygons_per_cluster=n_polygons_per_cluster,\n    )\n\n    # Save out the predicted classes into a copy of the original file\n    geospatial_polygons = gpd.read_file(geospatial_polygons_to_label)\n    geospatial_polygons[PRED_CLASS_ID_KEY] = polygon_labels\n    ensure_containing_folder(geospatial_polygons_labeled_savefile)\n    geospatial_polygons.to_file(geospatial_polygons_labeled_savefile)\n</code></pre>"},{"location":"API_reference/cameras/","title":"Cameras","text":"<ul> <li> <p>Cameras Docstrings</p> </li> <li> <p>Derived Cameras Docstrings</p> </li> <li> <p>Segmentor Docstrings</p> </li> </ul>"},{"location":"API_reference/cameras/cameras/","title":"Cameras Docstrings","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera","title":"<code>PhotogrammetryCamera</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCamera:\n    def __init__(\n        self,\n        image_filename: PATH_TYPE,\n        cam_to_world_transform: np.ndarray,\n        f: float,\n        cx: float,\n        cy: float,\n        image_width: int,\n        image_height: int,\n        distortion_params: Dict[str, float] = {},\n        lon_lat: Union[None, Tuple[float, float]] = None,\n        local_to_epsg_4978_transform: Union[np.array, None] = None,\n    ):\n        \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n        Args:\n            image_filename (PATH_TYPE): The image used for reconstruction\n            transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n            f (float): Focal length in pixels\n            cx (float): Principle point x (pixels) from center\n            cy (float): Principle point y (pixels) from center\n            image_width (int): Input image width pixels\n            image_height (int): Input image height pixels\n            distortion_params (dict, optional): Distortion parameters, currently unused\n            lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n        \"\"\"\n        self.image_filename = image_filename\n        self.cam_to_world_transform = cam_to_world_transform\n        self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n        self.f = f\n        self.cx = cx\n        self.cy = cy\n        self.image_width = image_width\n        self.image_height = image_height\n        self.distortion_params = distortion_params\n        self.local_to_epsg_4978_transform = local_to_epsg_4978_transform\n\n        if lon_lat is None:\n            self.lon_lat = (None, None)\n        else:\n            self.lon_lat = lon_lat\n\n        self.image_size = (image_height, image_width)\n        self.image = None\n        self.cache_image = (\n            False  # Only set to true if you can hold all images in memory\n        )\n\n    def get_camera_hash(self, include_image_hash: bool = False):\n        \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n        Args:\n            include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n        Returns:\n            int: A hash value representing the current state of the camera\n        \"\"\"\n        # Geometric information of hash\n        transform_hash = self.cam_to_world_transform.tolist()\n        camera_settings = {\n            \"transform\": transform_hash,\n            \"f\": self.f,\n            \"cx\": self.cx,\n            \"cy\": self.cy,\n            \"image_width\": self.image_width,\n            \"image_height\": self.image_height,\n            \"distortion_params\": self.distortion_params,\n            \"lon_lat\": self.lon_lat,\n        }\n\n        # Include the image associated with the hash if specified\n        if include_image_hash:\n            camera_settings[\"image_filename\"] = str(self.image_filename)\n\n        camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n        hasher = hashlib.sha256()\n        hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n        return hasher.hexdigest()\n\n    def get_camera_properties(self):\n        \"\"\"Returns the properties about a camera.\n\n        Returns:\n            dict: A dictionary containing the focal length, principal point coordinates,\n                image height, image width, distortion parameters, and world_to_cam_transform.\n        \"\"\"\n        camera_properties = {\n            \"focal_length\": self.f,\n            \"principal_point_x\": self.cx,\n            \"principal_point_y\": self.cy,\n            \"image_height\": self.image_height,\n            \"image_width\": self.image_width,\n            \"distortion_params\": self.distortion_params,\n            \"world_to_cam_transform\": self.world_to_cam_transform,\n        }\n        return camera_properties\n\n    def get_image(self, image_scale: float = 1.0) -&gt; np.ndarray:\n        # Check if the image is cached\n        if self.image is None:\n            image = imread(self.image_filename)\n            if image.dtype == np.uint8:\n                image = image / 255.0\n\n            # Avoid unneccesary read if we have memory\n            if self.cache_image:\n                self.image = image\n        else:\n            image = self.image\n\n        # Resizing is never cached, consider revisiting\n        if image_scale != 1.0:\n            image = resize(\n                image,\n                (int(image.shape[0] * image_scale), int(image.shape[1] * image_scale)),\n            )\n\n        return image\n\n    def get_image_filename(self):\n        return self.image_filename\n\n    def get_image_size(self, image_scale=1.0):\n        \"\"\"Return image size, potentially scaled\n\n        Args:\n            image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n        Returns:\n            tuple[int]: (h, w) in pixels\n        \"\"\"\n        # We should never have to deal with other cases if the reported size is accurate\n        if self.image_size is not None:\n            pass\n        elif self.image is not None:\n            self.image_size = self.image.shape[:2]\n        else:\n            image = self.get_image()\n            self.image_size = image.shape[:2]\n\n        return (\n            int(self.image_size[0] * image_scale),\n            int(self.image_size[1] * image_scale),\n        )\n\n    def get_lon_lat(self, negate_easting=True):\n        \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n        if None in self.lon_lat:\n            self.lon_lat = get_GPS_exif(self.image_filename)\n\n            if negate_easting:\n                self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n        return self.lon_lat\n\n    def get_camera_location(self, get_z_coordinate: bool = False):\n        \"\"\"Returns a tuple of camera coordinates from the camera-to-world transfromation matrix.\n        Args:\n            get_z_coordinate (bool):\n                Flag that user can set if they want z-coordinates. Defaults to False.\n        Returns:\n            Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n        \"\"\"\n        return (\n            tuple(self.cam_to_world_transform[0:3, 3])\n            if get_z_coordinate\n            else tuple(self.cam_to_world_transform[0:2, 3])\n        )\n\n    def get_camera_view_angle(self, in_deg: bool = True) -&gt; tuple:\n        \"\"\"Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result\n\n        Args:\n            in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n        Returns:\n            tuple: (pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter\n        \"\"\"\n        # This is the origin, a point at one unit along the principal axis, a point one unit\n        # up (-Y), and a point one unit right (+X)\n        points_in_camera_frame = np.array(\n            [[0, 0, 0, 1], [0, 0, 1, 1], [0, -1, 0, 1], [1, 0, 0, 1]]\n        ).T\n\n        # Transform the points first into the world frame and then into the earth-centered,\n        # earth-fixed frame\n        points_in_ECEF = (\n            self.local_to_epsg_4978_transform\n            @ self.cam_to_world_transform\n            @ points_in_camera_frame\n        )\n        # Remove the homogenous coordinate and transpose\n        points_in_ECEF = points_in_ECEF[:-1].T\n        # Convert to shapely points\n        points_in_ECEF = [Point(*point) for point in points_in_ECEF]\n        # Convert to a dataframe\n        points_in_ECEF = gpd.GeoDataFrame(\n            geometry=points_in_ECEF, crs=EARTH_CENTERED_EARTH_FIXED_CRS\n        )\n\n        # Convert to lat lon\n        points_in_lat_lon = points_in_ECEF.to_crs(LAT_LON_CRS)\n        # Convert to a local projected CRS\n        points_in_projected_CRS = ensure_projected_CRS(points_in_lat_lon)\n        # Extract the geometry\n        points_in_projected_CRS = np.array(\n            [[p.x, p.y, p.z] for p in points_in_projected_CRS.geometry]\n        )\n\n        # Compute three vectors starting at the camera origin\n        view_vector = points_in_projected_CRS[1] - points_in_projected_CRS[0]\n        up_vector = points_in_projected_CRS[2] - points_in_projected_CRS[0]\n        right_vector = points_in_projected_CRS[3] - points_in_projected_CRS[0]\n\n        # The nadir vector points straight down\n        NADIR_VEC = np.array([0, 0, -1])\n\n        # For pitch, project the view vector onto the plane defined by the up vector and the nadir\n        pitch_projection_view_vec = projection_onto_plane(\n            view_vector, up_vector, NADIR_VEC\n        )\n        # For yaw, project the view vector onto the plane defined by the right vector and the nadir\n        yaw_projection_view_vec = projection_onto_plane(\n            view_vector, right_vector, NADIR_VEC\n        )\n\n        # Find the angle between these projected vectors and the nadir vector\n        pitch_angle = angle_between(pitch_projection_view_vec, NADIR_VEC)\n        yaw_angle = angle_between(yaw_projection_view_vec, NADIR_VEC)\n\n        # Return in degrees if requested\n        if in_deg:\n            return (np.rad2deg(pitch_angle), np.rad2deg(yaw_angle))\n        # Return in radians\n        return (pitch_angle, yaw_angle)\n\n    def check_projected_in_image(\n        self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n    ):\n        \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n        Args:\n            homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n            image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n        Returns:\n            np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n            np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n        \"\"\"\n        img_width, image_height = image_size\n\n        # Divide by the z coord to project onto the image plane\n        image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n        # Transpose for convenience, (n_points, 3)\n        image_space_points = image_space_points.T\n\n        # We only want to consider points in front of the camera. Simple projection cannot tell\n        # if a point is on the same ray behind the camera\n        in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n        # Check that the point is projected within the image and is in front of the camera\n        # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n        valid_points_bool = (\n            (image_space_points[:, 0] &gt; 0)\n            * (image_space_points[:, 1] &gt; 0)\n            * (image_space_points[:, 0] &lt; img_width)\n            * (image_space_points[:, 1] &lt; image_height)\n            * in_front_of_cam\n        )\n\n        # Extract the points that are valid\n        valid_image_space_points = image_space_points[valid_points_bool, :].to(\n            torch.int\n        )\n        # Return the boolean array\n        valid_points_bool = valid_points_bool.cpu().numpy()\n        valid_image_space_points = valid_image_space_points.cpu().numpy\n        return valid_points_bool, valid_image_space_points\n\n    def extract_colors(\n        self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n    ):\n        \"\"\"_summary_\n\n        Args:\n            valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n            valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n            img (np.ndarray): (h, w, n_channels) image to query from\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # Set up the data arrays\n        colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n        mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n        # Set the entries which are valid to false, meaning a valid entry in the masked array\n        # TODO see if I can use valid_bool directly instead\n        valid_inds = np.where(valid_bool)[0]\n        mask[valid_inds, :] = False\n\n        # Extract coordinates\n        i_locs = valid_locs[:, 1]\n        j_locs = valid_locs[:, 0]\n        # Index based on the coordinates\n        valid_color_samples = img[i_locs, j_locs, :]\n        # Insert the valid samples into the array at the valid locations\n        colors_per_vertex[valid_inds, :] = valid_color_samples\n        # Convert to a masked array\n        masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n        return masked_color_per_vertex\n\n    def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n        \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # [R|t] matrix\n        transform_3x4_world_to_cam = torch.Tensor(\n            self.world_to_cam_transform[:3, :]\n        ).to(device)\n        K = torch.Tensor(\n            [\n                [self.f, 0, self.image_width / 2.0 + self.cx],\n                [0, self.f, self.image_width + self.cy],\n                [0, 0, 1],\n            ],\n            device=device,\n        )\n        # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n        camera_matrix = K @ transform_3x4_world_to_cam\n\n        # Add the extra dimension of ones for matrix multiplication\n        homogenous_mesh_verts = torch.concatenate(\n            (\n                torch.Tensor(mesh_verts).to(device),\n                torch.ones((mesh_verts.shape[0], 1)).to(device),\n            ),\n            axis=1,\n        ).T\n\n        # TODO review terminology\n        homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n        # Determine what points project onto the image and at what locations\n        valid_bool, valid_locs = self.check_projected_in_image(\n            projected_verts=homogenous_camera_points,\n            image_size=(self.image_width, self.image_height),\n        )\n        # Extract corresponding colors from the image\n        colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n        return colors_per_vertex\n\n    def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n        \"\"\"\n        Get a pyvista camera at the location specified by photogrammetry.\n        Note that there is no principle point and only the vertical field of view is set\n\n        Args:\n            focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n        Returns:\n            pv.Camera: The pyvista camera from that viewpoint.\n        \"\"\"\n        # Instantiate a new camera\n        camera = pv.Camera()\n        # Get the position as the translational part of the transform\n        camera_position = self.cam_to_world_transform[:3, 3]\n        # Get the look point by transforming a ray along the camera's Z axis into world\n        # coordinates and then adding this to the location\n        camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n            (0, 0, focal_dist)\n        )\n        # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n        camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n        # Compute the vertical field of view\n        vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n        # Set the values\n        camera.focal_point = camera_look\n        camera.position = camera_position\n        camera.up = camera_up\n        camera.view_angle = vertical_FOV_angle\n\n        return camera\n\n    def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n        \"\"\"\n        Visualize the camera as a frustum, at the appropriate translation and\n        rotation and with the given focal length and aspect ratio.\n\n\n        Args:\n            plotter (pv.Plotter): The plotter to add the visualization to\n            frustum_scale (float, optional): The length of the frustum in world units. Defaults to 0.5.\n        \"\"\"\n        scaled_halfwidth = self.image_width / (self.f * 2)\n        scaled_halfheight = self.image_height / (self.f * 2)\n\n        scaled_cx = self.cx / self.f\n        scaled_cy = self.cy / self.f\n\n        right = scaled_cx + scaled_halfwidth\n        left = scaled_cx - scaled_halfwidth\n        top = scaled_cy + scaled_halfheight\n        bottom = scaled_cy - scaled_halfheight\n\n        vertices = (\n            np.array(\n                [\n                    [0, 0, 0],\n                    [\n                        right,\n                        top,\n                        1,\n                    ],\n                    [\n                        right,\n                        bottom,\n                        1,\n                    ],\n                    [\n                        left,\n                        bottom,\n                        1,\n                    ],\n                    [\n                        left,\n                        top,\n                        1,\n                    ],\n                ]\n            ).T\n            * frustum_scale\n        )\n        # Make the coordinates homogenous\n        vertices = np.vstack((vertices, np.ones((1, 5))))\n\n        # Project the vertices into the world cordinates\n        projected_vertices = self.cam_to_world_transform @ vertices\n\n        # Deal with the case where there is a scale transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        ## mesh faces\n        faces = np.hstack(\n            [\n                [3, 0, 1, 2],  # side\n                [3, 0, 2, 3],  # bottom\n                [3, 0, 3, 4],  # side\n                [3, 0, 4, 1],  # top\n                [3, 1, 2, 3],  # endcap tiangle #1\n                [3, 3, 4, 1],  # endcap tiangle #2\n            ]\n        )\n        # All blue except the top (-Y) surface is red\n        face_colors = np.array(\n            [[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]]\n        ).astype(float)\n\n        # Create a mesh for the camera frustum\n        frustum = pv.PolyData(projected_vertices[:3].T, faces)\n        # Unsure exactly what's going on here, but it's required for it to be valid\n        frustum.triangulate()\n        # Show the mesh with the given face colors\n        # TODO understand how this understands it's face vs. vertex colors? Simply by checking the number of values?\n        plotter.add_mesh(frustum, scalars=face_colors, rgb=True)\n\n    def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n        \"\"\"Compute rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n        Returns:\n            np.array: The projected vertices, TODO\n        \"\"\"\n        # Transform from i, j to x, y\n        pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n        # Cast a ray from the center of the mesh for vis\n        principal_point = np.array(\n            [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n        )\n        centered_pixel_coords = pixel_coords_xy - principal_point\n        scaled_pixel_coords = centered_pixel_coords / self.f\n\n        n_points = len(scaled_pixel_coords)\n\n        if n_points == 0:\n            return\n\n        line_verts = [\n            np.array(\n                [\n                    [0, 0, 0, 1],\n                    [\n                        point[0] * line_length,\n                        point[1] * line_length,\n                        line_length,\n                        1,\n                    ],\n                ]\n            )\n            for point in scaled_pixel_coords\n        ]\n        line_verts = np.concatenate(line_verts, axis=0).T\n\n        projected_vertices = self.cam_to_world_transform @ line_verts\n\n        # Handle scale in transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        projected_vertices = projected_vertices[:3, :].T\n\n        return projected_vertices\n\n    def vis_rays(\n        self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n    ):\n        \"\"\"Show rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            plotter (pv.Plotter): Plotter to use.\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n        \"\"\"\n        # If there are no detections, just skip it\n        if len(pixel_coords_ij) == 0:\n            return\n\n        projected_vertices = self.cast_rays(\n            pixel_coords_ij=pixel_coords_ij, line_length=line_length\n        )\n        n_points = int(projected_vertices.shape[0] / 2)\n\n        lines = np.vstack(\n            (\n                np.full(n_points, fill_value=2),\n                np.arange(0, 2 * n_points, 2),\n                np.arange(0, 2 * n_points, 2) + 1,\n            )\n        ).T\n\n        mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n        plotter.add_mesh(mesh)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera-functions","title":"Functions","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.__init__","title":"<code>__init__(image_filename, cam_to_world_transform, f, cx, cy, image_width, image_height, distortion_params={}, lon_lat=None, local_to_epsg_4978_transform=None)</code>","text":"<p>Represents the information about one camera location/image as determined by photogrammetry</p> <p>Parameters:</p> Name Type Description Default <code>image_filename</code> <code>PATH_TYPE</code> <p>The image used for reconstruction</p> required <code>transform</code> <code>ndarray</code> <p>A 4x4 transform representing the camera-to-world transform</p> required <code>f</code> <code>float</code> <p>Focal length in pixels</p> required <code>cx</code> <code>float</code> <p>Principle point x (pixels) from center</p> required <code>cy</code> <code>float</code> <p>Principle point y (pixels) from center</p> required <code>image_width</code> <code>int</code> <p>Input image width pixels</p> required <code>image_height</code> <code>int</code> <p>Input image height pixels</p> required <code>distortion_params</code> <code>dict</code> <p>Distortion parameters, currently unused</p> <code>{}</code> <code>lon_lat</code> <code>Union[None, Tuple[float, float]]</code> <p>Location, defaults to None</p> <code>None</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    image_filename: PATH_TYPE,\n    cam_to_world_transform: np.ndarray,\n    f: float,\n    cx: float,\n    cy: float,\n    image_width: int,\n    image_height: int,\n    distortion_params: Dict[str, float] = {},\n    lon_lat: Union[None, Tuple[float, float]] = None,\n    local_to_epsg_4978_transform: Union[np.array, None] = None,\n):\n    \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n    Args:\n        image_filename (PATH_TYPE): The image used for reconstruction\n        transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n        f (float): Focal length in pixels\n        cx (float): Principle point x (pixels) from center\n        cy (float): Principle point y (pixels) from center\n        image_width (int): Input image width pixels\n        image_height (int): Input image height pixels\n        distortion_params (dict, optional): Distortion parameters, currently unused\n        lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n    \"\"\"\n    self.image_filename = image_filename\n    self.cam_to_world_transform = cam_to_world_transform\n    self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n    self.f = f\n    self.cx = cx\n    self.cy = cy\n    self.image_width = image_width\n    self.image_height = image_height\n    self.distortion_params = distortion_params\n    self.local_to_epsg_4978_transform = local_to_epsg_4978_transform\n\n    if lon_lat is None:\n        self.lon_lat = (None, None)\n    else:\n        self.lon_lat = lon_lat\n\n    self.image_size = (image_height, image_width)\n    self.image = None\n    self.cache_image = (\n        False  # Only set to true if you can hold all images in memory\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.cast_rays","title":"<code>cast_rays(pixel_coords_ij, line_length=10)</code>","text":"<p>Compute rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> <p>Returns:</p> Type Description <p>np.array: The projected vertices, TODO</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n    \"\"\"Compute rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n    Returns:\n        np.array: The projected vertices, TODO\n    \"\"\"\n    # Transform from i, j to x, y\n    pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n    # Cast a ray from the center of the mesh for vis\n    principal_point = np.array(\n        [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n    )\n    centered_pixel_coords = pixel_coords_xy - principal_point\n    scaled_pixel_coords = centered_pixel_coords / self.f\n\n    n_points = len(scaled_pixel_coords)\n\n    if n_points == 0:\n        return\n\n    line_verts = [\n        np.array(\n            [\n                [0, 0, 0, 1],\n                [\n                    point[0] * line_length,\n                    point[1] * line_length,\n                    line_length,\n                    1,\n                ],\n            ]\n        )\n        for point in scaled_pixel_coords\n    ]\n    line_verts = np.concatenate(line_verts, axis=0).T\n\n    projected_vertices = self.cam_to_world_transform @ line_verts\n\n    # Handle scale in transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    projected_vertices = projected_vertices[:3, :].T\n\n    return projected_vertices\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.check_projected_in_image","title":"<code>check_projected_in_image(homogenous_image_coords, image_size)</code>","text":"<p>Check if projected points are within the bound of the image and in front of camera</p> <p>Parameters:</p> Name Type Description Default <code>homogenous_image_coords</code> <code>ndarray</code> <p>The points after the application of K[R|t]. (3, n_points)</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>The size of the image (width, height) in pixels</p> required <p>Returns:</p> Type Description <p>np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)</p> <p>np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def check_projected_in_image(\n    self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n):\n    \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n    Args:\n        homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n        image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n    Returns:\n        np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n        np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n    \"\"\"\n    img_width, image_height = image_size\n\n    # Divide by the z coord to project onto the image plane\n    image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n    # Transpose for convenience, (n_points, 3)\n    image_space_points = image_space_points.T\n\n    # We only want to consider points in front of the camera. Simple projection cannot tell\n    # if a point is on the same ray behind the camera\n    in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n    # Check that the point is projected within the image and is in front of the camera\n    # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n    valid_points_bool = (\n        (image_space_points[:, 0] &gt; 0)\n        * (image_space_points[:, 1] &gt; 0)\n        * (image_space_points[:, 0] &lt; img_width)\n        * (image_space_points[:, 1] &lt; image_height)\n        * in_front_of_cam\n    )\n\n    # Extract the points that are valid\n    valid_image_space_points = image_space_points[valid_points_bool, :].to(\n        torch.int\n    )\n    # Return the boolean array\n    valid_points_bool = valid_points_bool.cpu().numpy()\n    valid_image_space_points = valid_image_space_points.cpu().numpy\n    return valid_points_bool, valid_image_space_points\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.extract_colors","title":"<code>extract_colors(valid_bool, valid_locs, img)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>valid_bool</code> <code>ndarray</code> <p>(n_points,) boolean array cooresponding to valid points</p> required <code>valid_locs</code> <code>ndarray</code> <p>(n_valid, 2) float array of image-space locations (x,y)</p> required <code>img</code> <code>ndarray</code> <p>(h, w, n_channels) image to query from</p> required <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def extract_colors(\n    self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n):\n    \"\"\"_summary_\n\n    Args:\n        valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n        valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n        img (np.ndarray): (h, w, n_channels) image to query from\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # Set up the data arrays\n    colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n    mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n    # Set the entries which are valid to false, meaning a valid entry in the masked array\n    # TODO see if I can use valid_bool directly instead\n    valid_inds = np.where(valid_bool)[0]\n    mask[valid_inds, :] = False\n\n    # Extract coordinates\n    i_locs = valid_locs[:, 1]\n    j_locs = valid_locs[:, 0]\n    # Index based on the coordinates\n    valid_color_samples = img[i_locs, j_locs, :]\n    # Insert the valid samples into the array at the valid locations\n    colors_per_vertex[valid_inds, :] = valid_color_samples\n    # Convert to a masked array\n    masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n    return masked_color_per_vertex\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_hash","title":"<code>get_camera_hash(include_image_hash=False)</code>","text":"<p>Generates a hash value for the camera's geometry and optionally includes the image</p> <p>Parameters:</p> Name Type Description Default <code>include_image_hash</code> <code>bool</code> <p>Whether to include the image filename in the hash computation. Defaults to false.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <p>A hash value representing the current state of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_hash(self, include_image_hash: bool = False):\n    \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n    Args:\n        include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n    Returns:\n        int: A hash value representing the current state of the camera\n    \"\"\"\n    # Geometric information of hash\n    transform_hash = self.cam_to_world_transform.tolist()\n    camera_settings = {\n        \"transform\": transform_hash,\n        \"f\": self.f,\n        \"cx\": self.cx,\n        \"cy\": self.cy,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"distortion_params\": self.distortion_params,\n        \"lon_lat\": self.lon_lat,\n    }\n\n    # Include the image associated with the hash if specified\n    if include_image_hash:\n        camera_settings[\"image_filename\"] = str(self.image_filename)\n\n    camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n    hasher = hashlib.sha256()\n    hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n    return hasher.hexdigest()\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_location","title":"<code>get_camera_location(get_z_coordinate=False)</code>","text":"<p>Returns a tuple of camera coordinates from the camera-to-world transfromation matrix. Args:     get_z_coordinate (bool):         Flag that user can set if they want z-coordinates. Defaults to False. Returns:     Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_location(self, get_z_coordinate: bool = False):\n    \"\"\"Returns a tuple of camera coordinates from the camera-to-world transfromation matrix.\n    Args:\n        get_z_coordinate (bool):\n            Flag that user can set if they want z-coordinates. Defaults to False.\n    Returns:\n        Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n    \"\"\"\n    return (\n        tuple(self.cam_to_world_transform[0:3, 3])\n        if get_z_coordinate\n        else tuple(self.cam_to_world_transform[0:2, 3])\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_properties","title":"<code>get_camera_properties()</code>","text":"<p>Returns the properties about a camera.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the focal length, principal point coordinates, image height, image width, distortion parameters, and world_to_cam_transform.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_properties(self):\n    \"\"\"Returns the properties about a camera.\n\n    Returns:\n        dict: A dictionary containing the focal length, principal point coordinates,\n            image height, image width, distortion parameters, and world_to_cam_transform.\n    \"\"\"\n    camera_properties = {\n        \"focal_length\": self.f,\n        \"principal_point_x\": self.cx,\n        \"principal_point_y\": self.cy,\n        \"image_height\": self.image_height,\n        \"image_width\": self.image_width,\n        \"distortion_params\": self.distortion_params,\n        \"world_to_cam_transform\": self.world_to_cam_transform,\n    }\n    return camera_properties\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_view_angle","title":"<code>get_camera_view_angle(in_deg=True)</code>","text":"<p>Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result</p> <p>Parameters:</p> Name Type Description Default <code>in_deg</code> <code>bool</code> <p>Return the angles in degrees rather than radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_view_angle(self, in_deg: bool = True) -&gt; tuple:\n    \"\"\"Get the off-nadir pitch and yaw angles, computed geometrically from the photogrammtery result\n\n    Args:\n        in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n    Returns:\n        tuple: (pitch-from-nadir, yaw-from-nadir). Units are defined by in_deg parameter\n    \"\"\"\n    # This is the origin, a point at one unit along the principal axis, a point one unit\n    # up (-Y), and a point one unit right (+X)\n    points_in_camera_frame = np.array(\n        [[0, 0, 0, 1], [0, 0, 1, 1], [0, -1, 0, 1], [1, 0, 0, 1]]\n    ).T\n\n    # Transform the points first into the world frame and then into the earth-centered,\n    # earth-fixed frame\n    points_in_ECEF = (\n        self.local_to_epsg_4978_transform\n        @ self.cam_to_world_transform\n        @ points_in_camera_frame\n    )\n    # Remove the homogenous coordinate and transpose\n    points_in_ECEF = points_in_ECEF[:-1].T\n    # Convert to shapely points\n    points_in_ECEF = [Point(*point) for point in points_in_ECEF]\n    # Convert to a dataframe\n    points_in_ECEF = gpd.GeoDataFrame(\n        geometry=points_in_ECEF, crs=EARTH_CENTERED_EARTH_FIXED_CRS\n    )\n\n    # Convert to lat lon\n    points_in_lat_lon = points_in_ECEF.to_crs(LAT_LON_CRS)\n    # Convert to a local projected CRS\n    points_in_projected_CRS = ensure_projected_CRS(points_in_lat_lon)\n    # Extract the geometry\n    points_in_projected_CRS = np.array(\n        [[p.x, p.y, p.z] for p in points_in_projected_CRS.geometry]\n    )\n\n    # Compute three vectors starting at the camera origin\n    view_vector = points_in_projected_CRS[1] - points_in_projected_CRS[0]\n    up_vector = points_in_projected_CRS[2] - points_in_projected_CRS[0]\n    right_vector = points_in_projected_CRS[3] - points_in_projected_CRS[0]\n\n    # The nadir vector points straight down\n    NADIR_VEC = np.array([0, 0, -1])\n\n    # For pitch, project the view vector onto the plane defined by the up vector and the nadir\n    pitch_projection_view_vec = projection_onto_plane(\n        view_vector, up_vector, NADIR_VEC\n    )\n    # For yaw, project the view vector onto the plane defined by the right vector and the nadir\n    yaw_projection_view_vec = projection_onto_plane(\n        view_vector, right_vector, NADIR_VEC\n    )\n\n    # Find the angle between these projected vectors and the nadir vector\n    pitch_angle = angle_between(pitch_projection_view_vec, NADIR_VEC)\n    yaw_angle = angle_between(yaw_projection_view_vec, NADIR_VEC)\n\n    # Return in degrees if requested\n    if in_deg:\n        return (np.rad2deg(pitch_angle), np.rad2deg(yaw_angle))\n    # Return in radians\n    return (pitch_angle, yaw_angle)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_image_size","title":"<code>get_image_size(image_scale=1.0)</code>","text":"<p>Return image size, potentially scaled</p> <p>Parameters:</p> Name Type Description Default <code>image_scale</code> <code>float</code> <p>How much to scale by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>tuple[int]: (h, w) in pixels</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_image_size(self, image_scale=1.0):\n    \"\"\"Return image size, potentially scaled\n\n    Args:\n        image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n    Returns:\n        tuple[int]: (h, w) in pixels\n    \"\"\"\n    # We should never have to deal with other cases if the reported size is accurate\n    if self.image_size is not None:\n        pass\n    elif self.image is not None:\n        self.image_size = self.image.shape[:2]\n    else:\n        image = self.get_image()\n        self.image_size = image.shape[:2]\n\n    return (\n        int(self.image_size[0] * image_scale),\n        int(self.image_size[1] * image_scale),\n    )\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_lon_lat","title":"<code>get_lon_lat(negate_easting=True)</code>","text":"<p>Return the lon, lat tuple, reading from exif metadata if neccessary</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat(self, negate_easting=True):\n    \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n    if None in self.lon_lat:\n        self.lon_lat = get_GPS_exif(self.image_filename)\n\n        if negate_easting:\n            self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n    return self.lon_lat\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_pyvista_camera","title":"<code>get_pyvista_camera(focal_dist=10)</code>","text":"<p>Get a pyvista camera at the location specified by photogrammetry. Note that there is no principle point and only the vertical field of view is set</p> <p>Parameters:</p> Name Type Description Default <code>focal_dist</code> <code>float</code> <p>How far away from the camera the center point should be. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Camera</code> <p>pv.Camera: The pyvista camera from that viewpoint.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n    \"\"\"\n    Get a pyvista camera at the location specified by photogrammetry.\n    Note that there is no principle point and only the vertical field of view is set\n\n    Args:\n        focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n    Returns:\n        pv.Camera: The pyvista camera from that viewpoint.\n    \"\"\"\n    # Instantiate a new camera\n    camera = pv.Camera()\n    # Get the position as the translational part of the transform\n    camera_position = self.cam_to_world_transform[:3, 3]\n    # Get the look point by transforming a ray along the camera's Z axis into world\n    # coordinates and then adding this to the location\n    camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n        (0, 0, focal_dist)\n    )\n    # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n    camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n    # Compute the vertical field of view\n    vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n    # Set the values\n    camera.focal_point = camera_look\n    camera.position = camera_position\n    camera.up = camera_up\n    camera.view_angle = vertical_FOV_angle\n\n    return camera\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.project_mesh_verts","title":"<code>project_mesh_verts(mesh_verts, img, device)</code>","text":"<p>Get a color per vertex using only projective geometry, without considering occlusion or distortion</p> <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n    \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # [R|t] matrix\n    transform_3x4_world_to_cam = torch.Tensor(\n        self.world_to_cam_transform[:3, :]\n    ).to(device)\n    K = torch.Tensor(\n        [\n            [self.f, 0, self.image_width / 2.0 + self.cx],\n            [0, self.f, self.image_width + self.cy],\n            [0, 0, 1],\n        ],\n        device=device,\n    )\n    # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n    camera_matrix = K @ transform_3x4_world_to_cam\n\n    # Add the extra dimension of ones for matrix multiplication\n    homogenous_mesh_verts = torch.concatenate(\n        (\n            torch.Tensor(mesh_verts).to(device),\n            torch.ones((mesh_verts.shape[0], 1)).to(device),\n        ),\n        axis=1,\n    ).T\n\n    # TODO review terminology\n    homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n    # Determine what points project onto the image and at what locations\n    valid_bool, valid_locs = self.check_projected_in_image(\n        projected_verts=homogenous_camera_points,\n        image_size=(self.image_width, self.image_height),\n    )\n    # Extract corresponding colors from the image\n    colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n    return colors_per_vertex\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis","title":"<code>vis(plotter=None, frustum_scale=0.1)</code>","text":"<p>Visualize the camera as a frustum, at the appropriate translation and rotation and with the given focal length and aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>The plotter to add the visualization to</p> <code>None</code> <code>frustum_scale</code> <code>float</code> <p>The length of the frustum in world units. Defaults to 0.5.</p> <code>0.1</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n    \"\"\"\n    Visualize the camera as a frustum, at the appropriate translation and\n    rotation and with the given focal length and aspect ratio.\n\n\n    Args:\n        plotter (pv.Plotter): The plotter to add the visualization to\n        frustum_scale (float, optional): The length of the frustum in world units. Defaults to 0.5.\n    \"\"\"\n    scaled_halfwidth = self.image_width / (self.f * 2)\n    scaled_halfheight = self.image_height / (self.f * 2)\n\n    scaled_cx = self.cx / self.f\n    scaled_cy = self.cy / self.f\n\n    right = scaled_cx + scaled_halfwidth\n    left = scaled_cx - scaled_halfwidth\n    top = scaled_cy + scaled_halfheight\n    bottom = scaled_cy - scaled_halfheight\n\n    vertices = (\n        np.array(\n            [\n                [0, 0, 0],\n                [\n                    right,\n                    top,\n                    1,\n                ],\n                [\n                    right,\n                    bottom,\n                    1,\n                ],\n                [\n                    left,\n                    bottom,\n                    1,\n                ],\n                [\n                    left,\n                    top,\n                    1,\n                ],\n            ]\n        ).T\n        * frustum_scale\n    )\n    # Make the coordinates homogenous\n    vertices = np.vstack((vertices, np.ones((1, 5))))\n\n    # Project the vertices into the world cordinates\n    projected_vertices = self.cam_to_world_transform @ vertices\n\n    # Deal with the case where there is a scale transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    ## mesh faces\n    faces = np.hstack(\n        [\n            [3, 0, 1, 2],  # side\n            [3, 0, 2, 3],  # bottom\n            [3, 0, 3, 4],  # side\n            [3, 0, 4, 1],  # top\n            [3, 1, 2, 3],  # endcap tiangle #1\n            [3, 3, 4, 1],  # endcap tiangle #2\n        ]\n    )\n    # All blue except the top (-Y) surface is red\n    face_colors = np.array(\n        [[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]]\n    ).astype(float)\n\n    # Create a mesh for the camera frustum\n    frustum = pv.PolyData(projected_vertices[:3].T, faces)\n    # Unsure exactly what's going on here, but it's required for it to be valid\n    frustum.triangulate()\n    # Show the mesh with the given face colors\n    # TODO understand how this understands it's face vs. vertex colors? Simply by checking the number of values?\n    plotter.add_mesh(frustum, scalars=face_colors, rgb=True)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis_rays","title":"<code>vis_rays(pixel_coords_ij, plotter, line_length=10)</code>","text":"<p>Show rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>plotter</code> <code>Plotter</code> <p>Plotter to use.</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis_rays(\n    self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n):\n    \"\"\"Show rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        plotter (pv.Plotter): Plotter to use.\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n    \"\"\"\n    # If there are no detections, just skip it\n    if len(pixel_coords_ij) == 0:\n        return\n\n    projected_vertices = self.cast_rays(\n        pixel_coords_ij=pixel_coords_ij, line_length=line_length\n    )\n    n_points = int(projected_vertices.shape[0] / 2)\n\n    lines = np.vstack(\n        (\n            np.full(n_points, fill_value=2),\n            np.arange(0, 2 * n_points, 2),\n            np.arange(0, 2 * n_points, 2) + 1,\n        )\n    ).T\n\n    mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n    plotter.add_mesh(mesh)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet","title":"<code>PhotogrammetryCameraSet</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCameraSet:\n    def __init__(\n        self,\n        cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n        cam_to_world_transforms: Union[None, List[np.ndarray]] = None,\n        intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n            0: EXAMPLE_INTRINSICS\n        },\n        image_filenames: Union[List[PATH_TYPE], None] = None,\n        lon_lats: Union[None, List[Union[None, Tuple[float, float]]]] = None,\n        image_folder: PATH_TYPE = None,\n        sensor_IDs: List[int] = None,\n        validate_images: bool = False,\n        local_to_epsg_4978_transform: Union[np.array, None] = None,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n            intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n            image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n            lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n            image_folder (PATH_TYPE): The top level folder of the images\n            sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n            validate_images (bool, optional): Should the existance of the images be checked. Defaults to False.\n\n        Raises:\n            ValueError: _description_\n        \"\"\"\n        # Create an object using the supplied cameras\n        if cameras is not None:\n            if isinstance(cameras, PhotogrammetryCamera):\n                cameras = [cameras]\n            self.cameras = cameras\n            return\n\n        # Standardization\n        n_transforms = len(cam_to_world_transforms)\n\n        # Create list of Nones for image filenames if not set\n        if image_filenames is None:\n            image_filenames = [None] * n_transforms\n\n        if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n            # Create a list of the only index if not set\n            sensor_IDs = [\n                list(intrinsic_params_per_sensor_type.keys())[0]\n            ] * n_transforms\n        elif len(sensor_IDs) != n_transforms:\n            raise ValueError(\n                f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n            )\n\n        # If lon lats is None, set it to a list of Nones per transform\n        if lon_lats is None:\n            lon_lats = [None] * n_transforms\n\n        if image_folder is None:\n            # TODO set it to the least common ancestor of all filenames\n            pass\n\n        # Record the values\n        # TODO see if we ever use these\n        self.cam_to_world_transforms = cam_to_world_transforms\n        self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n        self.image_filenames = image_filenames\n        self.lon_lats = lon_lats\n        self.sensor_IDs = sensor_IDs\n        self.image_folder = image_folder\n\n        if validate_images:\n            missing_images, invalid_images = self.find_mising_images()\n            if len(missing_images) &gt; 0:\n                print(f\"Deleting {len(missing_images)} missing images\")\n                valid_images = np.where(np.logical_not(invalid_images))[0]\n                self.image_filenames = np.array(self.image_filenames)[\n                    valid_images\n                ].tolist()\n                # Avoid calling .tolist() because this will recursively set all elements to lists\n                # when this should be a list of np.arrays\n                self.cam_to_world_transforms = [\n                    x for x in np.array(self.cam_to_world_transforms)[valid_images]\n                ]\n                self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n                self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n        self.cameras = []\n\n        for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n            self.image_filenames,\n            self.cam_to_world_transforms,\n            self.sensor_IDs,\n            self.lon_lats,\n        ):\n            sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n            # This means the sensor did not have enough parameters to be valid\n            if sensor_params is None:\n                continue\n\n            new_camera = PhotogrammetryCamera(\n                image_filename,\n                cam_to_world_transform,\n                lon_lat=lon_lat,\n                local_to_epsg_4978_transform=local_to_epsg_4978_transform,\n                **sensor_params,\n            )\n            self.cameras.append(new_camera)\n\n    def __len__(self):\n        return self.n_cameras()\n\n    def __getitem__(self, slice):\n        subset_cameras = self.cameras[slice]\n        if isinstance(subset_cameras, PhotogrammetryCamera):\n            # this is just one item indexed\n            return subset_cameras\n        # else, wrap the list of cameras in a CameraSet\n        return PhotogrammetryCameraSet(subset_cameras)\n\n    def get_image_folder(self):\n        return self.image_folder\n\n    def find_mising_images(self):\n        invalid_mask = []\n        for image_file in self.image_filenames:\n            if not image_file.is_file():\n                invalid_mask.append(True)\n            else:\n                invalid_mask.append(False)\n        invalid_images = np.array(self.image_filenames)[np.array(invalid_mask)].tolist()\n\n        return invalid_images, invalid_mask\n\n    def n_cameras(self) -&gt; int:\n        \"\"\"Return the number of cameras\"\"\"\n        return len(self.cameras)\n\n    def n_image_channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the image\"\"\"\n        return 3\n\n    def get_cameras_in_folder(self, folder: PATH_TYPE):\n        \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n        Args:\n            folder (PATH_TYPE): The folder location\n\n        Returns:\n            PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n        \"\"\"\n        # Get the inds where that camera is in the folder\n        imgs_in_folder_inds = [\n            i\n            for i in range(len(self.cameras))\n            if self.cameras[i].image_filename.is_relative_to(folder)\n        ]\n        # Return the PhotogrammetryCameraSet with those subset of cameras\n        subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n        return subset_cameras\n\n    def get_cameras_matching_filename_regex(\n        self, filename_regex: str\n    ) -&gt; \"PhotogrammetryCameraSet\":\n        \"\"\"Return the subset of cameras who's filenames match the provided regex\n\n        Args:\n            filename_regex (str): Regular expression passed to 're' engine\n\n        Returns:\n            PhotogrammetryCameraSet: Subset of cameras matching the regex\n        \"\"\"\n        # Compute boolean array for which ones match\n        imgs_matching_regex = [\n            bool(re.search(filename_regex, str(filename)))\n            for filename in self.image_filenames\n        ]\n        # Convert to integer indices within the set\n        imgs_matching_regex_inds = np.where(imgs_matching_regex)[0]\n\n        # Get the corresponding subset\n        subset_cameras = self.get_subset_cameras(imgs_matching_regex_inds)\n        return subset_cameras\n\n    def get_subset_cameras(self, inds: List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set[i] for i in inds]\n        return subset_camera_set\n\n    def get_image_by_index(self, index: int, image_scale: float = 1.0) -&gt; np.ndarray:\n        return self[index].get_image(image_scale=image_scale)\n\n    def get_camera_view_angles(self, in_deg: bool = True) -&gt; List[Tuple[float]]:\n        \"\"\"Compute the pitch and yaw off-nadir for all cameras in the set\n\n        Args:\n            in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n        Returns:\n            List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.\n        \"\"\"\n        return [\n            camera.get_camera_view_angle(in_deg=in_deg)\n            for camera in tqdm(self.cameras, desc=\"Computing view angles\")\n        ]\n\n    def get_image_filename(self, index: Union[int, None], absolute=True):\n        \"\"\"Get the image filename(s) based on the index\n\n        Args:\n            index (Union[int, None]):\n                Return the filename of the camera at this index, or all filenames if None.\n                #TODO update to support lists of integer indices as well\n            absolute (bool, optional):\n                Return the absolute filepath, as oposed to the path relative to the image folder.\n                Defaults to True.\n\n        Returns:\n            typing.Union[PATH_TYPE, list[PATH_TYPE]]:\n                If an integer index is provided, one path will be returned. If None, a list of paths\n                will be returned.\n        \"\"\"\n        if index is None:\n            return [\n                self.get_image_filename(i, absolute=absolute)\n                for i in range(len(self.cameras))\n            ]\n\n        filename = self.cameras[index].get_image_filename()\n        if absolute:\n            return Path(filename)\n        else:\n            return Path(filename).relative_to(self.get_image_folder())\n\n    def save_images(self, output_folder, copy=False, remove_folder: bool = True):\n        if remove_folder:\n            if os.path.isdir(output_folder):\n                print(f\"about to remove {output_folder}\")\n                shutil.rmtree(output_folder)\n\n        for i in tqdm(\n            range(len(self.cameras)),\n            f\"{'copying' if copy else 'linking'} images to {output_folder}\",\n        ):\n            output_file = Path(\n                output_folder, self.get_image_filename(i, absolute=False)\n            )\n            ensure_containing_folder(output_file)\n            src_file = self.get_image_filename(i, absolute=True)\n            if copy:\n                try:\n                    shutil.copy(src_file, output_file)\n                except FileNotFoundError:\n                    logging.warning(f\"Could not find {src_file}\")\n            else:\n                os.symlink(src_file, output_file)\n\n    def get_lon_lat_coords(self):\n        \"\"\"Returns a list of GPS coords for each camera\"\"\"\n        return [x.get_lon_lat() for x in self.cameras]\n\n    def get_camera_locations(self, **kwargs):\n        \"\"\"\n        Returns a list of camera locations for each camera.\n\n        Args:\n            **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n        Returns:\n            List[Tuple[float, float] or Tuple[float, float, float]]:\n                List of tuples containing the camera locations.\n        \"\"\"\n        return [x.get_camera_location(**kwargs) for x in self.cameras]\n\n    def get_subset_ROI(\n        self,\n        ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n        buffer_radius: float = 0,\n        is_geospatial: bool = None,\n    ):\n        \"\"\"Return cameras that are within a radius of the provided geometry\n\n        Args:\n            geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n                This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n                Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n            buffer_radius (float, optional):\n                Return points within this buffer of the geometry. Defaults to 0. Represents\n                meters if ROI is geospatial.\n            is_geospatial (bool, optional):\n                A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n                the flag is set if the provided geodata has a CRS.\n        Returns:\n            subset_camera_set (List[PhotogrammetryCamera]):\n                List of cameras that fall within the provided ROI\n        \"\"\"\n        # construct GeoDataFrame if not provided\n        if isinstance(ROI, (Polygon, MultiPolygon)):\n            # assume geodata is lat/lon if is_geospatial is True\n            if is_geospatial:\n                ROI = gpd.GeoDataFrame(crs=LAT_LON_CRS, geometry=[ROI])\n            else:\n                ROI = gpd.GeoDataFrame(geometry=[ROI])\n        elif not isinstance(ROI, gpd.GeoDataFrame):\n            # Read in the geofile\n            ROI = gpd.read_file(ROI)\n\n        if is_geospatial is None:\n            is_geospatial = ROI.crs is not None\n\n        if not is_geospatial:\n            # get internal coordinate system camera locations\n            image_locations = [Point(*x) for x in self.get_camera_locations()]\n            image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n        else:\n            # Make sure it's a geometric (meters-based) CRS\n            ROI = ensure_projected_CRS(ROI)\n            # Read the locations of all the points\n            image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n            # Create a dataframe, assuming inputs are lat lon\n            image_locations_df = gpd.GeoDataFrame(\n                geometry=image_locations, crs=LAT_LON_CRS\n            )\n            image_locations_df.to_crs(ROI.crs, inplace=True)\n\n        # Drop all the fields except the geometry for computational reasons\n        ROI = ROI[\"geometry\"]\n        # Buffer out by the requested distance\n        ROI = ROI.buffer(buffer_radius)\n        # Merge the potentially-individual polygons to one\n        # TODO Do experiments to see if this step should be before or after the buffer.\n        # Counterintuitively, it seems that after is faster\n        ROI = unary_union(ROI.tolist())\n\n        # Determine the binary mask for which cameras are within the ROI\n        cameras_in_ROI = image_locations_df.within(ROI).to_numpy()\n        # Convert to the integer indices\n        cameras_in_ROI = np.where(cameras_in_ROI)[0]\n        # Get the corresponding subset of cameras\n        subset_camera_set = self.get_subset_cameras(cameras_in_ROI)\n        return subset_camera_set\n\n    def triangulate_detections(\n        self,\n        detector: TabularRectangleSegmentor,\n        transform_to_epsg_4978=None,\n        similarity_threshold_meters: float = 0.1,\n        louvain_resolution: float = 2,\n        vis: bool = True,\n        plotter: pv.Plotter = pv.Plotter(),\n        vis_ray_length_meters: float = 200,\n    ) -&gt; np.ndarray:\n        \"\"\"Take per-image detections and triangulate them to 3D locations\n\n        Args:\n            detector (TabularRectangleSegmentor):\n                Produces detections per image using the get_detection_centers method\n            transform_to_epsg_4978 (typing.Union[np.ndarray, None], optional):\n                The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.\n            similarity_threshold_meters (float, optional):\n                Consider rays a potential match if the distance between them is less than this\n                value. Defaults to 0.1.\n            louvain_resolution (float, optional):\n                The resolution parameter of the networkx.louvain_communities function. Defaults to\n                2.0.\n            vis (bool, optional):\n                Whether to show the detection projections and intersecting points. Defaults to True.\n            plotter (pv.Plotter, optional):\n                The plotter to add the visualizations to is vis=True. If not set, a new one will be\n                created. Defaults to pv.Plotter().\n            vis_ray_length_meters (float, optional):\n                The length of the visualized rays in meters. Defaults to 200.\n\n        Returns:\n            np.ndarray:\n                (n unique objects, 3), the 3D locations of the identified objects.\n                If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the\n                local coordinate system of the mesh\n        \"\"\"\n        # Determine scale factor relating meters to internal coordinates\n        meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n        similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n        vis_ray_length_local = vis_ray_length_meters * meters_to_local_scale\n\n        # Record the lines corresponding to each detection and the associated image ID\n        all_line_segments = []\n        all_image_IDs = []\n\n        # Iterate over the cameras\n        for camera_ind in range(len(self)):\n            # Get the image filename\n            image_filename = str(self.get_image_filename(camera_ind, absolute=False))\n            # Get the centers of associated detection from the detector\n            # TODO, this only works with \"detectors\" that can look up the detections based on the\n            # filename alone. In the future we might want to support real detectors that actually\n            # use the image.\n            detection_centers_pixels = detector.get_detection_centers(image_filename)\n            # Get the individual camera\n            camera = self[camera_ind]\n            # Project rays given the locations of the detections in pixel coordinates\n            line_segments = camera.cast_rays(\n                pixel_coords_ij=detection_centers_pixels,\n                line_length=vis_ray_length_local,\n            )\n            # If there are no detections, this will be None\n            if line_segments is not None:\n                # Record the line segments, which will be ordered as alternating (start, end) rows\n                all_line_segments.append(line_segments)\n                # Record which image ID generated each line\n                all_image_IDs.append(\n                    np.full(int(line_segments.shape[0] / 2), fill_value=camera_ind)\n                )\n\n        # Concatenate the lists of arrays into a single array\n        all_line_segments = np.concatenate(all_line_segments, axis=0)\n        all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n        # Get the starts and ends, which are alternating rows\n        ray_starts = all_line_segments[0::2]\n        segment_ends = all_line_segments[1::2]\n        # Determine the direction\n        ray_directions = segment_ends - ray_starts\n        # Make the ray directions unit length\n        ray_directions = ray_directions / np.linalg.norm(\n            ray_directions, axis=1, keepdims=True\n        )\n\n        # Compute the distance matrix of ray-ray intersections\n        num_dets = ray_starts.shape[0]\n        interesection_dists = np.full((num_dets, num_dets), fill_value=np.nan)\n\n        # Calculate the upper triangular matrix of ray-ray interesections\n        for i in tqdm(range(num_dets), desc=\"Calculating quality of ray intersections\"):\n            for j in range(i, num_dets):\n                # Extract starts and directions\n                a0 = ray_starts[i]\n                a1 = segment_ends[i]\n                b0 = ray_starts[j]\n                b1 = segment_ends[j]\n                # TODO explore whether this could be vectorized\n                _, _, dist = compute_approximate_ray_intersection(a0, a1, b0, b1)\n\n                interesection_dists[i, j] = dist\n\n        # Filter out intersections that are above the threshold distance\n        interesection_dists[interesection_dists &gt; similarity_threshold_local] = np.nan\n\n        # Determine which intersections are valid, represented by finite values\n        i_inds, j_inds = np.where(np.isfinite(interesection_dists))\n\n        # Build a list of (i, j, info_dict) tuples encoding the valid edges and their intersection\n        # distance\n        positive_edges = [\n            (i, j, {\"weight\": 1 / interesection_dists[i, j]})\n            for i, j in zip(i_inds, j_inds)\n        ]\n\n        # Build a networkx graph. The nodes represent an individual detection while the edges\n        # represent the quality of the matches between detections.\n        graph = networkx.Graph(positive_edges)\n        # Determine Louvain communities which are sets of nodes. Ideally, this represents a set of\n        # detections that all coorespond to one 3D object\n        communities = networkx.community.louvain_communities(\n            graph, weight=\"weight\", resolution=louvain_resolution\n        )\n        # Sort the communities by size\n        communities = sorted(communities, key=len, reverse=True)\n\n        ## Triangulate the rays for each community to identify the 3D location\n        community_points = []\n        # Record the community IDs per detection\n        community_IDs = np.full(num_dets, fill_value=np.nan)\n        # Iterate over communities\n        for community_ID, community in enumerate(communities):\n            # Get the indices of the detections for that community\n            community_detection_inds = np.array(list(community))\n            # Record the community ID for the corresponding detection IDs\n            community_IDs[community_detection_inds] = community_ID\n\n            # Get the set of starts and directions for that community\n            community_starts = ray_starts[community_detection_inds]\n            community_directions = ray_directions[community_detection_inds]\n\n            # Determine the least squares triangulation of the rays\n            community_3D_point = triangulate_rays_lstsq(\n                community_starts, community_directions\n            )\n            community_points.append(community_3D_point)\n\n        # Stack all of the points into one vector\n        community_points = np.vstack(community_points)\n\n        # Show the rays and detections\n        if vis:\n            # Show the line segements\n            # TODO: consider coloring these lines by community\n            lines_mesh = pv.line_segments_from_points(all_line_segments)\n            plotter.add_mesh(\n                lines_mesh,\n                scalars=community_IDs,\n                label=\"Rays, colored by community ID\",\n            )\n            # Show the triangulated communtities as red spheres\n            detected_points = pv.PolyData(community_points)\n            plotter.add_points(\n                detected_points,\n                color=\"r\",\n                render_points_as_spheres=True,\n                point_size=10,\n                label=\"Triangulated locations\",\n            )\n            plotter.add_legend()\n\n        # Convert the intersection points from the local mesh coordinate system to lat lon\n        if transform_to_epsg_4978 is not None:\n            # Append a column of all ones to make the homogenous coordinates\n            community_points_homogenous = np.concatenate(\n                [community_points, np.ones_like(community_points[:, 0:1])], axis=1\n            )\n            # Use the transform matrix to transform the points into the earth centered, earth fixed\n            # frame, EPSG:4978\n            community_points_epsg_4978 = (\n                transform_to_epsg_4978 @ community_points_homogenous.T\n            ).T\n            # Convert the points from earth centered, earth fixed frame to lat lon\n            community_points_lat_lon = convert_CRS_3D_points(\n                community_points_epsg_4978,\n                input_CRS=EARTH_CENTERED_EARTH_FIXED_CRS,\n                output_CRS=LAT_LON_CRS,\n            )\n            # Set the community points to lat lon\n            community_points = community_points_lat_lon\n\n        # Return the 3D locations of the community points\n        return community_points\n\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        add_orientation_cube: bool = False,\n        show: bool = False,\n        frustum_scale: float = None,\n        force_xvfb: bool = False,\n        interactive_jupyter: bool = False,\n    ):\n        \"\"\"Visualize all the cameras\n\n        Args:\n            plotter (pv.Plotter):\n                Plotter to add the cameras to. If None, will be created and then plotted\n            add_orientation_cube (bool, optional):\n                Add a cube to visualize the coordinate system. Defaults to False.\n            show (bool, optional):\n                Show the results instead of waiting for other content to be added\n            frustum_scale (float, optional):\n                Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n                between two cameras.\n            force_xvfb (bool, optional):\n                Force a headless rendering backend\n            interactive_jupyter (bool, optional):\n                Will allow you to interact with the visualization in your notebook if supported by\n                the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n                to False.\n\n        \"\"\"\n\n        if plotter is None:\n            plotter = pv.Plotter()\n            show = True\n\n        # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n        if frustum_scale is None:\n            if self.n_cameras() &gt;= 2:\n                camera_translation_matrices = np.array(\n                    [transform[:3, 3] for transform in self.cam_to_world_transforms]\n                )\n                distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n                max_distance = np.max(distances)\n                frustum_scale = (\n                    (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n                )\n            # else, set it to a default\n            else:\n                frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n        for camera in self.cameras:\n            camera.vis(plotter, frustum_scale=frustum_scale)\n        if add_orientation_cube:\n            # TODO Consider adding to a freestanding vis module\n            ocube = demos.orientation_cube()\n            plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n            plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n            plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n            plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n            plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n            plotter.show_axes()\n\n        if show:\n            if force_xvfb:\n                safe_start_xvfb()\n            plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.__init__","title":"<code>__init__(cameras=None, cam_to_world_transforms=None, intrinsic_params_per_sensor_type={0: EXAMPLE_INTRINSICS}, image_filenames=None, lon_lats=None, image_folder=None, sensor_IDs=None, validate_images=False, local_to_epsg_4978_transform=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>cam_to_world_transforms</code> <code>List[ndarray]</code> <p>The list of 4x4 camera to world transforms</p> <code>None</code> <code>intrinsic_params_per_sensor</code> <code>Dict[int, Dict]</code> <p>A dictionary mapping from an int camera ID to the intrinsic parameters</p> required <code>image_filenames</code> <code>List[PATH_TYPE]</code> <p>The list of image filenames, ideally absolute paths</p> <code>None</code> <code>lon_lats</code> <code>Union[None, List[Union[None, Tuple[float, float]]]]</code> <p>A list of lon,lat tuples, or list of Nones, or None</p> <code>None</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>The top level folder of the images</p> <code>None</code> <code>sensor_IDs</code> <code>List[int]</code> <p>The list of sensor IDs, that index into the sensors_params_dict</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Should the existance of the images be checked. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n    cam_to_world_transforms: Union[None, List[np.ndarray]] = None,\n    intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n        0: EXAMPLE_INTRINSICS\n    },\n    image_filenames: Union[List[PATH_TYPE], None] = None,\n    lon_lats: Union[None, List[Union[None, Tuple[float, float]]]] = None,\n    image_folder: PATH_TYPE = None,\n    sensor_IDs: List[int] = None,\n    validate_images: bool = False,\n    local_to_epsg_4978_transform: Union[np.array, None] = None,\n):\n    \"\"\"_summary_\n\n    Args:\n        cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n        intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n        image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n        lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n        image_folder (PATH_TYPE): The top level folder of the images\n        sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n        validate_images (bool, optional): Should the existance of the images be checked. Defaults to False.\n\n    Raises:\n        ValueError: _description_\n    \"\"\"\n    # Create an object using the supplied cameras\n    if cameras is not None:\n        if isinstance(cameras, PhotogrammetryCamera):\n            cameras = [cameras]\n        self.cameras = cameras\n        return\n\n    # Standardization\n    n_transforms = len(cam_to_world_transforms)\n\n    # Create list of Nones for image filenames if not set\n    if image_filenames is None:\n        image_filenames = [None] * n_transforms\n\n    if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n        # Create a list of the only index if not set\n        sensor_IDs = [\n            list(intrinsic_params_per_sensor_type.keys())[0]\n        ] * n_transforms\n    elif len(sensor_IDs) != n_transforms:\n        raise ValueError(\n            f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n        )\n\n    # If lon lats is None, set it to a list of Nones per transform\n    if lon_lats is None:\n        lon_lats = [None] * n_transforms\n\n    if image_folder is None:\n        # TODO set it to the least common ancestor of all filenames\n        pass\n\n    # Record the values\n    # TODO see if we ever use these\n    self.cam_to_world_transforms = cam_to_world_transforms\n    self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n    self.image_filenames = image_filenames\n    self.lon_lats = lon_lats\n    self.sensor_IDs = sensor_IDs\n    self.image_folder = image_folder\n\n    if validate_images:\n        missing_images, invalid_images = self.find_mising_images()\n        if len(missing_images) &gt; 0:\n            print(f\"Deleting {len(missing_images)} missing images\")\n            valid_images = np.where(np.logical_not(invalid_images))[0]\n            self.image_filenames = np.array(self.image_filenames)[\n                valid_images\n            ].tolist()\n            # Avoid calling .tolist() because this will recursively set all elements to lists\n            # when this should be a list of np.arrays\n            self.cam_to_world_transforms = [\n                x for x in np.array(self.cam_to_world_transforms)[valid_images]\n            ]\n            self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n            self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n    self.cameras = []\n\n    for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n        self.image_filenames,\n        self.cam_to_world_transforms,\n        self.sensor_IDs,\n        self.lon_lats,\n    ):\n        sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n        # This means the sensor did not have enough parameters to be valid\n        if sensor_params is None:\n            continue\n\n        new_camera = PhotogrammetryCamera(\n            image_filename,\n            cam_to_world_transform,\n            lon_lat=lon_lat,\n            local_to_epsg_4978_transform=local_to_epsg_4978_transform,\n            **sensor_params,\n        )\n        self.cameras.append(new_camera)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_camera_locations","title":"<code>get_camera_locations(**kwargs)</code>","text":"<p>Returns a list of camera locations for each camera.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>List[Tuple[float, float] or Tuple[float, float, float]]: List of tuples containing the camera locations.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_locations(self, **kwargs):\n    \"\"\"\n    Returns a list of camera locations for each camera.\n\n    Args:\n        **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n    Returns:\n        List[Tuple[float, float] or Tuple[float, float, float]]:\n            List of tuples containing the camera locations.\n    \"\"\"\n    return [x.get_camera_location(**kwargs) for x in self.cameras]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_camera_view_angles","title":"<code>get_camera_view_angles(in_deg=True)</code>","text":"<p>Compute the pitch and yaw off-nadir for all cameras in the set</p> <p>Parameters:</p> Name Type Description Default <code>in_deg</code> <code>bool</code> <p>Return the angles in degrees rather than radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Tuple[float]]</code> <p>List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_view_angles(self, in_deg: bool = True) -&gt; List[Tuple[float]]:\n    \"\"\"Compute the pitch and yaw off-nadir for all cameras in the set\n\n    Args:\n        in_deg (bool, optional): Return the angles in degrees rather than radians. Defaults to True.\n\n    Returns:\n        List[Tuple[float]]: A list of (pitch, yaw) tuples for each camera.\n    \"\"\"\n    return [\n        camera.get_camera_view_angle(in_deg=in_deg)\n        for camera in tqdm(self.cameras, desc=\"Computing view angles\")\n    ]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_cameras_in_folder","title":"<code>get_cameras_in_folder(folder)</code>","text":"<p>Return the camera set with cameras corresponding to images in that folder</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PATH_TYPE</code> <p>The folder location</p> required <p>Returns:</p> Name Type Description <code>PhotogrammetryCameraSet</code> <p>A copy of the camera set with only the cameras from that folder</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_cameras_in_folder(self, folder: PATH_TYPE):\n    \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n    Args:\n        folder (PATH_TYPE): The folder location\n\n    Returns:\n        PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n    \"\"\"\n    # Get the inds where that camera is in the folder\n    imgs_in_folder_inds = [\n        i\n        for i in range(len(self.cameras))\n        if self.cameras[i].image_filename.is_relative_to(folder)\n    ]\n    # Return the PhotogrammetryCameraSet with those subset of cameras\n    subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n    return subset_cameras\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_cameras_matching_filename_regex","title":"<code>get_cameras_matching_filename_regex(filename_regex)</code>","text":"<p>Return the subset of cameras who's filenames match the provided regex</p> <p>Parameters:</p> Name Type Description Default <code>filename_regex</code> <code>str</code> <p>Regular expression passed to 're' engine</p> required <p>Returns:</p> Name Type Description <code>PhotogrammetryCameraSet</code> <code>PhotogrammetryCameraSet</code> <p>Subset of cameras matching the regex</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_cameras_matching_filename_regex(\n    self, filename_regex: str\n) -&gt; \"PhotogrammetryCameraSet\":\n    \"\"\"Return the subset of cameras who's filenames match the provided regex\n\n    Args:\n        filename_regex (str): Regular expression passed to 're' engine\n\n    Returns:\n        PhotogrammetryCameraSet: Subset of cameras matching the regex\n    \"\"\"\n    # Compute boolean array for which ones match\n    imgs_matching_regex = [\n        bool(re.search(filename_regex, str(filename)))\n        for filename in self.image_filenames\n    ]\n    # Convert to integer indices within the set\n    imgs_matching_regex_inds = np.where(imgs_matching_regex)[0]\n\n    # Get the corresponding subset\n    subset_cameras = self.get_subset_cameras(imgs_matching_regex_inds)\n    return subset_cameras\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_image_filename","title":"<code>get_image_filename(index, absolute=True)</code>","text":"<p>Get the image filename(s) based on the index</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, None]</code> <p>Return the filename of the camera at this index, or all filenames if None.</p> required <code>absolute</code> <code>bool</code> <p>Return the absolute filepath, as oposed to the path relative to the image folder. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <p>typing.Union[PATH_TYPE, list[PATH_TYPE]]: If an integer index is provided, one path will be returned. If None, a list of paths will be returned.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_image_filename(self, index: Union[int, None], absolute=True):\n    \"\"\"Get the image filename(s) based on the index\n\n    Args:\n        index (Union[int, None]):\n            Return the filename of the camera at this index, or all filenames if None.\n            #TODO update to support lists of integer indices as well\n        absolute (bool, optional):\n            Return the absolute filepath, as oposed to the path relative to the image folder.\n            Defaults to True.\n\n    Returns:\n        typing.Union[PATH_TYPE, list[PATH_TYPE]]:\n            If an integer index is provided, one path will be returned. If None, a list of paths\n            will be returned.\n    \"\"\"\n    if index is None:\n        return [\n            self.get_image_filename(i, absolute=absolute)\n            for i in range(len(self.cameras))\n        ]\n\n    filename = self.cameras[index].get_image_filename()\n    if absolute:\n        return Path(filename)\n    else:\n        return Path(filename).relative_to(self.get_image_folder())\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_image_filename--todo-update-to-support-lists-of-integer-indices-as-well","title":"TODO update to support lists of integer indices as well","text":""},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_lon_lat_coords","title":"<code>get_lon_lat_coords()</code>","text":"<p>Returns a list of GPS coords for each camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat_coords(self):\n    \"\"\"Returns a list of GPS coords for each camera\"\"\"\n    return [x.get_lon_lat() for x in self.cameras]\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_subset_ROI","title":"<code>get_subset_ROI(ROI, buffer_radius=0, is_geospatial=None)</code>","text":"<p>Return cameras that are within a radius of the provided geometry</p> <p>Parameters:</p> Name Type Description Default <code>geodata</code> <code>Union[PATH_TYPE, GeoDataFrame, Polygon, MultiPolygon]</code> <p>This can be a Geopandas dataframe, path to a geofile readable by geopandas, or Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe</p> required <code>buffer_radius</code> <code>float</code> <p>Return points within this buffer of the geometry. Defaults to 0. Represents meters if ROI is geospatial.</p> <code>0</code> <code>is_geospatial</code> <code>bool</code> <p>A flag for user to indicate if ROI is geospatial or not; if no flag is provided, the flag is set if the provided geodata has a CRS.</p> <code>None</code> <p>Returns:     subset_camera_set (List[PhotogrammetryCamera]):         List of cameras that fall within the provided ROI</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_subset_ROI(\n    self,\n    ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n    buffer_radius: float = 0,\n    is_geospatial: bool = None,\n):\n    \"\"\"Return cameras that are within a radius of the provided geometry\n\n    Args:\n        geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n            This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n            Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n        buffer_radius (float, optional):\n            Return points within this buffer of the geometry. Defaults to 0. Represents\n            meters if ROI is geospatial.\n        is_geospatial (bool, optional):\n            A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n            the flag is set if the provided geodata has a CRS.\n    Returns:\n        subset_camera_set (List[PhotogrammetryCamera]):\n            List of cameras that fall within the provided ROI\n    \"\"\"\n    # construct GeoDataFrame if not provided\n    if isinstance(ROI, (Polygon, MultiPolygon)):\n        # assume geodata is lat/lon if is_geospatial is True\n        if is_geospatial:\n            ROI = gpd.GeoDataFrame(crs=LAT_LON_CRS, geometry=[ROI])\n        else:\n            ROI = gpd.GeoDataFrame(geometry=[ROI])\n    elif not isinstance(ROI, gpd.GeoDataFrame):\n        # Read in the geofile\n        ROI = gpd.read_file(ROI)\n\n    if is_geospatial is None:\n        is_geospatial = ROI.crs is not None\n\n    if not is_geospatial:\n        # get internal coordinate system camera locations\n        image_locations = [Point(*x) for x in self.get_camera_locations()]\n        image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n    else:\n        # Make sure it's a geometric (meters-based) CRS\n        ROI = ensure_projected_CRS(ROI)\n        # Read the locations of all the points\n        image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n        # Create a dataframe, assuming inputs are lat lon\n        image_locations_df = gpd.GeoDataFrame(\n            geometry=image_locations, crs=LAT_LON_CRS\n        )\n        image_locations_df.to_crs(ROI.crs, inplace=True)\n\n    # Drop all the fields except the geometry for computational reasons\n    ROI = ROI[\"geometry\"]\n    # Buffer out by the requested distance\n    ROI = ROI.buffer(buffer_radius)\n    # Merge the potentially-individual polygons to one\n    # TODO Do experiments to see if this step should be before or after the buffer.\n    # Counterintuitively, it seems that after is faster\n    ROI = unary_union(ROI.tolist())\n\n    # Determine the binary mask for which cameras are within the ROI\n    cameras_in_ROI = image_locations_df.within(ROI).to_numpy()\n    # Convert to the integer indices\n    cameras_in_ROI = np.where(cameras_in_ROI)[0]\n    # Get the corresponding subset of cameras\n    subset_camera_set = self.get_subset_cameras(cameras_in_ROI)\n    return subset_camera_set\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_cameras","title":"<code>n_cameras()</code>","text":"<p>Return the number of cameras</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_cameras(self) -&gt; int:\n    \"\"\"Return the number of cameras\"\"\"\n    return len(self.cameras)\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_image_channels","title":"<code>n_image_channels()</code>","text":"<p>Return the number of channels in the image</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_image_channels(self) -&gt; int:\n    \"\"\"Return the number of channels in the image\"\"\"\n    return 3\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.triangulate_detections","title":"<code>triangulate_detections(detector, transform_to_epsg_4978=None, similarity_threshold_meters=0.1, louvain_resolution=2, vis=True, plotter=pv.Plotter(), vis_ray_length_meters=200)</code>","text":"<p>Take per-image detections and triangulate them to 3D locations</p> <p>Parameters:</p> Name Type Description Default <code>detector</code> <code>TabularRectangleSegmentor</code> <p>Produces detections per image using the get_detection_centers method</p> required <code>transform_to_epsg_4978</code> <code>Union[ndarray, None]</code> <p>The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.</p> <code>None</code> <code>similarity_threshold_meters</code> <code>float</code> <p>Consider rays a potential match if the distance between them is less than this value. Defaults to 0.1.</p> <code>0.1</code> <code>louvain_resolution</code> <code>float</code> <p>The resolution parameter of the networkx.louvain_communities function. Defaults to 2.0.</p> <code>2</code> <code>vis</code> <code>bool</code> <p>Whether to show the detection projections and intersecting points. Defaults to True.</p> <code>True</code> <code>plotter</code> <code>Plotter</code> <p>The plotter to add the visualizations to is vis=True. If not set, a new one will be created. Defaults to pv.Plotter().</p> <code>Plotter()</code> <code>vis_ray_length_meters</code> <code>float</code> <p>The length of the visualized rays in meters. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (n unique objects, 3), the 3D locations of the identified objects. If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the local coordinate system of the mesh</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def triangulate_detections(\n    self,\n    detector: TabularRectangleSegmentor,\n    transform_to_epsg_4978=None,\n    similarity_threshold_meters: float = 0.1,\n    louvain_resolution: float = 2,\n    vis: bool = True,\n    plotter: pv.Plotter = pv.Plotter(),\n    vis_ray_length_meters: float = 200,\n) -&gt; np.ndarray:\n    \"\"\"Take per-image detections and triangulate them to 3D locations\n\n    Args:\n        detector (TabularRectangleSegmentor):\n            Produces detections per image using the get_detection_centers method\n        transform_to_epsg_4978 (typing.Union[np.ndarray, None], optional):\n            The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.\n        similarity_threshold_meters (float, optional):\n            Consider rays a potential match if the distance between them is less than this\n            value. Defaults to 0.1.\n        louvain_resolution (float, optional):\n            The resolution parameter of the networkx.louvain_communities function. Defaults to\n            2.0.\n        vis (bool, optional):\n            Whether to show the detection projections and intersecting points. Defaults to True.\n        plotter (pv.Plotter, optional):\n            The plotter to add the visualizations to is vis=True. If not set, a new one will be\n            created. Defaults to pv.Plotter().\n        vis_ray_length_meters (float, optional):\n            The length of the visualized rays in meters. Defaults to 200.\n\n    Returns:\n        np.ndarray:\n            (n unique objects, 3), the 3D locations of the identified objects.\n            If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the\n            local coordinate system of the mesh\n    \"\"\"\n    # Determine scale factor relating meters to internal coordinates\n    meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n    similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n    vis_ray_length_local = vis_ray_length_meters * meters_to_local_scale\n\n    # Record the lines corresponding to each detection and the associated image ID\n    all_line_segments = []\n    all_image_IDs = []\n\n    # Iterate over the cameras\n    for camera_ind in range(len(self)):\n        # Get the image filename\n        image_filename = str(self.get_image_filename(camera_ind, absolute=False))\n        # Get the centers of associated detection from the detector\n        # TODO, this only works with \"detectors\" that can look up the detections based on the\n        # filename alone. In the future we might want to support real detectors that actually\n        # use the image.\n        detection_centers_pixels = detector.get_detection_centers(image_filename)\n        # Get the individual camera\n        camera = self[camera_ind]\n        # Project rays given the locations of the detections in pixel coordinates\n        line_segments = camera.cast_rays(\n            pixel_coords_ij=detection_centers_pixels,\n            line_length=vis_ray_length_local,\n        )\n        # If there are no detections, this will be None\n        if line_segments is not None:\n            # Record the line segments, which will be ordered as alternating (start, end) rows\n            all_line_segments.append(line_segments)\n            # Record which image ID generated each line\n            all_image_IDs.append(\n                np.full(int(line_segments.shape[0] / 2), fill_value=camera_ind)\n            )\n\n    # Concatenate the lists of arrays into a single array\n    all_line_segments = np.concatenate(all_line_segments, axis=0)\n    all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n    # Get the starts and ends, which are alternating rows\n    ray_starts = all_line_segments[0::2]\n    segment_ends = all_line_segments[1::2]\n    # Determine the direction\n    ray_directions = segment_ends - ray_starts\n    # Make the ray directions unit length\n    ray_directions = ray_directions / np.linalg.norm(\n        ray_directions, axis=1, keepdims=True\n    )\n\n    # Compute the distance matrix of ray-ray intersections\n    num_dets = ray_starts.shape[0]\n    interesection_dists = np.full((num_dets, num_dets), fill_value=np.nan)\n\n    # Calculate the upper triangular matrix of ray-ray interesections\n    for i in tqdm(range(num_dets), desc=\"Calculating quality of ray intersections\"):\n        for j in range(i, num_dets):\n            # Extract starts and directions\n            a0 = ray_starts[i]\n            a1 = segment_ends[i]\n            b0 = ray_starts[j]\n            b1 = segment_ends[j]\n            # TODO explore whether this could be vectorized\n            _, _, dist = compute_approximate_ray_intersection(a0, a1, b0, b1)\n\n            interesection_dists[i, j] = dist\n\n    # Filter out intersections that are above the threshold distance\n    interesection_dists[interesection_dists &gt; similarity_threshold_local] = np.nan\n\n    # Determine which intersections are valid, represented by finite values\n    i_inds, j_inds = np.where(np.isfinite(interesection_dists))\n\n    # Build a list of (i, j, info_dict) tuples encoding the valid edges and their intersection\n    # distance\n    positive_edges = [\n        (i, j, {\"weight\": 1 / interesection_dists[i, j]})\n        for i, j in zip(i_inds, j_inds)\n    ]\n\n    # Build a networkx graph. The nodes represent an individual detection while the edges\n    # represent the quality of the matches between detections.\n    graph = networkx.Graph(positive_edges)\n    # Determine Louvain communities which are sets of nodes. Ideally, this represents a set of\n    # detections that all coorespond to one 3D object\n    communities = networkx.community.louvain_communities(\n        graph, weight=\"weight\", resolution=louvain_resolution\n    )\n    # Sort the communities by size\n    communities = sorted(communities, key=len, reverse=True)\n\n    ## Triangulate the rays for each community to identify the 3D location\n    community_points = []\n    # Record the community IDs per detection\n    community_IDs = np.full(num_dets, fill_value=np.nan)\n    # Iterate over communities\n    for community_ID, community in enumerate(communities):\n        # Get the indices of the detections for that community\n        community_detection_inds = np.array(list(community))\n        # Record the community ID for the corresponding detection IDs\n        community_IDs[community_detection_inds] = community_ID\n\n        # Get the set of starts and directions for that community\n        community_starts = ray_starts[community_detection_inds]\n        community_directions = ray_directions[community_detection_inds]\n\n        # Determine the least squares triangulation of the rays\n        community_3D_point = triangulate_rays_lstsq(\n            community_starts, community_directions\n        )\n        community_points.append(community_3D_point)\n\n    # Stack all of the points into one vector\n    community_points = np.vstack(community_points)\n\n    # Show the rays and detections\n    if vis:\n        # Show the line segements\n        # TODO: consider coloring these lines by community\n        lines_mesh = pv.line_segments_from_points(all_line_segments)\n        plotter.add_mesh(\n            lines_mesh,\n            scalars=community_IDs,\n            label=\"Rays, colored by community ID\",\n        )\n        # Show the triangulated communtities as red spheres\n        detected_points = pv.PolyData(community_points)\n        plotter.add_points(\n            detected_points,\n            color=\"r\",\n            render_points_as_spheres=True,\n            point_size=10,\n            label=\"Triangulated locations\",\n        )\n        plotter.add_legend()\n\n    # Convert the intersection points from the local mesh coordinate system to lat lon\n    if transform_to_epsg_4978 is not None:\n        # Append a column of all ones to make the homogenous coordinates\n        community_points_homogenous = np.concatenate(\n            [community_points, np.ones_like(community_points[:, 0:1])], axis=1\n        )\n        # Use the transform matrix to transform the points into the earth centered, earth fixed\n        # frame, EPSG:4978\n        community_points_epsg_4978 = (\n            transform_to_epsg_4978 @ community_points_homogenous.T\n        ).T\n        # Convert the points from earth centered, earth fixed frame to lat lon\n        community_points_lat_lon = convert_CRS_3D_points(\n            community_points_epsg_4978,\n            input_CRS=EARTH_CENTERED_EARTH_FIXED_CRS,\n            output_CRS=LAT_LON_CRS,\n        )\n        # Set the community points to lat lon\n        community_points = community_points_lat_lon\n\n    # Return the 3D locations of the community points\n    return community_points\n</code></pre>"},{"location":"API_reference/cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.vis","title":"<code>vis(plotter=None, add_orientation_cube=False, show=False, frustum_scale=None, force_xvfb=False, interactive_jupyter=False)</code>","text":"<p>Visualize all the cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to add the cameras to. If None, will be created and then plotted</p> <code>None</code> <code>add_orientation_cube</code> <code>bool</code> <p>Add a cube to visualize the coordinate system. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Show the results instead of waiting for other content to be added</p> <code>False</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. If None, will set to 1/120th of the maximum distance between two cameras.</p> <code>None</code> <code>force_xvfb</code> <code>bool</code> <p>Force a headless rendering backend</p> <code>False</code> <code>interactive_jupyter</code> <code>bool</code> <p>Will allow you to interact with the visualization in your notebook if supported by the notebook server. Otherwise will fail. Only applicable if <code>show=True</code>. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    add_orientation_cube: bool = False,\n    show: bool = False,\n    frustum_scale: float = None,\n    force_xvfb: bool = False,\n    interactive_jupyter: bool = False,\n):\n    \"\"\"Visualize all the cameras\n\n    Args:\n        plotter (pv.Plotter):\n            Plotter to add the cameras to. If None, will be created and then plotted\n        add_orientation_cube (bool, optional):\n            Add a cube to visualize the coordinate system. Defaults to False.\n        show (bool, optional):\n            Show the results instead of waiting for other content to be added\n        frustum_scale (float, optional):\n            Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n            between two cameras.\n        force_xvfb (bool, optional):\n            Force a headless rendering backend\n        interactive_jupyter (bool, optional):\n            Will allow you to interact with the visualization in your notebook if supported by\n            the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n            to False.\n\n    \"\"\"\n\n    if plotter is None:\n        plotter = pv.Plotter()\n        show = True\n\n    # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n    if frustum_scale is None:\n        if self.n_cameras() &gt;= 2:\n            camera_translation_matrices = np.array(\n                [transform[:3, 3] for transform in self.cam_to_world_transforms]\n            )\n            distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n            max_distance = np.max(distances)\n            frustum_scale = (\n                (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n            )\n        # else, set it to a default\n        else:\n            frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n    for camera in self.cameras:\n        camera.vis(plotter, frustum_scale=frustum_scale)\n    if add_orientation_cube:\n        # TODO Consider adding to a freestanding vis module\n        ocube = demos.orientation_cube()\n        plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n        plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n        plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n        plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n        plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n        plotter.show_axes()\n\n    if show:\n        if force_xvfb:\n            safe_start_xvfb()\n        plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/","title":"Derived Cameras Docstrings","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet","title":"<code>MetashapeCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class MetashapeCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        camera_file: PATH_TYPE,\n        image_folder: PATH_TYPE,\n        original_image_folder: typing.Optional[PATH_TYPE] = None,\n        validate_images: bool = False,\n        default_sensor_params: dict = {\"cx\": 0.0, \"cy\": 0.0},\n    ):\n        \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n        Args:\n            camera_file (PATH_TYPE):\n                Path to metashape .xml export\n            image_folder (PATH_TYPE):\n                Path to image folder root\n            original_image_folder (PATH_TYPE, optional):\n                Path to where the original images for photogrammetry were. This is removed from the\n                absolute path recorded in the file. Defaults to None.\n            validate_images (bool, optional):\n                Should you ensure that the images are present on disk. Defaults to False.\n            default_sensor_params (dict, optional):\n                Default parameters for the intrinsic parameters if not present. Defaults to zeros\n                \"cx\" and \"cy\".\n\n        Raises:\n            ValueError: If camera calibration does not contain the f, cx, and cy params\n        \"\"\"\n        # Load the xml file\n        # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n        tree = ET.parse(camera_file)\n        root = tree.getroot()\n        # first level\n        chunk = root.find(\"chunk\")\n        # second level\n        sensors = chunk.find(\"sensors\")\n        # Parse the sensors representation\n        sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n        # Set up the lists to populate\n        image_filenames = []\n        cam_to_world_transforms = []\n        sensor_IDs = []\n\n        cameras = chunk[2]\n        # Iterate over metashape cameras and fill out required information\n        for cam_or_group in cameras:\n            if cam_or_group.tag == \"group\":\n                for cam in cam_or_group:\n                    update_lists(\n                        cam,\n                        image_folder,\n                        cam_to_world_transforms,\n                        image_filenames,\n                        sensor_IDs,\n                        original_image_folder=original_image_folder,\n                    )\n            else:\n                update_lists(\n                    cam_or_group,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                    original_image_folder=original_image_folder,\n                )\n\n        # Compute the lat lon using the transforms, because the reference values recorded in the file\n        # reflect the EXIF values, not the optimized ones\n\n        # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n        chunk_to_epsg4327 = parse_transform_metashape(camera_file=camera_file)\n\n        if chunk_to_epsg4327 is not None:\n            # Compute the location of each camera in ECEF\n            cam_locs_in_epsg4327 = []\n            for cam_to_world_transform in cam_to_world_transforms:\n                cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n                cam_locs_in_epsg4327.append(chunk_to_epsg4327 @ cam_loc_in_chunk)\n            cam_locs_in_epsg4327 = np.concatenate(cam_locs_in_epsg4327, axis=1)[:3].T\n            # Transform these points into lat-lon-alt\n            transformer = pyproj.Transformer.from_crs(\n                EARTH_CENTERED_EARTH_FIXED_CRS, LAT_LON_CRS\n            )\n            lat, lon, _ = transformer.transform(\n                xx=cam_locs_in_epsg4327[:, 0],\n                yy=cam_locs_in_epsg4327[:, 1],\n                zz=cam_locs_in_epsg4327[:, 2],\n            )\n            lon_lats = list(zip(lon, lat))\n        else:\n            # TODO consider trying to parse from the xml\n            lon_lats = None\n\n        # Actually construct the camera objects using the base class\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            lon_lats=lon_lats,\n            image_folder=image_folder,\n            sensor_IDs=sensor_IDs,\n            validate_images=validate_images,\n            local_to_epsg_4978_transform=chunk_to_epsg4327,\n        )\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet.__init__","title":"<code>__init__(camera_file, image_folder, original_image_folder=None, validate_images=False, default_sensor_params={'cx': 0.0, 'cy': 0.0})</code>","text":"<p>Parse the information about the camera intrinsics and extrinsics</p> <p>Parameters:</p> Name Type Description Default <code>camera_file</code> <code>PATH_TYPE</code> <p>Path to metashape .xml export</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to image folder root</p> required <code>original_image_folder</code> <code>PATH_TYPE</code> <p>Path to where the original images for photogrammetry were. This is removed from the absolute path recorded in the file. Defaults to None.</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Should you ensure that the images are present on disk. Defaults to False.</p> <code>False</code> <code>default_sensor_params</code> <code>dict</code> <p>Default parameters for the intrinsic parameters if not present. Defaults to zeros \"cx\" and \"cy\".</p> <code>{'cx': 0.0, 'cy': 0.0}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If camera calibration does not contain the f, cx, and cy params</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    camera_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    original_image_folder: typing.Optional[PATH_TYPE] = None,\n    validate_images: bool = False,\n    default_sensor_params: dict = {\"cx\": 0.0, \"cy\": 0.0},\n):\n    \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n    Args:\n        camera_file (PATH_TYPE):\n            Path to metashape .xml export\n        image_folder (PATH_TYPE):\n            Path to image folder root\n        original_image_folder (PATH_TYPE, optional):\n            Path to where the original images for photogrammetry were. This is removed from the\n            absolute path recorded in the file. Defaults to None.\n        validate_images (bool, optional):\n            Should you ensure that the images are present on disk. Defaults to False.\n        default_sensor_params (dict, optional):\n            Default parameters for the intrinsic parameters if not present. Defaults to zeros\n            \"cx\" and \"cy\".\n\n    Raises:\n        ValueError: If camera calibration does not contain the f, cx, and cy params\n    \"\"\"\n    # Load the xml file\n    # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n    tree = ET.parse(camera_file)\n    root = tree.getroot()\n    # first level\n    chunk = root.find(\"chunk\")\n    # second level\n    sensors = chunk.find(\"sensors\")\n    # Parse the sensors representation\n    sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n    # Set up the lists to populate\n    image_filenames = []\n    cam_to_world_transforms = []\n    sensor_IDs = []\n\n    cameras = chunk[2]\n    # Iterate over metashape cameras and fill out required information\n    for cam_or_group in cameras:\n        if cam_or_group.tag == \"group\":\n            for cam in cam_or_group:\n                update_lists(\n                    cam,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                    original_image_folder=original_image_folder,\n                )\n        else:\n            update_lists(\n                cam_or_group,\n                image_folder,\n                cam_to_world_transforms,\n                image_filenames,\n                sensor_IDs,\n                original_image_folder=original_image_folder,\n            )\n\n    # Compute the lat lon using the transforms, because the reference values recorded in the file\n    # reflect the EXIF values, not the optimized ones\n\n    # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n    chunk_to_epsg4327 = parse_transform_metashape(camera_file=camera_file)\n\n    if chunk_to_epsg4327 is not None:\n        # Compute the location of each camera in ECEF\n        cam_locs_in_epsg4327 = []\n        for cam_to_world_transform in cam_to_world_transforms:\n            cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n            cam_locs_in_epsg4327.append(chunk_to_epsg4327 @ cam_loc_in_chunk)\n        cam_locs_in_epsg4327 = np.concatenate(cam_locs_in_epsg4327, axis=1)[:3].T\n        # Transform these points into lat-lon-alt\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_CRS, LAT_LON_CRS\n        )\n        lat, lon, _ = transformer.transform(\n            xx=cam_locs_in_epsg4327[:, 0],\n            yy=cam_locs_in_epsg4327[:, 1],\n            zz=cam_locs_in_epsg4327[:, 2],\n        )\n        lon_lats = list(zip(lon, lat))\n    else:\n        # TODO consider trying to parse from the xml\n        lon_lats = None\n\n    # Actually construct the camera objects using the base class\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        lon_lats=lon_lats,\n        image_folder=image_folder,\n        sensor_IDs=sensor_IDs,\n        validate_images=validate_images,\n        local_to_epsg_4978_transform=chunk_to_epsg4327,\n    )\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet","title":"<code>COLMAPCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class COLMAPCameraSet(PhotogrammetryCameraSet):\n\n    def __init__(\n        self,\n        cameras_file: PATH_TYPE,\n        images_file: PATH_TYPE,\n        image_folder: typing.Union[None, PATH_TYPE] = None,\n        validate_images: bool = False,\n    ):\n        \"\"\"\n        Create a camera set from the files exported by the open-source structure-from-motion\n        software COLMAP as defined here: https://colmap.github.io/format.html\n\n        Args:\n            cameras_file (PATH_TYPE):\n                Path to the file containing the camera models definitions\n            images_file (PATH_TYPE):\n                Path to the per-image information, including the pose and which camera model is used\n            image_folder (typing.Union[None, PATH_TYPE], optional):\n                Path to the folder of images used to generate the reconstruction. Defaults to None.\n            validate_images (bool, optional):\n                Ensure that the images described in images_file are present in image_folder.\n                Defaults to False.\n\n        Raises:\n            NotImplementedError: If the camera is not a Simple radial model\n        \"\"\"\n        # Parse the csv representation of the camera models\n        cameras_data = pd.read_csv(\n            cameras_file,\n            sep=\" \",\n            skiprows=[0, 1, 2],\n            header=None,\n            names=(\n                \"CAMERA_ID\",\n                \"MODEL\",\n                \"WIDTH\",\n                \"HEIGHT\",\n                \"PARAMS_F\",\n                \"PARAMS_CX\",\n                \"PARAMS_CY\",\n                \"PARAMS_RADIAL\",\n            ),\n        )\n        # Parse the csv of the per-image information\n        # Note that every image has first the useful information on one row and then unneeded\n        # keypoint information on the following row. Therefore, the keypoints are discarded.\n        images_data = pd.read_csv(\n            images_file,\n            sep=\" \",\n            skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n            header=None,\n            names=(\n                \"IMAGE_ID\",\n                \"QW\",\n                \"QX\",\n                \"QY\",\n                \"QZ\",\n                \"TX\",\n                \"TY\",\n                \"TZ\",\n                \"CAMERA_ID\",\n                \"NAME\",\n            ),\n            usecols=list(range(10)),\n        )\n\n        # TODO support more camera models\n        if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n            raise NotImplementedError(\"Not a supported camera model\")\n\n        # Parse the camera parameters, creating a dict for each distinct camera model\n        sensors_dict = {}\n        for _, row in cameras_data.iterrows():\n            # Note that the convention in this tool is for cx, cy to be defined from the center\n            # not the corner so it must be shifted\n            sensor_dict = {\n                \"image_width\": row[\"WIDTH\"],\n                \"image_height\": row[\"HEIGHT\"],\n                \"f\": row[\"PARAMS_F\"],\n                \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n                \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n                \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n            }\n            sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n        # Parse the per-image information\n        cam_to_world_transforms = []\n        sensor_IDs = []\n        image_filenames = []\n\n        for _, row in images_data.iterrows():\n            # Convert from the quaternion representation to the matrix one. Note that the W element\n            # is the first one in the COLMAP convention but the last one in scipy.\n            rot_mat = Rotation.from_quat(\n                (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n            ).as_matrix()\n            # Get the camera translation\n            translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n            # Create a 4x4 homogenous matrix representing the world_to_cam transform\n            world_to_cam = np.eye(4)\n            # Populate the sub-elements\n            world_to_cam[:3, :3] = rot_mat\n            world_to_cam[:3, 3] = translation_vec\n            # We need the cam to world transform. Since we're using a 4x4 representation, we can\n            # just invert the matrix\n            cam_to_world = np.linalg.inv(world_to_cam)\n            cam_to_world_transforms.append(cam_to_world)\n\n            # Record which camera model is used and the image filename\n            sensor_IDs.append(row[\"CAMERA_ID\"])\n            image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n        # Instantiate the camera set\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            sensor_IDs=sensor_IDs,\n            image_folder=image_folder,\n            validate_images=validate_images,\n        )\n</code></pre>"},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet.__init__","title":"<code>__init__(cameras_file, images_file, image_folder=None, validate_images=False)</code>","text":"<p>Create a camera set from the files exported by the open-source structure-from-motion software COLMAP as defined here: https://colmap.github.io/format.html</p> <p>Parameters:</p> Name Type Description Default <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the file containing the camera models definitions</p> required <code>images_file</code> <code>PATH_TYPE</code> <p>Path to the per-image information, including the pose and which camera model is used</p> required <code>image_folder</code> <code>Union[None, PATH_TYPE]</code> <p>Path to the folder of images used to generate the reconstruction. Defaults to None.</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Ensure that the images described in images_file are present in image_folder. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the camera is not a Simple radial model</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras_file: PATH_TYPE,\n    images_file: PATH_TYPE,\n    image_folder: typing.Union[None, PATH_TYPE] = None,\n    validate_images: bool = False,\n):\n    \"\"\"\n    Create a camera set from the files exported by the open-source structure-from-motion\n    software COLMAP as defined here: https://colmap.github.io/format.html\n\n    Args:\n        cameras_file (PATH_TYPE):\n            Path to the file containing the camera models definitions\n        images_file (PATH_TYPE):\n            Path to the per-image information, including the pose and which camera model is used\n        image_folder (typing.Union[None, PATH_TYPE], optional):\n            Path to the folder of images used to generate the reconstruction. Defaults to None.\n        validate_images (bool, optional):\n            Ensure that the images described in images_file are present in image_folder.\n            Defaults to False.\n\n    Raises:\n        NotImplementedError: If the camera is not a Simple radial model\n    \"\"\"\n    # Parse the csv representation of the camera models\n    cameras_data = pd.read_csv(\n        cameras_file,\n        sep=\" \",\n        skiprows=[0, 1, 2],\n        header=None,\n        names=(\n            \"CAMERA_ID\",\n            \"MODEL\",\n            \"WIDTH\",\n            \"HEIGHT\",\n            \"PARAMS_F\",\n            \"PARAMS_CX\",\n            \"PARAMS_CY\",\n            \"PARAMS_RADIAL\",\n        ),\n    )\n    # Parse the csv of the per-image information\n    # Note that every image has first the useful information on one row and then unneeded\n    # keypoint information on the following row. Therefore, the keypoints are discarded.\n    images_data = pd.read_csv(\n        images_file,\n        sep=\" \",\n        skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n        header=None,\n        names=(\n            \"IMAGE_ID\",\n            \"QW\",\n            \"QX\",\n            \"QY\",\n            \"QZ\",\n            \"TX\",\n            \"TY\",\n            \"TZ\",\n            \"CAMERA_ID\",\n            \"NAME\",\n        ),\n        usecols=list(range(10)),\n    )\n\n    # TODO support more camera models\n    if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n        raise NotImplementedError(\"Not a supported camera model\")\n\n    # Parse the camera parameters, creating a dict for each distinct camera model\n    sensors_dict = {}\n    for _, row in cameras_data.iterrows():\n        # Note that the convention in this tool is for cx, cy to be defined from the center\n        # not the corner so it must be shifted\n        sensor_dict = {\n            \"image_width\": row[\"WIDTH\"],\n            \"image_height\": row[\"HEIGHT\"],\n            \"f\": row[\"PARAMS_F\"],\n            \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n            \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n            \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n        }\n        sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n    # Parse the per-image information\n    cam_to_world_transforms = []\n    sensor_IDs = []\n    image_filenames = []\n\n    for _, row in images_data.iterrows():\n        # Convert from the quaternion representation to the matrix one. Note that the W element\n        # is the first one in the COLMAP convention but the last one in scipy.\n        rot_mat = Rotation.from_quat(\n            (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n        ).as_matrix()\n        # Get the camera translation\n        translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n        # Create a 4x4 homogenous matrix representing the world_to_cam transform\n        world_to_cam = np.eye(4)\n        # Populate the sub-elements\n        world_to_cam[:3, :3] = rot_mat\n        world_to_cam[:3, 3] = translation_vec\n        # We need the cam to world transform. Since we're using a 4x4 representation, we can\n        # just invert the matrix\n        cam_to_world = np.linalg.inv(world_to_cam)\n        cam_to_world_transforms.append(cam_to_world)\n\n        # Record which camera model is used and the image filename\n        sensor_IDs.append(row[\"CAMERA_ID\"])\n        image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n    # Instantiate the camera set\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        sensor_IDs=sensor_IDs,\n        image_folder=image_folder,\n        validate_images=validate_images,\n    )\n</code></pre>"},{"location":"API_reference/cameras/segmentor/","title":"Segmentor Docstrings","text":""},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet","title":"<code>SegmentorPhotogrammetryCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>class SegmentorPhotogrammetryCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        base_camera_set: PhotogrammetryCameraSet,\n        segmentor: Segmentor,\n        dont_load_base_image: bool = True,\n    ):\n        \"\"\"Wraps a camera set to provide segmented versions of the image\n\n        Args:\n            base_camera_set (PhotogrammetryCameraSet): The original camera set\n            segmentor (Segmentor): A fully instantiated segmentor\n        \"\"\"\n        self.base_camera_set = base_camera_set\n        self.segmentor = segmentor\n        self.dont_load_base_image = dont_load_base_image\n\n        # This should allow all un-overridden methods to work as expected\n        self.cameras = self.base_camera_set.cameras\n\n    def get_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        if self.dont_load_base_image:\n            raw_image = None\n        else:\n            raw_image = self.base_camera_set.get_image_by_index(index, image_scale)\n        image_filename = self.base_camera_set.get_image_filename(index, absolute=True)\n        segmented_image = self.segmentor.segment_image(\n            raw_image, filename=image_filename, image_scale=image_scale\n        )\n        return segmented_image\n\n    def get_raw_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        return self.base_camera_set.get_image_by_index(\n            index=index, image_scale=image_scale\n        )\n\n    def get_subset_cameras(self, inds: typing.List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set.cameras[i] for i in inds]\n        subset_camera_set.base_camera_set = (\n            subset_camera_set.base_camera_set.get_subset_cameras(inds)\n        )\n        return subset_camera_set\n\n    def n_image_channels(self) -&gt; int:\n        return self.segmentor.num_classes\n\n    def get_subset_with_valid_segmentation(self) -&gt; \"SegmentorPhotogrammetryCameraSet\":\n        \"\"\"Get a new camera set consisting of all images that have a valid segmentation result\n\n        Returns:\n            SegmentorPhotogrammetryCameraSet: The subset of cameras with valid segmentation\n        \"\"\"\n        valid_inds = []\n        for i in range(len(self)):\n            try:\n                # Try to get the segmented result\n                self.get_image_by_index(i)\n                # If successful, append it to the list of valid IDs\n                valid_inds.append(i)\n            except:\n                pass\n        # Return the valid subset\n        return self.get_subset_cameras(valid_inds)\n</code></pre>"},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet.__init__","title":"<code>__init__(base_camera_set, segmentor, dont_load_base_image=True)</code>","text":"<p>Wraps a camera set to provide segmented versions of the image</p> <p>Parameters:</p> Name Type Description Default <code>base_camera_set</code> <code>PhotogrammetryCameraSet</code> <p>The original camera set</p> required <code>segmentor</code> <code>Segmentor</code> <p>A fully instantiated segmentor</p> required Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>def __init__(\n    self,\n    base_camera_set: PhotogrammetryCameraSet,\n    segmentor: Segmentor,\n    dont_load_base_image: bool = True,\n):\n    \"\"\"Wraps a camera set to provide segmented versions of the image\n\n    Args:\n        base_camera_set (PhotogrammetryCameraSet): The original camera set\n        segmentor (Segmentor): A fully instantiated segmentor\n    \"\"\"\n    self.base_camera_set = base_camera_set\n    self.segmentor = segmentor\n    self.dont_load_base_image = dont_load_base_image\n\n    # This should allow all un-overridden methods to work as expected\n    self.cameras = self.base_camera_set.cameras\n</code></pre>"},{"location":"API_reference/cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet.get_subset_with_valid_segmentation","title":"<code>get_subset_with_valid_segmentation()</code>","text":"<p>Get a new camera set consisting of all images that have a valid segmentation result</p> <p>Returns:</p> Name Type Description <code>SegmentorPhotogrammetryCameraSet</code> <code>SegmentorPhotogrammetryCameraSet</code> <p>The subset of cameras with valid segmentation</p> Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>def get_subset_with_valid_segmentation(self) -&gt; \"SegmentorPhotogrammetryCameraSet\":\n    \"\"\"Get a new camera set consisting of all images that have a valid segmentation result\n\n    Returns:\n        SegmentorPhotogrammetryCameraSet: The subset of cameras with valid segmentation\n    \"\"\"\n    valid_inds = []\n    for i in range(len(self)):\n        try:\n            # Try to get the segmented result\n            self.get_image_by_index(i)\n            # If successful, append it to the list of valid IDs\n            valid_inds.append(i)\n        except:\n            pass\n    # Return the valid subset\n    return self.get_subset_cameras(valid_inds)\n</code></pre>"},{"location":"API_reference/meshes/","title":"Meshes","text":"<ul> <li> <p>Derived Meshes Docstrings</p> </li> <li> <p>Meshes Docstrings</p> </li> </ul>"},{"location":"API_reference/meshes/derived_meshes/","title":"Derived Mesh Docstrings","text":""},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked","title":"<code>TexturedPhotogrammetryMeshChunked</code>","text":"<p>               Bases: <code>TexturedPhotogrammetryMesh</code></p> <p>Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>class TexturedPhotogrammetryMeshChunked(TexturedPhotogrammetryMesh):\n    \"\"\"Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes\"\"\"\n\n    def get_mesh_chunks_for_cameras(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        include_texture: bool = False,\n    ):\n        \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The chunks of the mesh are generated by clustering the cameras\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n            include_texture (bool, optional): Should the texture from the full mesh be included\n                in the subset mesh. Defaults to False.\n\n        Yields:\n            pv.PolyData: The subset mesh\n            PhotogrammetryCameraSet: The cameras associated with that mesh\n            np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n        \"\"\"\n        # Extract the points depending on whether it's a single camera or a set\n        if isinstance(cameras, PhotogrammetryCamera):\n            camera_points = [Point(*cameras.get_lon_lat())]\n        else:\n            # Get the lat lon for each camera point and turn into a shapely Point\n            camera_points = [\n                Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n            ]\n\n        # Create a geodataframe from the points\n        camera_points = gpd.GeoDataFrame(\n            geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n        )\n        # Make sure the gdf has a gemetric CRS so there is no warping of the space\n        camera_points = ensure_projected_CRS(camera_points)\n        # Extract the x, y points now in a geometric CRS\n        camera_points_numpy = np.stack(\n            camera_points.geometry.apply(lambda point: (point.x, point.y))\n        )\n\n        # Assign each camera to a cluster\n        camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n            camera_points_numpy\n        )\n        if vis_clusters:\n            # Show the camera locations, colored by which one they were assigned to\n            plt.scatter(\n                camera_points_numpy[:, 0],\n                camera_points_numpy[:, 1],\n                c=camera_cluster_IDs,\n                cmap=\"tab20\",\n            )\n            plt.show()\n\n        # Get the texture from the full mesh\n        full_mesh_texture = (\n            self.get_texture(request_vertex_texture=False) if include_texture else None\n        )\n\n        # Iterate over the clusters of cameras\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n            # Get indices of cameras for that cluster\n            matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n            # Get the segmentor camera set for the subset of the camera inds\n            sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n            # Extract the rows in the dataframe for those IDs\n            subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n            # TODO this could be accellerated by computing the membership for all points at the begining.\n            # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n            # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n            # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n            # ROIs may actually lead to slower computations than doing it sequentially\n\n            # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n            sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n                region_of_interest=subset_camera_points,\n                buffer_meters=buffer_dist_meters,\n                return_original_IDs=True,\n            )\n            # Extract the corresponding texture elements for this sub mesh if needed\n            # If include_texture=False, the full_mesh_texture will not be set\n            # If there is no mesh, the texture should also be set to None, otherwise it will be\n            # ambigious whether it's a face or vertex texture\n            sub_mesh_texture = (\n                full_mesh_texture[face_IDs]\n                if full_mesh_texture is not None and len(face_IDs) &gt; 0\n                else None\n            )\n\n            # Wrap this pyvista mesh in a photogrammetry mesh\n            sub_mesh_TPM = TexturedPhotogrammetryMesh(\n                sub_mesh_pv, texture=sub_mesh_texture\n            )\n\n            # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n            # face IDs mapping the faces in the sub mesh back to the full one\n            yield sub_mesh_TPM, sub_camera_set, face_IDs\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output.\n        This version first clusters the cameras, extracts a region of the mesh surrounding each\n        cluster of cameras, and then performs rendering on each sub-region.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n            n_clusters (int, optional):\n                Number of clusters to break the cameras into. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                How far around the cameras to include the mesh. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the clusters of camera locations be shown. Defaults to False.\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        # Create a generator to chunked meshes based on clusters of cameras\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n            include_texture=True,\n        )\n\n        for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n            chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n        ):\n            # Create the render generator\n            render_gen = sub_mesh_TPM.render_flat(\n                sub_camera_set,\n                batch_size=batch_size,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n            # Yield items from the returned generator\n            for render_item in render_gen:\n                yield render_item\n\n            # This is another attempt to free memory\n            sub_mesh_TPM.pix2face_plotter.deep_clean()\n            # There's a strange memory leak, I think it may be because of the sub-mesh sticking around\n            print(\"About to delete sub mesh\")\n            del sub_mesh_TPM\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n        vis_clusters: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n        mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 250.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n\n        # Initialize the values that will be incremented per cluster\n        summed_projections = np.zeros(\n            (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n        )\n        projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n        # Create a generator to generate chunked meshes\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n        )\n\n        # Iterate over chunks in the mesh\n        for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n            # This means there was no mesh for these cameras\n            if len(face_IDs) == 0:\n                continue\n\n            # Aggregate the projections from a set of cameras corresponding to\n            _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n                sub_camera_set,\n                batch_size=batch_size,\n                aggregate_img_scale=aggregate_img_scale,\n                return_all=False,\n                **kwargs,\n            )\n\n            # Increment the summed predictions and counts\n            # Make sure that nans don't propogate, since they should just be treated as zeros\n            # TODO ensure this is correct\n            summed_projections[face_IDs] = np.nansum(\n                [\n                    summed_projections[face_IDs],\n                    additional_information_submesh[\"summed_projections\"],\n                ],\n                axis=0,\n            )\n            projection_counts[face_IDs] = (\n                projection_counts[face_IDs]\n                + additional_information_submesh[\"projection_counts\"]\n            )\n\n        # Same as the parent class\n        no_projections = projection_counts == 0\n        summed_projections[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projections,\n        }\n\n        average_projections = np.divide(\n            summed_projections, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2,\n        n_polygons_per_cluster: int = 1000,\n    ):\n        \"\"\"\n        Assign a class label to polygons using labels per face. This implementation is useful for\n        large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n        implementation first clusters the polygons and labels each cluster indepenently. This makes\n        use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n        Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n        better performance.\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n            n_polygons_per_cluster: (int):\n                Set the number of clusters so there are approximately this number polygons per\n                cluster on average. Defaults to 1000\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Load in the polygons\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract the centroid of each one and convert to a numpy array\n        centroids_xy = np.stack(\n            polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n        )\n        # Determine how many clusters there should be\n        n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n        # Assign each polygon to a cluster\n        polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n        # This will be set later once we figure out the datatype of the per-cluster labels\n        all_labels = None\n\n        # Loop over the individual clusters\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n            # Determine which polygons are part of that cluster\n            cluster_mask = polygon_cluster_IDs == cluster_ID\n            # Extract the polygons from one cluster\n            cluster_polygons = polygons_gdf.iloc[cluster_mask]\n            # Compute the labeling per polygon\n            cluster_labels = super().label_polygons(\n                face_labels,\n                cluster_polygons,\n                face_weighting,\n                sjoin_overlay,\n                return_class_labels,\n                unknown_class_label,\n                buffer_dist_meters,\n            )\n            # Convert to numpy array\n            cluster_labels = np.array(cluster_labels)\n            # Create the aggregation array with the appropriate datatype\n            if all_labels is None:\n                # We assume that this list will be at least one element since each cluster\n                # should be non-empty. All values should be overwritten so the default value doesn't matter\n                all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n            # Set the appropriate elements of the full array with the newly-computed cluster labels\n            all_labels[cluster_mask] = cluster_labels\n\n        # The output is expected to be a list\n        all_labels = all_labels.tolist()\n        return all_labels\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked-functions","title":"Functions","text":""},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges. This version chunks the mesh up and performs aggregation on sub-regions to decrease the runtime.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 250.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n    mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 250.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n\n    # Initialize the values that will be incremented per cluster\n    summed_projections = np.zeros(\n        (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n    )\n    projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n    # Create a generator to generate chunked meshes\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n    )\n\n    # Iterate over chunks in the mesh\n    for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n        # This means there was no mesh for these cameras\n        if len(face_IDs) == 0:\n            continue\n\n        # Aggregate the projections from a set of cameras corresponding to\n        _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n            sub_camera_set,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            return_all=False,\n            **kwargs,\n        )\n\n        # Increment the summed predictions and counts\n        # Make sure that nans don't propogate, since they should just be treated as zeros\n        # TODO ensure this is correct\n        summed_projections[face_IDs] = np.nansum(\n            [\n                summed_projections[face_IDs],\n                additional_information_submesh[\"summed_projections\"],\n            ],\n            axis=0,\n        )\n        projection_counts[face_IDs] = (\n            projection_counts[face_IDs]\n            + additional_information_submesh[\"projection_counts\"]\n        )\n\n    # Same as the parent class\n    no_projections = projection_counts == 0\n    summed_projections[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projections,\n    }\n\n    average_projections = np.divide(\n        summed_projections, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.get_mesh_chunks_for_cameras","title":"<code>get_mesh_chunks_for_cameras(cameras, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, include_texture=False)</code>","text":"<p>Return a generator of sub-meshes, chunked to align with clusters of cameras</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The chunks of the mesh are generated by clustering the cameras</p> required <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 50.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <code>include_texture</code> <code>bool</code> <p>Should the texture from the full mesh be included in the subset mesh. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <p>pv.PolyData: The subset mesh</p> <code>PhotogrammetryCameraSet</code> <p>The cameras associated with that mesh</p> <p>np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def get_mesh_chunks_for_cameras(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    include_texture: bool = False,\n):\n    \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The chunks of the mesh are generated by clustering the cameras\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n        include_texture (bool, optional): Should the texture from the full mesh be included\n            in the subset mesh. Defaults to False.\n\n    Yields:\n        pv.PolyData: The subset mesh\n        PhotogrammetryCameraSet: The cameras associated with that mesh\n        np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n    \"\"\"\n    # Extract the points depending on whether it's a single camera or a set\n    if isinstance(cameras, PhotogrammetryCamera):\n        camera_points = [Point(*cameras.get_lon_lat())]\n    else:\n        # Get the lat lon for each camera point and turn into a shapely Point\n        camera_points = [\n            Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n        ]\n\n    # Create a geodataframe from the points\n    camera_points = gpd.GeoDataFrame(\n        geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n    )\n    # Make sure the gdf has a gemetric CRS so there is no warping of the space\n    camera_points = ensure_projected_CRS(camera_points)\n    # Extract the x, y points now in a geometric CRS\n    camera_points_numpy = np.stack(\n        camera_points.geometry.apply(lambda point: (point.x, point.y))\n    )\n\n    # Assign each camera to a cluster\n    camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n        camera_points_numpy\n    )\n    if vis_clusters:\n        # Show the camera locations, colored by which one they were assigned to\n        plt.scatter(\n            camera_points_numpy[:, 0],\n            camera_points_numpy[:, 1],\n            c=camera_cluster_IDs,\n            cmap=\"tab20\",\n        )\n        plt.show()\n\n    # Get the texture from the full mesh\n    full_mesh_texture = (\n        self.get_texture(request_vertex_texture=False) if include_texture else None\n    )\n\n    # Iterate over the clusters of cameras\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n        # Get indices of cameras for that cluster\n        matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n        # Get the segmentor camera set for the subset of the camera inds\n        sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n        # Extract the rows in the dataframe for those IDs\n        subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n        # TODO this could be accellerated by computing the membership for all points at the begining.\n        # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n        # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n        # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n        # ROIs may actually lead to slower computations than doing it sequentially\n\n        # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n        sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n            region_of_interest=subset_camera_points,\n            buffer_meters=buffer_dist_meters,\n            return_original_IDs=True,\n        )\n        # Extract the corresponding texture elements for this sub mesh if needed\n        # If include_texture=False, the full_mesh_texture will not be set\n        # If there is no mesh, the texture should also be set to None, otherwise it will be\n        # ambigious whether it's a face or vertex texture\n        sub_mesh_texture = (\n            full_mesh_texture[face_IDs]\n            if full_mesh_texture is not None and len(face_IDs) &gt; 0\n            else None\n        )\n\n        # Wrap this pyvista mesh in a photogrammetry mesh\n        sub_mesh_TPM = TexturedPhotogrammetryMesh(\n            sub_mesh_pv, texture=sub_mesh_texture\n        )\n\n        # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n        # face IDs mapping the faces in the sub mesh back to the full one\n        yield sub_mesh_TPM, sub_camera_set, face_IDs\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2, n_polygons_per_cluster=1000)</code>","text":"<p>Assign a class label to polygons using labels per face. This implementation is useful for large numbers of polygons. To make the expensive sjoin/overlay more efficient, this implementation first clusters the polygons and labels each cluster indepenently. This makes use of the fact that the mesh faces around this cluster can be extracted relatively quickly. Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to better performance.</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2</code> <code>n_polygons_per_cluster</code> <code>int</code> <p>(int): Set the number of clusters so there are approximately this number polygons per cluster on average. Defaults to 1000</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2,\n    n_polygons_per_cluster: int = 1000,\n):\n    \"\"\"\n    Assign a class label to polygons using labels per face. This implementation is useful for\n    large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n    implementation first clusters the polygons and labels each cluster indepenently. This makes\n    use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n    Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n    better performance.\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n        n_polygons_per_cluster: (int):\n            Set the number of clusters so there are approximately this number polygons per\n            cluster on average. Defaults to 1000\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Load in the polygons\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract the centroid of each one and convert to a numpy array\n    centroids_xy = np.stack(\n        polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n    )\n    # Determine how many clusters there should be\n    n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n    # Assign each polygon to a cluster\n    polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n    # This will be set later once we figure out the datatype of the per-cluster labels\n    all_labels = None\n\n    # Loop over the individual clusters\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n        # Determine which polygons are part of that cluster\n        cluster_mask = polygon_cluster_IDs == cluster_ID\n        # Extract the polygons from one cluster\n        cluster_polygons = polygons_gdf.iloc[cluster_mask]\n        # Compute the labeling per polygon\n        cluster_labels = super().label_polygons(\n            face_labels,\n            cluster_polygons,\n            face_weighting,\n            sjoin_overlay,\n            return_class_labels,\n            unknown_class_label,\n            buffer_dist_meters,\n        )\n        # Convert to numpy array\n        cluster_labels = np.array(cluster_labels)\n        # Create the aggregation array with the appropriate datatype\n        if all_labels is None:\n            # We assume that this list will be at least one element since each cluster\n            # should be non-empty. All values should be overwritten so the default value doesn't matter\n            all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n        # Set the appropriate elements of the full array with the newly-computed cluster labels\n        all_labels[cluster_mask] = cluster_labels\n\n    # The output is expected to be a list\n    all_labels = all_labels.tolist()\n    return all_labels\n</code></pre>"},{"location":"API_reference/meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, n_clusters=8, buffer_dist_meters=CHUNKED_MESH_BUFFER_DIST_METERS, vis_clusters=False, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output. This version first clusters the cameras, extracts a region of the mesh surrounding each cluster of cameras, and then performs rendering on each sub-region.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters to break the cameras into. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>How far around the cameras to include the mesh. Defaults to 50.</p> <code>CHUNKED_MESH_BUFFER_DIST_METERS</code> <code>vis_clusters</code> <code>bool</code> <p>Should the clusters of camera locations be shown. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = CHUNKED_MESH_BUFFER_DIST_METERS,\n    vis_clusters: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output.\n    This version first clusters the cameras, extracts a region of the mesh surrounding each\n    cluster of cameras, and then performs rendering on each sub-region.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n        n_clusters (int, optional):\n            Number of clusters to break the cameras into. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            How far around the cameras to include the mesh. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the clusters of camera locations be shown. Defaults to False.\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    # Create a generator to chunked meshes based on clusters of cameras\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n        include_texture=True,\n    )\n\n    for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n        chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n    ):\n        # Create the render generator\n        render_gen = sub_mesh_TPM.render_flat(\n            sub_camera_set,\n            batch_size=batch_size,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n        # Yield items from the returned generator\n        for render_item in render_gen:\n            yield render_item\n\n        # This is another attempt to free memory\n        sub_mesh_TPM.pix2face_plotter.deep_clean()\n        # There's a strange memory leak, I think it may be because of the sub-mesh sticking around\n        print(\"About to delete sub mesh\")\n        del sub_mesh_TPM\n</code></pre>"},{"location":"API_reference/meshes/meshes/","title":"Mesh Docstrings","text":""},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh","title":"<code>TexturedPhotogrammetryMesh</code>","text":"Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>class TexturedPhotogrammetryMesh:\n    def __init__(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        downsample_target: float = 1.0,\n        transform_filename: PATH_TYPE = None,\n        texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n        texture_column_name: typing.Union[PATH_TYPE, None] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n        ROI=None,\n        ROI_buffer_meters: float = 0,\n        require_transform: bool = False,\n        log_level: str = \"INFO\",\n    ):\n        \"\"\"_summary_\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]): Path to the mesh, in a format pyvista can read, or pyvista mesh\n            downsample_target (float, optional): Downsample to this fraction of vertices. Defaults to 1.0.\n            texture (typing.Union[PATH_TYPE, np.ndarray, None]): Texture or path to one. See more details in `load_texture` documentation\n            texture_column_name: The name of the column to use for a vectorfile input\n            IDs_to_labels (typing.Union[PATH_TYPE, dict, None]): dictionary or JSON file containing the mapping from integer IDs to string class names\n        \"\"\"\n        self.downsample_target = downsample_target\n\n        self.pyvista_mesh = None\n        self.texture = None\n        self.vertex_texture = None\n        self.face_texture = None\n        self.local_to_epgs_4978_transform = None\n        self.IDs_to_labels = None\n        # Create the plotter that will later be used to compute correspondences between pixels\n        # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n        # plotters. See https://github.com/pyvista/pyvista/issues/2252\n        self.pix2face_plotter = create_pv_plotter(off_screen=True)\n        self.face_polygons_cache = {}\n        self.face_2d_3d_ratios_cache = {}\n\n        self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n        self.logger.setLevel(log_level)\n        # Potentially necessary for Jupyter\n        # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n        # If you don't check that there's already a handler, you can have situations with duplicated\n        # print outs if you have multiple mesh objects\n        if not self.logger.hasHandlers():\n            self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n        # Load the transform\n        self.logger.info(\"Loading transform to EPSG:4326\")\n        self.load_transform_to_epsg_4326(\n            transform_filename, require_transform=require_transform\n        )\n        # Load the mesh with the pyvista loader\n        self.logger.info(\"Loading mesh\")\n        self.load_mesh(\n            mesh=mesh,\n            downsample_target=downsample_target,\n            ROI=ROI,\n            ROI_buffer_meters=ROI_buffer_meters,\n        )\n        # Load the texture\n        self.logger.info(\"Loading texture\")\n        # load IDs_to_labels\n        # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n        if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n            possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n            if possible_json.exists():\n                IDs_to_labels = possible_json\n        # convert IDs_to_labels from file to dict\n        if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n            with open(IDs_to_labels, \"r\") as file:\n                IDs_to_labels = json.load(file)\n                IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n        self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n\n    # Setup methods\n    def load_mesh(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        downsample_target: float = 1.0,\n        ROI=None,\n        ROI_buffer_meters=0,\n        ROI_simplify_tol_meters=2,\n    ):\n        \"\"\"Load the pyvista mesh and create the texture\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n                Path to the mesh or actual mesh\n            downsample_target (float, optional):\n                What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n            ROI:\n                See select_mesh_ROI. Defaults to None\n            ROI_buffer_meters:\n                See select_mesh_ROI. Defaults to 0.\n            ROI_simplify_tol_meters:\n                See select_mesh_ROI. Defaults to 2.\n        \"\"\"\n        if isinstance(mesh, pv.PolyData):\n            self.pyvista_mesh = mesh\n        else:\n            # Load the mesh using pyvista\n            # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n            self.logger.info(\"Reading the mesh\")\n            self.pyvista_mesh = pv.read(mesh)\n\n        self.logger.info(\"Selecting an ROI from mesh\")\n        # Select a region of interest if needed\n        self.pyvista_mesh = self.select_mesh_ROI(\n            region_of_interest=ROI,\n            buffer_meters=ROI_buffer_meters,\n            simplify_tol_meters=ROI_simplify_tol_meters,\n        )\n\n        # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n        if downsample_target != 1.0:\n            # TODO try decimate_pro and compare quality and runtime\n            # TODO see if there's a way to preserve the mesh colors\n            # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n            self.logger.info(\"Downsampling the mesh\")\n            # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n            downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n                target_reduction=(1 - downsample_target)\n            )\n            self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n        self.logger.info(\"Extracting faces from mesh\")\n        # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n        self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n\n    def transfer_texture(self, downsampled_mesh):\n        \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n        Args:\n            downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n        Returns:\n            pv.PolyData: The downsampled mesh with the transferred textures\n        \"\"\"\n        # Only transfer textures if there are point based scalars in the original mesh\n        if self.pyvista_mesh.point_data:\n            # Store original mesh points in KDTree for nearest neighbor search\n            kdtree = KDTree(self.pyvista_mesh.points)\n\n            # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n            _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n            # Iterate over all the point based scalars\n            for scalar_name in self.pyvista_mesh.point_data.keys():\n                # Retrieve scalar data of appropriate index using the nearest neighbor indices\n                transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                    nearest_neighbor_indices\n                ]\n                # Set the corresponding scalar data in the downsampled mesh\n                downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n            # Set active mesh of downsampled mesh\n            if self.pyvista_mesh.active_scalars_name:\n                downsampled_mesh.active_scalars_name = (\n                    self.pyvista_mesh.active_scalars_name\n                )\n        else:\n            self.logger.warning(\n                \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n            )\n        return downsampled_mesh\n\n    def load_transform_to_epsg_4326(\n        self, transform_filename: PATH_TYPE, require_transform: bool = False\n    ):\n        \"\"\"\n        Load the 4x4 transform projects points from their local coordnate system into EPSG:4326,\n        the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying\n        it directly or extracted from a Metashape camera output\n\n        Args\n            transform_filename (PATH_TYPE):\n            require_transform (bool): Does a local-to-global transform file need to be available\"\n        Raises:\n            FileNotFoundError: Cannot find texture file\n            ValueError: Transform file doesn't have 4x4 matrix\n        \"\"\"\n        if transform_filename is None:\n            if require_transform:\n                raise ValueError(\"Transform is required but not provided\")\n            # If not required, do nothing. TODO consider adding a warning\n            return\n\n        elif Path(transform_filename).suffix == \".xml\":\n            self.local_to_epgs_4978_transform = parse_transform_metashape(\n                transform_filename\n            )\n        elif Path(transform_filename).suffix == \".csv\":\n            self.local_to_epgs_4978_transform = np.loadtxt(\n                transform_filename, delimiter=\",\"\n            )\n            if self.local_to_epgs_4978_transform.shape != (4, 4):\n                raise ValueError(\n                    f\"Transform should be (4,4) but is {self.local_to_epgs_4978_transform.shape}\"\n                )\n        else:\n            if require_transform:\n                raise ValueError(\n                    f\"Transform could not be loaded from {transform_filename}\"\n                )\n            # Not set\n            return\n\n    def standardize_texture(self, texture_array: np.ndarray):\n        # TODO consider coercing into a numpy array\n\n        # Check the dimensions\n        if texture_array.ndim == 1:\n            texture_array = np.expand_dims(texture_array, axis=1)\n        elif texture_array.ndim != 2:\n            raise ValueError(\n                f\"Input texture should have 1 or 2 dimensions but instead has {texture_array.ndim}\"\n            )\n        return texture_array\n\n    def get_texture(\n        self,\n        request_vertex_texture: typing.Union[bool, None] = None,\n        try_verts_faces_conversion: bool = True,\n    ):\n        if self.vertex_texture is None and self.face_texture is None:\n            return\n\n        # If this is unset, try to infer it\n        if request_vertex_texture is None:\n            if self.vertex_texture is not None and self.face_texture is not None:\n                raise ValueError(\n                    \"Ambigious which texture is requested, set request_vertex_texture appropriately\"\n                )\n\n            # Assume that the only one available is being requested\n            request_vertex_texture = self.vertex_texture is not None\n\n        if request_vertex_texture:\n            if self.vertex_texture is not None:\n                return self.standardize_texture(self.vertex_texture)\n            elif try_verts_faces_conversion:\n                self.set_texture(self.face_to_vert_texture(self.face_texture))\n                self.vertex_texture\n            else:\n                raise ValueError(\n                    \"Vertex texture not present and conversion was not requested\"\n                )\n        else:\n            if self.face_texture is not None:\n                return self.standardize_texture(self.face_texture)\n            elif try_verts_faces_conversion:\n                face_texture = self.vert_to_face_texture(\n                    self.vertex_texture, discrete=self.is_discrete_texture()\n                )\n                self.set_texture(face_texture)\n                return self.face_texture\n            else:\n                raise ValueError(\n                    \"Face texture not present and conversion was not requested\"\n                )\n\n    def is_discrete_texture(self):\n        return self.IDs_to_labels is not None\n\n    def set_texture(\n        self,\n        texture_array: np.ndarray,\n        IDs_to_labels: typing.Union[None, dict] = None,\n        all_discrete_texture_values: typing.Union[typing.List, None] = None,\n        is_vertex_texture: typing.Union[bool, None] = None,\n        use_derived_IDs_to_labels: bool = False,\n        delete_existing: bool = True,\n    ):\n        \"\"\"Set the internal texture representation\n\n        Args:\n            texture_array (np.ndarray):\n                The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.\n            IDs_to_labels (typing.Union[None, dict], optional): Mapping from integer IDs to string names. Defaults to None.\n            all_discrete_texture_values (typing.Union[typing.List, None], optional):\n                Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.\n            is_vertex_texture (typing.Union[bool, None], optional):\n                Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.\n            use_derived_IDs_to_labels (bool, optional): Use IDs to labels derived from data if not explicitly provided. Defaults to False.\n            delete_existing (bool, optional): Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n\n        Raises:\n            ValueError: If the size of the texture doesn't match the number of either faces or vertices\n            ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n        \"\"\"\n        texture_array = self.standardize_texture(texture_array)\n        # IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n\n        # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n        # TODO consider refactoring to check whether it matches the number of one of them,\n        # no matter whether is_vertex_texture is specified\n        if is_vertex_texture is None:\n            # Check that the number of matches face or verts\n            n_values = texture_array.shape[0]\n            n_faces = self.faces.shape[0]\n            n_verts = self.pyvista_mesh.points.shape[0]\n\n            if n_verts == n_faces:\n                raise ValueError(\n                    \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n                )\n            elif n_values == n_verts:\n                is_vertex_texture = True\n            elif n_values == n_faces:\n                is_vertex_texture = False\n            else:\n                raise ValueError(\n                    f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n                )\n\n        # Ensure that the actual data type is float, and record label names\n        if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n            # If it is more than one column, it's assumed to be a real-valued\n            # quantity and we try to cast it to a float\n            texture_array = texture_array.astype(float)\n            derived_IDs_to_labels = None\n        else:\n            texture_array, derived_IDs_to_labels = ensure_float_labels(\n                texture_array, full_array=all_discrete_texture_values\n            )\n\n        # If IDs to labels is explicitly provided, trust that\n        # TODO should do some type checking here\n        if isinstance(IDs_to_labels, dict):\n            self.IDs_to_labels = IDs_to_labels\n        # If not, but we can compute it, use that. Otherwise, we might want to force them to be set to None\n        elif use_derived_IDs_to_labels:\n            self.IDs_to_labels = derived_IDs_to_labels\n\n        # Set the appropriate texture and optionally delete the other one\n        if is_vertex_texture:\n            self.vertex_texture = texture_array\n            if delete_existing:\n                self.face_texture = None\n        else:\n            self.face_texture = texture_array\n            if delete_existing:\n                self.vertex_texture = None\n\n    def load_texture(\n        self,\n        texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n        texture_column_name: typing.Union[None, PATH_TYPE] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    ):\n        \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n           one will be left as None\n\n        Args:\n            texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n                * A numpy array file in \".npy\" format\n                * A vector file readable by geopandas and a label(s) specifying which column to use.\n                  This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n                  regions with different labels. These regions may be assigned based on the order of the rows.\n                * A raster file readable by rasterio. We may want to support using a subset of bands\n            texture_column_name: The column to use as the label for a vector data input\n            IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n        \"\"\"\n        # The easy case, a texture is passed in directly\n        if isinstance(texture, np.ndarray):\n            self.set_texture(\n                texture_array=texture,\n                IDs_to_labels=IDs_to_labels,\n                use_derived_IDs_to_labels=True,\n            )\n        # If the texture is None, try to load it from the mesh\n        # Note that this requires us to have not decimated yet\n        elif texture is None:\n            # See if the mesh has a texture, else this will be None\n            texture_array = self.pyvista_mesh.active_scalars\n\n            if texture_array is not None:\n                # Check if this was a really one channel that had to be tiled to\n                # three for saving\n                if len(texture_array.shape) == 2:\n                    min_val_per_row = np.min(texture_array, axis=1)\n                    max_val_per_row = np.max(texture_array, axis=1)\n                    if np.array_equal(min_val_per_row, max_val_per_row):\n                        # This is supposted to be one channel\n                        texture_array = texture_array[:, 0].astype(float)\n                        # Set any values that are the ignore int value to nan\n                texture_array = texture_array.astype(float)\n                texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n                self.set_texture(\n                    texture_array,\n                    IDs_to_labels=IDs_to_labels,\n                    use_derived_IDs_to_labels=True,\n                )\n            else:\n                if IDs_to_labels is not None:\n                    self.IDs_to_labels = IDs_to_labels\n                # Assume that no texture will be needed, consider printing a warning\n                self.logger.warn(\"No texture provided\")\n        else:\n            # Try handling all the other supported filetypes\n            texture_array = None\n            all_values = None\n\n            # Name of scalar in the mesh\n            try:\n                self.logger.warn(\n                    \"Trying to read texture as a scalar from the pyvista mesh:\"\n                )\n                texture_array = self.pyvista_mesh[texture]\n                self.logger.warn(\"- success\")\n            except (KeyError, ValueError):\n                self.logger.warn(\"- failed\")\n\n            # Numpy file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as a numpy file:\")\n                    texture_array = np.load(texture, allow_pickle=True)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Vector file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as vector file:\")\n                    # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                    texture_array, all_values = self.get_values_for_verts_from_vector(\n                        column_names=texture_column_name,\n                        vector_source=texture,\n                    )\n                    self.logger.warn(\"- success\")\n                except (IndexError, fiona.errors.DriverError):\n                    self.logger.warn(\"- failed\")\n\n            # Raster file\n            if texture_array is None:\n                try:\n                    # TODO\n                    self.logger.warn(\"Trying to read as texture as raster file: \")\n                    texture_array = self.get_vert_values_from_raster_file(texture)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Error out if not set, since we assume the intent was to have a texture at this point\n            if texture_array is None:\n                raise ValueError(f\"Could not load texture for {texture}\")\n\n            # This will error if something is wrong with the texture that was loaded\n            self.set_texture(\n                texture_array,\n                all_discrete_texture_values=all_values,\n                use_derived_IDs_to_labels=True,\n                IDs_to_labels=IDs_to_labels,\n            )\n\n    def select_mesh_ROI(\n        self,\n        region_of_interest: typing.Union[\n            gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n        ],\n        buffer_meters: float = 0,\n        simplify_tol_meters: int = 0,\n        default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n        return_original_IDs: bool = False,\n    ):\n        \"\"\"Get a subset of the mesh based on geospatial data\n\n        Args:\n            region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n                Region of interest. Can be a\n                * dataframe, where all columns will be colapsed\n                * A shapely polygon/multipolygon\n                * A file that can be loaded by geopandas\n            buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n            simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n            default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n            return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n        Returns:\n            pyvista.PolyData: The subset of the mesh\n            np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n            np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n        \"\"\"\n        if region_of_interest is None:\n            return self.pyvista_mesh\n\n        # Get the ROI into a geopandas GeoDataFrame\n        self.logger.info(\"Standardizing ROI\")\n        if isinstance(region_of_interest, gpd.GeoDataFrame):\n            ROI_gpd = region_of_interest\n        elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n            ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n        else:\n            ROI_gpd = gpd.read_file(region_of_interest)\n\n        self.logger.info(\"Dissolving ROI\")\n        # Disolve to ensure there is only one row\n        ROI_gpd = ROI_gpd.dissolve()\n        self.logger.info(\"Setting CRS and buffering ROI\")\n        # Make sure we're using a projected CRS so a buffer can be applied\n        ROI_gpd = ensure_projected_CRS(ROI_gpd)\n        # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n        ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n        # Simplify the geometry to reduce the computational load\n        ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n        self.logger.info(\"Dissolving buffered ROI\")\n        # Disolve again in case\n        ROI_gpd = ROI_gpd.dissolve()\n\n        self.logger.info(\"Extracting verts for dataframe\")\n        # Get the vertices as a dataframe in the same CRS\n        verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n        self.logger.info(\"Checking intersection of verts with ROI\")\n        # Determine which vertices are within the ROI polygon\n        verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n        # Extract the IDs of the set within the polygon\n        vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n        self.logger.info(\"Extracting points from pyvista mesh\")\n        # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n        subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n        self.logger.info(\"Extraction surface from subset mesh\")\n        # Convert the unstructured grid to a PolyData (mesh) again\n        subset_mesh = subset_unstructured_grid.extract_surface()\n\n        # If we need the indices into the original mesh, return those\n        if return_original_IDs:\n            try:\n                point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n                face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n            except KeyError:\n                point_IDs = np.array([])\n                face_IDs = np.array([])\n\n            return (\n                subset_mesh,\n                point_IDs,\n                face_IDs,\n            )\n        # Else return just the mesh\n        return subset_mesh\n\n    def add_label(self, label_name, label_ID):\n        if label_ID is not np.nan:\n            self.IDs_to_labels[label_ID] = label_name\n\n    def get_IDs_to_labels(self):\n        return self.IDs_to_labels\n\n    def get_label_names(self):\n        self.logger.warning(\n            \"This method will be deprecated in favor of get_IDs_to_labels since it doesn't handle non-sequential indices\"\n        )\n        if self.IDs_to_labels is None:\n            return None\n        return list(self.IDs_to_labels.values())\n\n    # Vertex methods\n\n    def transform_vertices(self, transform_4x4: np.ndarray, in_place: bool = False):\n        \"\"\"Apply a transform to the vertex coordinates\n\n        Args:\n            transform_4x4 (np.ndarray): Transform to be applied\n            in_place (bool): Should the vertices be updated for all member objects\n        \"\"\"\n        homogenous_local_points = np.vstack(\n            (self.pyvista_mesh.points.T, np.ones(self.pyvista_mesh.points.shape[0]))\n        )\n        transformed_local_points = transform_4x4 @ homogenous_local_points\n        transformed_local_points = transformed_local_points[:3].T\n\n        # Overwrite existing vertices in both pytorch3d and pyvista mesh\n        if in_place:\n            self.pyvista_mesh.points = transformed_local_points.copy()\n        return transformed_local_points\n\n    def get_vertices_in_CRS(\n        self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n    ):\n        \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n        Args:\n            output_CRS (pyproj.CRS): The coordinate reference system to transform to\n            force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n        Returns:\n            np.ndarray: (n_points, 3)\n        \"\"\"\n        # If no CRS is requested, just return the points\n        if output_CRS is None:\n            return self.pyvista_mesh.points\n\n        # The mesh points are defined in an arbitrary local coordinate system but we can transform them to EPGS:4978,\n        # the earth-centered, earth-fixed coordinate system, using an included transform\n        epgs4978_verts = self.transform_vertices(self.local_to_epgs_4978_transform)\n\n        # TODO figure out why this conversion was required. I think it was some typing issue\n        output_CRS = pyproj.CRS.from_epsg(output_CRS.to_epsg())\n        # Build a pyproj transfrormer from EPGS:4978 to the desired CRS\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_CRS, output_CRS\n        )\n\n        # Transform the coordinates\n        verts_in_output_CRS = transformer.transform(\n            xx=epgs4978_verts[:, 0],\n            yy=epgs4978_verts[:, 1],\n            zz=epgs4978_verts[:, 2],\n        )\n        # Stack and transpose\n        verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n        # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n        # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n        # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n        if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n            # Swap first two columns\n            verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n        return verts_in_output_CRS\n\n    def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n        \"\"\"Obtain the vertices as a dataframe\n\n        Args:\n            crs (pyproj.CRS): The CRS to use\n\n        Returns:\n            gpd.GeoDataFrame: A dataframe with all the vertices\n        \"\"\"\n        # Get the vertices in the same CRS as the geofile\n        verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n        df = pd.DataFrame(\n            {\n                \"east\": verts_in_geopolygon_crs[:, 0],\n                \"north\": verts_in_geopolygon_crs[:, 1],\n            }\n        )\n        # Create a column of Point objects to use as the geometry\n        df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n        points = gpd.GeoDataFrame(df, crs=crs)\n\n        # Add an index column because the normal index will not be preserved in future operations\n        points[VERT_ID] = df.index\n\n        return points\n\n    def get_faces_2d_gdf(\n        self,\n        crs: pyproj.CRS,\n        include_3d_2d_ratio: bool = False,\n        data_dict: dict = {},\n        faces_mask: typing.Union[np.ndarray, None] = None,\n        cache_data: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n        Args:\n            crs (pyproj.CRS):\n                Coordinate reference system of the dataframe\n            include_3d_2d_ratio (bool, optional):\n                Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n                slope of the face relative to horizontal. The computed data will be stored in the\n                column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n            data_dict (dict, optional):\n                Additional information to add to the dataframe. It must be a dict where the keys\n                are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n                Defaults to {}.\n            faces_mask (typing.Union[np.ndarray, None], optional):\n                A binary mask corresponding to which faces to return. Used to improve runtime of\n                creating the dataframe or downstream steps. Defaults to None.\n            cache_data (bool):\n                Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n        Returns:\n            geopandas.GeoDataFrame: A dataframe for each triangular face\n        \"\"\"\n        # Computing this data can be slow, and we might call it multiple times. This is especially\n        # true for doing clustered polygon labeling\n        if cache_data:\n            mesh_hash = self.get_mesh_hash()\n            transform_hash = self.get_transform_hash()\n            faces_mask_hash = hash(\n                faces_mask.tobytes() if faces_mask is not None else 0\n            )\n            # Create a key that uniquely identifies the relavant inputs\n            cache_key = (mesh_hash, transform_hash, faces_mask_hash, crs)\n\n            # See if the face polygons were in the cache. If not, None will be returned\n            cached_values = self.face_polygons_cache.get(cache_key)\n        else:\n            cached_values = None\n\n        if cached_values is not None:\n            face_polygons, faces = cached_values\n            logging.info(\"Using cached face polygons\")\n        else:\n            self.logger.info(\"Computing faces in working CRS\")\n            # Get the mesh vertices in the desired export CRS\n            verts_in_crs = self.get_vertices_in_CRS(crs)\n            # Get a triangle in geospatial coords for each face\n            # (n_faces, 3 points, xyz)\n            faces = verts_in_crs[self.faces]\n\n            # Select only the requested faces\n            if faces_mask is not None:\n                faces = faces[faces_mask]\n\n            # Extract the first two columns and convert them to a list of tuples of tuples\n            faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n            face_polygons = [\n                Polygon(face_tuple)\n                for face_tuple in tqdm(\n                    faces_2d_tuples, desc=f\"Converting faces to polygons\"\n                )\n            ]\n            self.logger.info(\"Creating dataframe of faces\")\n\n            if cache_data:\n                # Save computed data to the cache for the future\n                self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n        # Remove data corresponding to masked faces\n        if faces_mask is not None:\n            data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n        # Compute the ratio between the 3D area and the projected top-down 2D area\n        if include_3d_2d_ratio:\n            if cache_data:\n                # Check if ratios are cached\n                ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n            else:\n                ratios = None\n\n            # Ratios need to be computed\n            if ratios is None:\n                ratios = []\n                for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                    area, area_2d = compute_3D_triangle_area(face)\n                    ratios.append(area / area_2d)\n\n                if cache_data:\n                    self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n            # Add the ratios to the data dict\n            data_dict[RATIO_3D_2D_KEY] = ratios\n\n        # Create the dataframe\n        faces_gdf = gpd.GeoDataFrame(\n            data=data_dict,\n            geometry=face_polygons,\n            crs=crs,\n        )\n\n        return faces_gdf\n\n    # Transform labels face&lt;-&gt;vertex methods\n\n    def face_to_vert_texture(self, face_IDs):\n        \"\"\"_summary_\n\n        Args:\n            face_IDs (np.array): (n_faces,) The integer IDs of the faces\n        \"\"\"\n        raise NotImplementedError()\n        # TODO figure how to have a NaN class that\n        for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n            # Find which faces are using this vertex\n            matching = np.sum(self.faces == i, axis=1)\n            # matching_inds = np.where(matching)[0]\n            # matching_IDs = face_IDs[matching_inds]\n            # most_common_ind = Counter(matching_IDs).most_common(1)\n\n    def vert_to_face_texture(self, vert_IDs, discrete=True):\n        if vert_IDs is None:\n            raise ValueError(\"None\")\n\n        vert_IDs = np.squeeze(vert_IDs)\n        if vert_IDs.ndim != 1 and discrete:\n            raise ValueError(\n                f\"Can only perform discrete conversion with one dimensional array but instead had {vert_IDs.ndim}\"\n            )\n\n        # Each row contains the IDs of each vertex\n        values_per_face = vert_IDs[self.faces]\n        if discrete:\n            # Now we need to \"vote\" for the best one\n            max_ID = np.nanmax(vert_IDs)\n            # This means that all textures are nans\n            if not np.isfinite(max_ID):\n                self.logger.warn(\n                    \"In vertex to face texture conversion, all nans encountered\"\n                )\n                # Return all nans\n                return np.full(values_per_face.shape[0], fill_value=np.nan)\n\n            max_ID = int(max_ID)\n            # TODO consider using unique if these indices are sparse\n            counts_per_class_per_face = np.array(\n                [np.sum(values_per_face == i, axis=1) for i in range(max_ID + 1)]\n            ).T\n            # Check which entires had no classes reported and mask them out\n            # TODO consider removing these rows beforehand\n            zeros_mask = np.all(counts_per_class_per_face == 0, axis=1)\n            # We want to fairly tiebreak since np.argmax will always take th first index\n            # This is hard to do in a vectorized way, so we just add a small random value\n            # independently to each element\n            counts_per_class_per_face = (\n                counts_per_class_per_face\n                + np.random.random(counts_per_class_per_face.shape) * 0.5\n            )\n            most_common_class_per_face = np.argmax(\n                counts_per_class_per_face, axis=1\n            ).astype(float)\n            # Set any faces with zero counts to nan\n            most_common_class_per_face[zeros_mask] = np.nan\n\n            return most_common_class_per_face\n        else:\n            average_value_per_face = np.mean(values_per_face, axis=1)\n            return average_value_per_face\n\n    # Operations on vector data\n    def get_values_for_verts_from_vector(\n        self,\n        vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n        column_names: typing.Union[str, typing.List[str]],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the value from a dataframe for each vertex\n\n        Args:\n            vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n            column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n        Returns:\n            np.ndarray: Array of values for each vertex if there is one column name or\n            dict[np.ndarray]: A dict mapping from column names to numpy arrays\n        \"\"\"\n        # Lead the vector data if not already provided in memory\n        if isinstance(vector_source, gpd.GeoDataFrame):\n            gdf = vector_source\n        else:\n            # This will error if not readable\n            gdf = gpd.read_file(vector_source)\n\n        # Infer or standardize the column names\n        if column_names is None:\n            # Check if there is only one real column\n            if len(gdf.columns) == 2:\n                column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n            else:\n                # Log as well since this may be caught by an exception handler,\n                # and it's a user error that can be corrected\n                self.logger.error(\n                    \"No column name provided and ambigious which column to use\"\n                )\n                raise ValueError(\n                    \"No column name provided and ambigious which column to use\"\n                )\n        # If only one column is provided, make it a one-length list\n        elif isinstance(column_names, str):\n            column_names = [column_names]\n\n        # Get a dataframe of vertices\n        verts_df = self.get_verts_geodataframe(gdf.crs)\n\n        # See which vertices are in the geopolygons\n        points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n        # Get the index array\n        index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n        # This is one entry per vertex\n        labeled_verts_dict = {}\n        all_values_dict = {}\n        # Extract the data from each\n        for column_name in column_names:\n            # Create an array corresponding to all the points and initialize to NaN\n            column_values = points_in_polygons_gdf[column_name]\n            # TODO clean this up\n            if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n                # TODO be set to the default value for the type of the column\n                null_value = \"null\"\n            elif column_values.dtype == int:\n                null_value = 255\n            else:\n                null_value = np.nan\n            # Create an array, one per vertex, with the null value\n            values = np.full(\n                shape=verts_df.shape[0],\n                dtype=column_values.dtype,\n                fill_value=null_value,\n            )\n            # Assign the labeled values\n            values[index_array] = column_values\n\n            # Record the results\n            labeled_verts_dict[column_name] = values\n            all_values_dict[column_name] = gdf[column_name]\n\n        # If only one name was requested, just return that\n        if len(column_names) == 1:\n            labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n            all_values = np.array(list(all_values_dict.values())[0])\n\n            return labeled_verts, all_values\n        # Else return a dict of all requested values\n        return labeled_verts_dict, all_values_dict\n\n    def save_IDs_to_labels(self, savepath: PATH_TYPE):\n        \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n        Args:\n            savepath (PATH_TYPE): path to the file where the data must be saved\n        \"\"\"\n\n        # Save the classes filename\n        ensure_containing_folder(savepath)\n        if self.is_discrete_texture():\n            self.logger.info(\"discrete texture, saving classes\")\n            self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n            with open(savepath, \"w\") as outfile_h:\n                json.dump(\n                    self.get_IDs_to_labels(), outfile_h, ensure_ascii=False, indent=4\n                )\n        else:\n            self.logger.warn(\"non-discrete texture, not saving classes\")\n\n    def save_mesh(self, savepath: PATH_TYPE, save_vert_texture: bool = True):\n        # TODO consider moving most of this functionality to a utils file\n        if save_vert_texture:\n            vert_texture = self.get_texture(request_vertex_texture=True)\n            n_channels = vert_texture.shape[1]\n\n            if n_channels == 1:\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                vert_texture = np.tile(vert_texture, reps=(1, 3))\n            if n_channels &gt; 3:\n                self.logger.warning(\n                    \"Too many channels to save, attempting to treat them as class probabilities and take the argmax\"\n                )\n                # Take the argmax\n                vert_texture = np.nanargmax(vert_texture, axis=1, keepdims=True)\n                # Replace nan with 255\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                # Expand to the right number of channels\n                vert_texture = np.repeat(vert_texture, repeats=(1, 3))\n\n            vert_texture = vert_texture.astype(np.uint8)\n        else:\n            vert_texture = None\n\n        # Create folder if it doesn't exist\n        ensure_containing_folder(savepath)\n        # Actually save the mesh\n        self.pyvista_mesh.save(savepath, texture=vert_texture)\n        self.save_IDs_to_labels(Path(savepath).stem + \"_IDs_to_labels.json\")\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2.0,\n    ):\n        \"\"\"Assign a class label to polygons using labels per face\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Premptive error checking before expensive operations\n        face_labels = np.squeeze(face_labels)\n        if face_labels.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n            )\n        if face_weighting is not None:\n            face_weighting = np.squeeze(face_weighting)\n            if face_weighting.ndim != 1:\n                raise ValueError(\n                    f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n                )\n\n        # Ensure that the input is a geopandas dataframe\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract just the geometry\n        polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n        # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n        faces_mask = np.isfinite(face_labels)\n\n        # Get the faces of the mesh as a geopandas dataframe\n        # Include the predicted face labels as a column in the dataframe\n        faces_2d_gdf = self.get_faces_2d_gdf(\n            polygons_gdf.crs,\n            include_3d_2d_ratio=True,\n            data_dict={CLASS_ID_KEY: face_labels},\n            faces_mask=faces_mask,\n            cache_data=True,\n        )\n\n        # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n        if face_weighting is not None:\n            face_weighting = face_weighting[faces_mask]\n            faces_2d_gdf[\"face_weighting\"] = (\n                faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n            )\n        # If not, just use the ratio\n        else:\n            faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n        # Set the precision to avoid approximate coliniearity errors\n        faces_2d_gdf.geometry = shapely.set_precision(\n            faces_2d_gdf.geometry.values, 1e-6\n        )\n        polygons_gdf.geometry = shapely.set_precision(\n            polygons_gdf.geometry.values, 1e-6\n        )\n\n        # Set the ID field so it's available after the overlay operation\n        # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n        # one and the index would not start from 0\n        polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n        # Since overlay is expensive, we first discard faces that are not near the polygons\n\n        # Dissolve the polygons to form one ROI\n        merged_polygons = polygons_gdf.dissolve()\n        # Try to decrease the number of elements in the polygon by expanding\n        # and then simplifying the number of elements in the polygon\n        merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n        merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n        # Determine which face IDs intersect the ROI. This is slow\n        start = time()\n        self.logger.info(\"Starting to subset to ROI\")\n\n        # Check which faces are fully within the buffered regions around the query polygons\n        # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n        # to compute more information than something like gpd.within\n        contained_faces = gpd.sjoin(\n            faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n        )[\"index_right\"].notna()\n        faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n        self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n        start = time()\n        self.logger.info(\"Starting `overlay`\")\n        if sjoin_overlay:\n            overlay = gpd.sjoin(\n                faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n            )\n            self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n        else:\n            # Drop faces not included\n            overlay = polygons_gdf.overlay(\n                faces_2d_gdf, how=\"identity\", keep_geom_type=False\n            )\n            self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n        # Drop nan, for geometries that don't intersect the polygons\n        overlay.dropna(inplace=True)\n        # Compute the weighted area for each face, which may have been broken up by the overlay\n        overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n        # Extract only the neccessary columns\n        overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n        aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n        # Compute the highest weighted class prediction\n        # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n        max_rows = aggregated_data.loc[\n            aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n                \"weighted_area\"\n            ].idxmax()\n        ].reset_index()\n\n        # Make the class predictions a list of IDs with nans where no information is available\n        pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n        pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n        predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n        predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n            pred_subset_IDs\n        )\n        predicted_class_IDs = predicted_class_IDs.tolist()\n\n        # Post-process to string label names if requested and IDs_to_labels exists\n        if return_class_labels and (\n            (IDs_to_labels := self.get_IDs_to_labels()) is not None\n        ):\n            # convert the IDs into labels\n            # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n            predicted_class_IDs = [\n                (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n                for pi in predicted_class_IDs\n            ]\n        return predicted_class_IDs\n\n    def export_face_labels_vector(\n        self,\n        face_labels: typing.Union[np.ndarray, None] = None,\n        export_file: PATH_TYPE = None,\n        export_crs: pyproj.CRS = LAT_LON_CRS,\n        label_names: typing.Tuple = None,\n        ensure_non_overlapping: bool = False,\n        simplify_tol: float = 0.0,\n        drop_nan: bool = True,\n        vis: bool = True,\n        batched_unary_union_kwargs: typing.Dict = {\n            \"batch_size\": 500000,\n            \"sort_by_loc\": True,\n            \"grid_size\": 0.05,\n            \"simplify_tol\": 0.05,\n        },\n        vis_kwargs: typing.Dict = {},\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n        Args:\n            face_labels (np.ndarray):\n                This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n                is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n                nonzero element at (i, j) represents a class prediction for the ith faces and jth\n                class\n            export_file (PATH_TYPE, optional):\n                Where to export. The extension must be a filetype that geopandas can write.\n                Defaults to None, if unset, nothing will be written.\n            export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n            label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n            ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n            simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n            drop_nan (bool, optional): Don't export the nan class, often used for background\n            vis: should the result be visualzed\n            batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n            vis_kwargs: keyword argmument dict for visualization\n\n        Raises:\n            ValueError: If the wrong number of faces labels are provided\n\n        Returns:\n            gpd.GeoDataFrame: Merged data\n        \"\"\"\n        # Compute the working projected CRS\n        # This is important because having things in meters makes things easier\n        self.logger.info(\"Computing working CRS\")\n        lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_CRS)[0]\n        working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n        # Try to extract face labels if not set\n        if face_labels is None:\n            face_labels = self.get_texture(request_vertex_texture=False)\n\n        # Check that the correct number of labels are provided\n        if face_labels.shape[0] != self.faces.shape[0]:\n            raise ValueError()\n\n        # Get the geospatial faces dataframe\n        faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n        self.logger.info(\"Creating dataframe of multipolygons\")\n\n        # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n        face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n        if face_labels_is_2d:\n            # Non-null columns\n            unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n        else:\n            face_labels = np.squeeze(face_labels)\n            unique_IDs = np.unique(face_labels)\n\n        if drop_nan:\n            # Drop nan from the list of IDs\n            unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n        multipolygon_list = []\n        # For each unique ID, aggregate all the faces together\n        # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n        # due to the large number of polygons\n        # Instead, we replace the default shapely.unary_union with our batched implementation\n        for unique_ID in tqdm(unique_IDs, desc=\"Merging faces for each class\"):\n            if face_labels_is_2d:\n                # Nonzero elements of the column\n                matching_face_mask = face_labels[:, unique_ID] &gt; 0\n            else:\n                # Elements that match the ID in question\n                matching_face_mask = face_labels == unique_ID\n            matching_face_inds = np.nonzero(matching_face_mask)[0]\n            matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n            list_of_polygons = matching_face_polygons.geometry.values\n            multipolygon = batched_unary_union(\n                list_of_polygons, **batched_unary_union_kwargs\n            )\n            multipolygon_list.append(multipolygon)\n\n        working_gdf = gpd.GeoDataFrame(\n            {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n        )\n\n        if label_names is not None:\n            names = [\n                (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n                for ID in working_gdf[CLASS_ID_KEY]\n            ]\n            working_gdf[CLASS_NAMES_KEY] = names\n\n        # Simplify the output geometry\n        if simplify_tol &gt; 0.0:\n            self.logger.info(\"Running simplification\")\n            working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n        # Make sure that the polygons are non-overlapping\n        if ensure_non_overlapping:\n            # TODO create a version that tie-breaks based on the number of predicted faces for each\n            # class and optionally the ratios of 3D to top-down areas for the input triangles.\n            self.logger.info(\"Ensuring non-overlapping polygons\")\n            working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n        # Transform from the working crs to export crs\n        export_gdf = working_gdf.to_crs(export_crs)\n\n        # Export if a file is provided\n        if export_file is not None:\n            ensure_containing_folder(export_file)\n            export_gdf.to_file(export_file)\n\n        # Vis if requested\n        if vis:\n            self.logger.info(\"Plotting\")\n            export_gdf.plot(\n                column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n                aspect=1,\n                legend=True,\n                **vis_kwargs,\n            )\n            plt.show()\n\n        return export_gdf\n\n    # Operations on raster files\n\n    def get_vert_values_from_raster_file(\n        self,\n        raster_file: PATH_TYPE,\n        return_verts_in_CRS: bool = False,\n        nodata_fill_value: float = np.nan,\n    ):\n        \"\"\"Compute the height above groun for each point on the mesh\n\n        Args:\n            raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n            return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n            nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n        Returns:\n            np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n            np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n        \"\"\"\n        # Open the DTM file\n        raster = rio.open(raster_file)\n        # Get the mesh points in the coordinate reference system of the DTM\n        verts_in_raster_CRS = self.get_vertices_in_CRS(\n            raster.crs, force_easting_northing=True\n        )\n\n        # Get the points as a list\n        easting_points = verts_in_raster_CRS[:, 0].tolist()\n        northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n        # Zip them together\n        zipped_locations = zip(easting_points, northing_points)\n        sampling_iter = tqdm(\n            zipped_locations,\n            desc=f\"Sampling values from raster {raster_file}\",\n            total=verts_in_raster_CRS.shape[0],\n        )\n        # Sample the raster file and squeeze if single channel\n        sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n        # Set nodata locations to nan\n        # TODO figure out if it will ever be a problem to take the first value\n        sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n            nodata_fill_value\n        )\n\n        if return_verts_in_CRS:\n            return sampled_raster_values, verts_in_raster_CRS\n\n        return sampled_raster_values\n\n    def get_height_above_ground(\n        self, DTM_file: PATH_TYPE, threshold: float = None\n    ) -&gt; np.ndarray:\n        \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the digital terrain model raster\n            threshold (float, optional):\n                If not None, return a boolean mask for points under this height. Defaults to None.\n\n        Returns:\n            np.ndarray: Either the height above ground or a boolean mask for ground points\n        \"\"\"\n        # Get the height from the DTM and the points in the same CRS\n        DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n            DTM_file, return_verts_in_CRS=True\n        )\n        # Extract the vertex height as the third channel\n        verts_height = verts_in_raster_CRS[:, 2]\n        # Subtract the two to get the height above ground\n        height_above_ground = verts_height - DTM_heights\n\n        # If the threshold is not None, return a boolean mask that is true for ground points\n        if threshold is not None:\n            # Return boolean mask\n            # TODO see if this will break for nan values\n            return height_above_ground &lt; threshold\n        # Return height above ground\n        return height_above_ground\n\n    def label_ground_class(\n        self,\n        DTM_file: PATH_TYPE,\n        height_above_ground_threshold: float,\n        labels: typing.Union[None, np.ndarray] = None,\n        only_label_existing_labels: bool = True,\n        ground_class_name: str = \"ground\",\n        ground_ID: typing.Union[None, int] = None,\n        set_mesh_texture: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Set vertices to a potentially-new class with a thresholded height above the DTM.\n        TODO, consider handling face textures as well\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the DTM file\n            height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n            labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n            only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n            ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n            ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n        Returns:\n            np.ndarray: The updated labels\n        \"\"\"\n\n        if labels is None:\n            # Default to using vertex labels since it's the native way to check height above the DTM\n            use_vertex_labels = True\n        elif labels is not None:\n            # Check the size of the input labels and set what type they are. Note this could override existing value\n            if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n                use_vertex_labels = True\n            elif labels.shape[0] == self.faces.shape[0]:\n                use_vertex_labels = False\n            else:\n                raise ValueError(\n                    \"Labels were provided but didn't match the shape of vertices or faces\"\n                )\n\n        # if a labels are not provided, get it from the mesh\n        if labels is None:\n            # Get the vertex textures from the mesh\n            labels = self.get_texture(\n                request_vertex_texture=use_vertex_labels,\n            )\n\n        # Compute which vertices are part of the ground by thresholding the height above the DTM\n        ground_mask = self.get_height_above_ground(\n            DTM_file=DTM_file, threshold=height_above_ground_threshold\n        )\n        # If we needed a mask for the faces, compute that instead\n        if not use_vertex_labels:\n            ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n                bool\n            )\n\n        # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n        if only_label_existing_labels:\n            # Find which vertices are labeled\n            is_labeled = np.isfinite(labels[:, 0])\n            # Find which points are ground that were previously labeled as something else\n            ground_mask = np.logical_and(is_labeled, ground_mask)\n\n        # Get the existing label names\n        IDs_to_labels = self.get_IDs_to_labels()\n\n        if IDs_to_labels is None and ground_ID is None:\n            # This means that the label is continous, so the concept of ID is meaningless\n            ground_ID = np.nan\n        elif IDs_to_labels is not None and ground_class_name in IDs_to_labels.values():\n            # If the ground class name is already in the list, set newly-predicted vertices to that class\n            # Get the dictionary mapping in the reverse direction\n            labels_to_IDs = {v: k for k, v in IDs_to_labels.items()}\n            # Determine the ID corresponding to the ground class name\n            ground_ID = labels_to_IDs.get(ground_class_name)\n        elif IDs_to_labels is not None:\n            # If the label names are present, and the class is not already included, add it as the last element\n            if ground_ID is None:\n                # Set it to the first unused ID\n                # TODO improve this since it should be the max plus one\n                ground_ID = len(IDs_to_labels)\n\n        self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n        # Replace mask for ground_vertices\n        labels[ground_mask, 0] = ground_ID\n\n        # Optionally apply the texture to the mesh\n        if set_mesh_texture:\n            self.set_texture(labels, use_derived_IDs_to_labels=False)\n\n        return labels\n\n    def get_transform_hash(self):\n        \"\"\"Generates a hash value for the transform to geospatial coordinates\n        Returns:\n            int: A hash value representing transformation.\n        \"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(\n            self.local_to_epgs_4978_transform.tobytes()\n            if self.local_to_epgs_4978_transform is not None\n            else 0\n        )\n        return hasher.hexdigest()\n\n    def get_mesh_hash(self):\n        \"\"\"Generates a hash value for the mesh based on its points and faces\n        Returns:\n            int: A hash value representing the current mesh.\n        \"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(self.pyvista_mesh.points.tobytes())\n        hasher.update(self.pyvista_mesh.faces.tobytes())\n        return hasher.hexdigest()\n\n    def pix2face(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        render_img_scale: float = 1,\n        save_to_cache: bool = False,\n        cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                A single camera or set of cameras. For each camera, the correspondences between\n                pixels and the face IDs of the mesh will be computed. The images of all cameras\n                are assumed to be the same size.\n            render_img_scale (float, optional):\n                Create a pix2face map that is this fraction of the original image scale. Defaults\n                to 1.\n            save_to_cache (bool, optional):\n                Should newly-computed values be saved to the cache. This may speed up future operations\n                but can take up 100s of GBs of space. Defaults to False.\n            cache_folder ((PATH_TYPE, None), optional):\n                Where to check for and save to cached data. Only applicable if use_cache=True.\n                Defaults to CACHE_FOLDER\n\n        Returns:\n            np.ndarray: For each camera, there is an array that is the shape of an image and\n            contains the integer face index for the ray originating at that pixel. Any pixel for\n            which the given ray does not intersect a face is given a value of -1. If the input is\n            a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n            (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n        \"\"\"\n        # If a set of cameras is passed in, call this method on each camera and concatenate\n        # Other derived methods might be able to compute a batch of renders and once, but pyvista\n        # cannot as far as I can tell\n        if isinstance(cameras, PhotogrammetryCameraSet):\n            pix2face_list = [\n                self.pix2face(camera, render_img_scale=render_img_scale)\n                for camera in cameras\n            ]\n            pix2face = np.stack(pix2face_list, axis=0)\n            return pix2face\n\n        ## Single camera case\n\n        # Check if the cache contains a valid pix2face for the camera based on the dependencies\n        # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n        # The cache will generate a unique key for each combination of the dependencies\n        # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n        # If no match is found, recompute pix2face\n        # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n        mesh_hash = self.get_mesh_hash()\n        camera_hash = cameras.get_camera_hash()\n        cacher = ub.Cacher(\n            \"pix2face\",\n            depends=[mesh_hash, camera_hash, render_img_scale],\n            dpath=cache_folder,\n            verbose=0,\n        )\n        pix2face = cacher.tryload(on_error=\"clear\")\n        ## Cache is valid\n        if pix2face is not None:\n            return pix2face\n\n        # This needs to be an attribute of the class because creating a large number of plotters\n        # results in an un-fixable memory leak.\n        # See https://github.com/pyvista/pyvista/issues/2252\n        # The first step is to clear it\n        self.pix2face_plotter.clear()\n        # This is important so there aren't intermediate values\n        self.pix2face_plotter.disable_anti_aliasing()\n        # Set the camera to the corresponding viewpoint\n        self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n        ## Compute the base 256 encoding of the face ID\n        n_faces = self.faces.shape[0]\n        ID_values = np.arange(n_faces)\n\n        # determine how many channels will be required to represent the number of faces\n        n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n        channel_multipliers = [256**i for i in range(n_channels)]\n\n        # Compute the encoding of each value, least significant value first\n        base_256_encoding = [\n            np.mod(np.floor(ID_values / m).astype(int), 256)\n            for m in channel_multipliers\n        ]\n\n        # ensure that there's a multiple of three channels\n        n_padding = int(np.ceil(n_channels / 3.0) * 3 - n_channels)\n        base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n        # Assume that all images are the same size\n        image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n        # Initialize pix2face\n        pix2face = np.zeros(image_size, dtype=int)\n        # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n        for chunk_ind in range(int(len(base_256_encoding) / 3)):\n            chunk_scalars = np.stack(\n                base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n            ).astype(np.uint8)\n            # Add the mesh with the associated scalars\n            self.pix2face_plotter.add_mesh(\n                self.pyvista_mesh,\n                scalars=chunk_scalars.copy(),\n                rgb=True,\n                diffuse=0.0,\n                ambient=1.0,\n            )\n\n            # Perform rendering, this is the slow step\n            rendered_img = self.pix2face_plotter.screenshot(\n                window_size=(image_size[1], image_size[0]),\n            )\n            # Take the rendered values and interpret them as the encoded value\n            # Make sure to not try to interpret channels that are not used in the encoding\n            channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n            for i in range(channels_to_decode):\n                channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n                channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n                pix2face += channel_value\n\n        # Mask out pixels for which the mesh was not visible\n        # This is because the background will render as white\n        # If there happen to be an exact power of (256^3) number of faces, the last one may get\n        # erronously masked. This seems like a minimal concern but it could be addressed by adding\n        # another channel or something like that\n        pix2face[pix2face &gt; n_faces] = -1\n\n        if save_to_cache:\n            # Save the most recently computed pix2face correspondance in the cache\n            cacher.save(pix2face)\n\n        return pix2face\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        return_camera: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n            return_camera (bool, optional):\n                Should the camera be yielded as the second value\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        if isinstance(cameras, PhotogrammetryCamera):\n            # Construct a camera set of length one\n            cameras = PhotogrammetryCameraSet([cameras])\n        elif not isinstance(cameras, PhotogrammetryCameraSet):\n            raise TypeError()\n\n        # Get the face texture from the mesh\n        # TODO consider whether the user should be able to pass a texture to this method. It could\n        # make the user's life easier but makes this method more complex\n        face_texture = self.get_texture(\n            request_vertex_texture=False, try_verts_faces_conversion=True\n        )\n        texture_dim = face_texture.shape[1]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_end = batch_start + batch_size\n            batch_cameras = cameras[batch_start:batch_end]\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n\n            # Iterate over the batch dimension\n            for i, pix2face in enumerate(batch_pix2face):\n                # Record the original shape of the image\n                img_shape = pix2face.shape[:2]\n                # Flatten for indexing\n                pix2face = pix2face.flatten()\n                # Compute which pixels intersected the mesh\n                mesh_pixel_inds = np.where(pix2face != -1)[0]\n                # Initialize and all-nan array\n                rendered_flattened = np.full(\n                    (pix2face.shape[0], texture_dim), fill_value=np.nan\n                )\n                # Fill the values for which correspondences exist\n                rendered_flattened[mesh_pixel_inds] = face_texture[\n                    pix2face[mesh_pixel_inds]\n                ]\n                # reshape to an image, where the last dimension is the texture dimension\n                rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n\n                if return_camera:\n                    yield (rendered_img, batch_cameras[i])\n                else:\n                    yield rendered_img\n\n    def project_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        check_null_image: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to project images from. cam.get_image() will be called on each one\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            check_null_image (bool, optional):\n                Only do indexing if there are non-null image values. This adds additional overhead,\n                but can save the expensive operation of indexing in cases where it would be a no-op.\n\n        Yields:\n            np.ndarray: The per-face projection of an image in the camera set\n        \"\"\"\n        n_faces = self.faces.shape[0]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_inds = list(range(batch_start, batch_start + batch_size))\n            batch_cameras = cameras.get_subset_cameras(batch_inds)\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                render_img_scale=aggregate_img_scale,\n                **pix2face_kwargs,\n            )\n            for i, pix2face in enumerate(batch_pix2face):\n                img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n                n_channels = 1 if img.ndim == 2 else img.shape[-1]\n                textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n                # Only do the expensive indexing step if there are finite values in the image. This is most\n                # significant for sparse detection tasks where some images may have no real data\n                if not check_null_image or np.any(np.isfinite(img)):\n                    flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                    flat_pix2face = pix2face.flatten()\n                    # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                    # my guess is the later pixel in the flattened array will override the former\n                    # TODO make sure that null pix2face values are handled properly\n                    textured_faces[flat_pix2face] = flat_img\n                yield textured_faces\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        return_all: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            return_all (bool, optional):\n                Return the projection of each individual image, rather than just the aggregates.\n                Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n        project_images_generator = self.project_images(\n            cameras=cameras,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            **kwargs,\n        )\n\n        if return_all:\n            all_projections = []\n\n        # TODO this should be a convenience method\n        n_faces = self.faces.shape[0]\n\n        projection_counts = np.zeros(n_faces)\n        summed_projection = None\n\n        for projection_for_image in tqdm(\n            project_images_generator,\n            total=len(cameras),\n            desc=\"Aggregating projected viewpoints\",\n        ):\n            if return_all:\n                all_projections.append(projection_for_image)\n\n            if summed_projection is None:\n                summed_projection = projection_for_image.astype(float)\n            else:\n                summed_projection = np.nansum(\n                    [summed_projection, projection_for_image], axis=0\n                )\n\n            projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n                int\n            )\n            projection_counts += projected_faces\n\n        no_projections = projection_counts == 0\n        summed_projection[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projection,\n        }\n\n        if return_all:\n            additional_information[\"all_projections\"] = all_projections\n\n        average_projections = np.divide(\n            summed_projection, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    # Visualization and saving methods\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        interactive: bool = True,\n        camera_set: PhotogrammetryCameraSet = None,\n        screenshot_filename: PATH_TYPE = None,\n        vis_scalars: typing.Union[None, np.ndarray] = None,\n        mesh_kwargs: typing.Dict = None,\n        interactive_jupyter: bool = False,\n        plotter_kwargs: typing.Dict = {},\n        enable_ssao: bool = True,\n        force_xvfb: bool = False,\n        frustum_scale: float = 2,\n        IDs_to_labels: typing.Union[None, dict] = None,\n    ):\n        \"\"\"Show the mesh and cameras\n\n        Args:\n            plotter (pyvista.Plotter, optional):\n                Plotter to use, else one will be created\n            off_screen (bool, optional):\n                Show offscreen\n            camera_set (PhotogrammetryCameraSet, optional):\n                Cameras to visualize. Defaults to None.\n            screenshot_filename (PATH_TYPE, optional):\n                Filepath to save to, will show interactively if None. Defaults to None.\n            vis_scalars (None, np.ndarray):\n                Scalars to show\n            mesh_kwargs:\n                dict of keyword arguments for the mesh\n            interactive_jupyter (bool):\n                Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n            plotter_kwargs:\n                dict of keyword arguments for the plotter\n            frustum_scale (float, optional):\n                Size of cameras in world units. Defaults to None.\n            IDs_to_labels ([None, dict], optional):\n                Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n                IDs_to_labels if unset.\n        \"\"\"\n        off_screen = (not interactive) or (screenshot_filename is not None)\n\n        # If the IDs to labels is not set, use the default ones for this mesh\n        if IDs_to_labels is None:\n            IDs_to_labels = self.get_IDs_to_labels()\n\n        # Set the mesh kwargs if not set\n        if mesh_kwargs is None:\n            # This needs to be a dict, even if it's empty\n            mesh_kwargs = {}\n\n            # If there are discrete labels, set the colormap and limits inteligently\n            if IDs_to_labels is not None:\n                # Compute the largest ID\n                max_ID = max(IDs_to_labels.keys())\n                if max_ID &lt; 20:\n                    colors = [\n                        matplotlib.colors.to_hex(c)\n                        for c in plt.get_cmap(\n                            (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                        ).colors\n                    ]\n                    mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                    mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n        # Create the plotter if it's None\n        plotter = create_pv_plotter(\n            off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n        )\n\n        # If the vis scalars are None, use the saved texture\n        if vis_scalars is None:\n            vis_scalars = self.get_texture(\n                # Request vertex texture if both are available\n                request_vertex_texture=(\n                    True\n                    if (\n                        self.vertex_texture is not None\n                        and self.face_texture is not None\n                    )\n                    else None\n                )\n            )\n\n        is_rgb = (\n            self.pyvista_mesh.active_scalars_name == \"RGB\"\n            if vis_scalars is None\n            else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n        )\n\n        # Data in the range [0, 255] must be uint8 type\n        if is_rgb and np.nanmax(vis_scalars) &gt; 1.0:\n            vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n        scalar_bar_args = {\"vertical\": True}\n        if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n            mesh_kwargs[\"annotations\"] = IDs_to_labels\n            scalar_bar_args[\"n_labels\"] = 0\n\n        # Add the mesh\n        plotter.add_mesh(\n            self.pyvista_mesh,\n            scalars=vis_scalars,\n            rgb=is_rgb,\n            scalar_bar_args=scalar_bar_args,\n            **mesh_kwargs,\n        )\n        # If the camera set is provided, show this too\n        if camera_set is not None:\n            # Adjust the frustum scale if the mesh came from metashape\n            # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n            if (\n                self.local_to_epgs_4978_transform is not None\n                and frustum_scale is not None\n            ):\n                transform_determinant = np.linalg.det(\n                    self.local_to_epgs_4978_transform[:3, :3]\n                )\n                scale_factor = np.cbrt(transform_determinant)\n                frustum_scale = frustum_scale / scale_factor\n            camera_set.vis(\n                plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n            )\n\n        # Enable screen space shading\n        if enable_ssao:\n            plotter.enable_ssao()\n\n        # Create parent folder if none exists\n        if screenshot_filename is not None:\n            ensure_containing_folder(screenshot_filename)\n\n        if \"jupyter_backend\" not in plotter_kwargs:\n            if interactive_jupyter:\n                plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n            else:\n                plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n        if \"title\" not in plotter_kwargs:\n            plotter_kwargs[\"title\"] = \"Geograypher mesh viewer\"\n\n        # Show\n        return plotter.show(\n            screenshot=screenshot_filename,\n            **plotter_kwargs,\n        )\n\n    def save_renders(\n        self,\n        camera_set: PhotogrammetryCameraSet,\n        render_image_scale=1.0,\n        output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n        make_composites: bool = False,\n        save_native_resolution: bool = False,\n        cast_to_uint8: bool = True,\n        uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n        **render_kwargs,\n    ):\n        \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n        Args:\n            camera_set (PhotogrammetryCameraSet):\n                Camera set to use for rendering\n            render_image_scale (float, optional):\n                Multiplier on the real image scale to obtain size for rendering. Lower values\n                yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n            render_folder (PATH_TYPE, optional):\n                Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n            make_composites (bool, optional):\n                Should a triple pane composite with the original image be saved rather than the\n                raw label\n            cast_to_uint8: (bool, optional):\n                cast the float valued data to unit8 for saving efficiency. May dramatically increase\n                efficiency due to png compression\n            uint8_value_for_null_texture (np.uint8, optional):\n                What value to assign for values that can't be represented as unsigned 8-bit data.\n                Defaults to NULL_TEXTURE_INT_VALUE\n            render_kwargs:\n                keyword arguments passed to the render.\n        \"\"\"\n\n        ensure_folder(output_folder)\n        self.logger.info(f\"Saving renders to {output_folder}\")\n\n        # Save the classes filename\n        self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n        # Create the generator object to render the images\n        # Since this is a generator, this will be fast\n        render_gen = self.render_flat(\n            camera_set,\n            render_img_scale=render_image_scale,\n            return_camera=True,\n            **render_kwargs,\n        )\n\n        # The computation only happens when items are requested from the generator\n        for rendered, camera in tqdm(\n            render_gen,\n            total=len(camera_set),\n            desc=\"Computing and saving renders\",\n        ):\n            ## All this is post-processing to visualize the rendered label.\n            # rendered could either be a one channel image of integer IDs,\n            # a one-channel image of scalars, or a three-channel image of\n            # RGB. It could also be multi-channel image corresponding to anything,\n            # but we don't expect that yet\n            if save_native_resolution and render_image_scale != 1:\n                native_size = camera.get_image_size()\n                # Upsample using nearest neighbor interpolation for discrete labels and\n                # bilinear for non-discrete\n                # TODO this will need to be fixed for multi-channel images since I don't think resize works\n                rendered = resize(\n                    rendered,\n                    native_size,\n                    order=(0 if self.is_discrete_texture() else 1),\n                )\n\n            if cast_to_uint8:\n                # Deterimine values that cannot be represented as uint8\n                mask = np.logical_or.reduce(\n                    [\n                        rendered &lt; 0,\n                        rendered &gt; 255,\n                        np.logical_not(np.isfinite(rendered)),\n                    ]\n                )\n                rendered[mask] = uint8_value_for_null_texture\n                # Cast and squeeze since you can't save a one-channel image\n                rendered = np.squeeze(rendered.astype(np.uint8))\n\n            if make_composites:\n                RGB_image = camera.get_image(\n                    image_scale=(1.0 if save_native_resolution else render_image_scale)\n                )\n                rendered = create_composite(\n                    RGB_image=RGB_image,\n                    label_image=rendered,\n                    IDs_to_labels=self.get_IDs_to_labels(),\n                )\n            else:\n                # Clip channels if needed\n                if rendered.ndim == 3:\n                    rendered = rendered[..., :3]\n\n            # Saving\n            camera_filename = camera.get_image_filename().relative_to(\n                camera_set.image_folder\n            )\n            output_filename = Path(output_folder, camera_filename)\n            # This may create nested folders in the output dir\n            ensure_containing_folder(output_filename)\n            if rendered.dtype == np.uint8:\n                output_filename = str(output_filename.with_suffix(\".png\"))\n\n                # Save the image\n                skimage.io.imsave(output_filename, rendered, check_contrast=False)\n            else:\n                output_filename = str(output_filename.with_suffix(\".npy\"))\n                # Save the image\n                np.save(output_filename, rendered)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh-functions","title":"Functions","text":""},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.__init__","title":"<code>__init__(mesh, downsample_target=1.0, transform_filename=None, texture=None, texture_column_name=None, IDs_to_labels=None, ROI=None, ROI_buffer_meters=0, require_transform=False, log_level='INFO')</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh, in a format pyvista can read, or pyvista mesh</p> required <code>downsample_target</code> <code>float</code> <p>Downsample to this fraction of vertices. Defaults to 1.0.</p> <code>1.0</code> <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>Texture or path to one. See more details in <code>load_texture</code> documentation</p> <code>None</code> <code>texture_column_name</code> <code>Union[PATH_TYPE, None]</code> <p>The name of the column to use for a vectorfile input</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[PATH_TYPE, dict, None]</code> <p>dictionary or JSON file containing the mapping from integer IDs to string class names</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def __init__(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    downsample_target: float = 1.0,\n    transform_filename: PATH_TYPE = None,\n    texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n    texture_column_name: typing.Union[PATH_TYPE, None] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    ROI=None,\n    ROI_buffer_meters: float = 0,\n    require_transform: bool = False,\n    log_level: str = \"INFO\",\n):\n    \"\"\"_summary_\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]): Path to the mesh, in a format pyvista can read, or pyvista mesh\n        downsample_target (float, optional): Downsample to this fraction of vertices. Defaults to 1.0.\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]): Texture or path to one. See more details in `load_texture` documentation\n        texture_column_name: The name of the column to use for a vectorfile input\n        IDs_to_labels (typing.Union[PATH_TYPE, dict, None]): dictionary or JSON file containing the mapping from integer IDs to string class names\n    \"\"\"\n    self.downsample_target = downsample_target\n\n    self.pyvista_mesh = None\n    self.texture = None\n    self.vertex_texture = None\n    self.face_texture = None\n    self.local_to_epgs_4978_transform = None\n    self.IDs_to_labels = None\n    # Create the plotter that will later be used to compute correspondences between pixels\n    # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n    # plotters. See https://github.com/pyvista/pyvista/issues/2252\n    self.pix2face_plotter = create_pv_plotter(off_screen=True)\n    self.face_polygons_cache = {}\n    self.face_2d_3d_ratios_cache = {}\n\n    self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n    self.logger.setLevel(log_level)\n    # Potentially necessary for Jupyter\n    # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n    # If you don't check that there's already a handler, you can have situations with duplicated\n    # print outs if you have multiple mesh objects\n    if not self.logger.hasHandlers():\n        self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n    # Load the transform\n    self.logger.info(\"Loading transform to EPSG:4326\")\n    self.load_transform_to_epsg_4326(\n        transform_filename, require_transform=require_transform\n    )\n    # Load the mesh with the pyvista loader\n    self.logger.info(\"Loading mesh\")\n    self.load_mesh(\n        mesh=mesh,\n        downsample_target=downsample_target,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_meters,\n    )\n    # Load the texture\n    self.logger.info(\"Loading texture\")\n    # load IDs_to_labels\n    # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n    if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n        possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n        if possible_json.exists():\n            IDs_to_labels = possible_json\n    # convert IDs_to_labels from file to dict\n    if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n        with open(IDs_to_labels, \"r\") as file:\n            IDs_to_labels = json.load(file)\n            IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n    self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, return_all=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>return_all</code> <code>bool</code> <p>Return the projection of each individual image, rather than just the aggregates. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    return_all: bool = False,\n    **kwargs,\n):\n    \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        return_all (bool, optional):\n            Return the projection of each individual image, rather than just the aggregates.\n            Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n    project_images_generator = self.project_images(\n        cameras=cameras,\n        batch_size=batch_size,\n        aggregate_img_scale=aggregate_img_scale,\n        **kwargs,\n    )\n\n    if return_all:\n        all_projections = []\n\n    # TODO this should be a convenience method\n    n_faces = self.faces.shape[0]\n\n    projection_counts = np.zeros(n_faces)\n    summed_projection = None\n\n    for projection_for_image in tqdm(\n        project_images_generator,\n        total=len(cameras),\n        desc=\"Aggregating projected viewpoints\",\n    ):\n        if return_all:\n            all_projections.append(projection_for_image)\n\n        if summed_projection is None:\n            summed_projection = projection_for_image.astype(float)\n        else:\n            summed_projection = np.nansum(\n                [summed_projection, projection_for_image], axis=0\n            )\n\n        projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n            int\n        )\n        projection_counts += projected_faces\n\n    no_projections = projection_counts == 0\n    summed_projection[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projection,\n    }\n\n    if return_all:\n        additional_information[\"all_projections\"] = all_projections\n\n    average_projections = np.divide(\n        summed_projection, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.export_face_labels_vector","title":"<code>export_face_labels_vector(face_labels=None, export_file=None, export_crs=LAT_LON_CRS, label_names=None, ensure_non_overlapping=False, simplify_tol=0.0, drop_nan=True, vis=True, batched_unary_union_kwargs={'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}, vis_kwargs={})</code>","text":"<p>Export the labels for each face as a on-per-class multipolygon</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a nonzero element at (i, j) represents a class prediction for the ith faces and jth class</p> <code>None</code> <code>export_file</code> <code>PATH_TYPE</code> <p>Where to export. The extension must be a filetype that geopandas can write. Defaults to None, if unset, nothing will be written.</p> <code>None</code> <code>export_crs</code> <code>CRS</code> <p>What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.</p> <code>LAT_LON_CRS</code> <code>label_names</code> <code>Tuple</code> <p>Optional names, that are indexed by the labels. Defaults to None.</p> <code>None</code> <code>ensure_non_overlapping</code> <code>bool</code> <p>Should regions where two classes are predicted at different z heights be assigned to one class</p> <code>False</code> <code>simplify_tol</code> <code>float</code> <p>(float, optional): Tolerence in meters to use to simplify geometry</p> <code>0.0</code> <code>drop_nan</code> <code>bool</code> <p>Don't export the nan class, often used for background</p> <code>True</code> <code>vis</code> <code>bool</code> <p>should the result be visualzed</p> <code>True</code> <code>batched_unary_union_kwargs</code> <code>dict</code> <p>Keyword arguments for batched_unary_union_call</p> <code>{'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}</code> <code>vis_kwargs</code> <code>Dict</code> <p>keyword argmument dict for visualization</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the wrong number of faces labels are provided</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Merged data</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def export_face_labels_vector(\n    self,\n    face_labels: typing.Union[np.ndarray, None] = None,\n    export_file: PATH_TYPE = None,\n    export_crs: pyproj.CRS = LAT_LON_CRS,\n    label_names: typing.Tuple = None,\n    ensure_non_overlapping: bool = False,\n    simplify_tol: float = 0.0,\n    drop_nan: bool = True,\n    vis: bool = True,\n    batched_unary_union_kwargs: typing.Dict = {\n        \"batch_size\": 500000,\n        \"sort_by_loc\": True,\n        \"grid_size\": 0.05,\n        \"simplify_tol\": 0.05,\n    },\n    vis_kwargs: typing.Dict = {},\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n    Args:\n        face_labels (np.ndarray):\n            This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n            is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n            nonzero element at (i, j) represents a class prediction for the ith faces and jth\n            class\n        export_file (PATH_TYPE, optional):\n            Where to export. The extension must be a filetype that geopandas can write.\n            Defaults to None, if unset, nothing will be written.\n        export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n        label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n        ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n        simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n        drop_nan (bool, optional): Don't export the nan class, often used for background\n        vis: should the result be visualzed\n        batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n        vis_kwargs: keyword argmument dict for visualization\n\n    Raises:\n        ValueError: If the wrong number of faces labels are provided\n\n    Returns:\n        gpd.GeoDataFrame: Merged data\n    \"\"\"\n    # Compute the working projected CRS\n    # This is important because having things in meters makes things easier\n    self.logger.info(\"Computing working CRS\")\n    lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_CRS)[0]\n    working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n    # Try to extract face labels if not set\n    if face_labels is None:\n        face_labels = self.get_texture(request_vertex_texture=False)\n\n    # Check that the correct number of labels are provided\n    if face_labels.shape[0] != self.faces.shape[0]:\n        raise ValueError()\n\n    # Get the geospatial faces dataframe\n    faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n    self.logger.info(\"Creating dataframe of multipolygons\")\n\n    # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n    face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n    if face_labels_is_2d:\n        # Non-null columns\n        unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n    else:\n        face_labels = np.squeeze(face_labels)\n        unique_IDs = np.unique(face_labels)\n\n    if drop_nan:\n        # Drop nan from the list of IDs\n        unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n    multipolygon_list = []\n    # For each unique ID, aggregate all the faces together\n    # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n    # due to the large number of polygons\n    # Instead, we replace the default shapely.unary_union with our batched implementation\n    for unique_ID in tqdm(unique_IDs, desc=\"Merging faces for each class\"):\n        if face_labels_is_2d:\n            # Nonzero elements of the column\n            matching_face_mask = face_labels[:, unique_ID] &gt; 0\n        else:\n            # Elements that match the ID in question\n            matching_face_mask = face_labels == unique_ID\n        matching_face_inds = np.nonzero(matching_face_mask)[0]\n        matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n        list_of_polygons = matching_face_polygons.geometry.values\n        multipolygon = batched_unary_union(\n            list_of_polygons, **batched_unary_union_kwargs\n        )\n        multipolygon_list.append(multipolygon)\n\n    working_gdf = gpd.GeoDataFrame(\n        {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n    )\n\n    if label_names is not None:\n        names = [\n            (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n            for ID in working_gdf[CLASS_ID_KEY]\n        ]\n        working_gdf[CLASS_NAMES_KEY] = names\n\n    # Simplify the output geometry\n    if simplify_tol &gt; 0.0:\n        self.logger.info(\"Running simplification\")\n        working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n    # Make sure that the polygons are non-overlapping\n    if ensure_non_overlapping:\n        # TODO create a version that tie-breaks based on the number of predicted faces for each\n        # class and optionally the ratios of 3D to top-down areas for the input triangles.\n        self.logger.info(\"Ensuring non-overlapping polygons\")\n        working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n    # Transform from the working crs to export crs\n    export_gdf = working_gdf.to_crs(export_crs)\n\n    # Export if a file is provided\n    if export_file is not None:\n        ensure_containing_folder(export_file)\n        export_gdf.to_file(export_file)\n\n    # Vis if requested\n    if vis:\n        self.logger.info(\"Plotting\")\n        export_gdf.plot(\n            column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n            aspect=1,\n            legend=True,\n            **vis_kwargs,\n        )\n        plt.show()\n\n    return export_gdf\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.face_to_vert_texture","title":"<code>face_to_vert_texture(face_IDs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>face_IDs</code> <code>array</code> <p>(n_faces,) The integer IDs of the faces</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def face_to_vert_texture(self, face_IDs):\n    \"\"\"_summary_\n\n    Args:\n        face_IDs (np.array): (n_faces,) The integer IDs of the faces\n    \"\"\"\n    raise NotImplementedError()\n    # TODO figure how to have a NaN class that\n    for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n        # Find which faces are using this vertex\n        matching = np.sum(self.faces == i, axis=1)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_faces_2d_gdf","title":"<code>get_faces_2d_gdf(crs, include_3d_2d_ratio=False, data_dict={}, faces_mask=None, cache_data=False)</code>","text":"<p>Get a geodataframe of triangles for the 2D projection of each face of the mesh</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>Coordinate reference system of the dataframe</p> required <code>include_3d_2d_ratio</code> <code>bool</code> <p>Compute the ratio of the 3D area of the face to the 2D area. This relates to the slope of the face relative to horizontal. The computed data will be stored in the column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.</p> <code>False</code> <code>data_dict</code> <code>dict</code> <p>Additional information to add to the dataframe. It must be a dict where the keys are the names of the columns and the data is a np.ndarray of n_faces elemenets. Defaults to {}.</p> <code>{}</code> <code>faces_mask</code> <code>Union[ndarray, None]</code> <p>A binary mask corresponding to which faces to return. Used to improve runtime of creating the dataframe or downstream steps. Defaults to None.</p> <code>None</code> <code>cache_data</code> <code>bool</code> <p>Whether to cache expensive results in memory as object attributes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A dataframe for each triangular face</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_faces_2d_gdf(\n    self,\n    crs: pyproj.CRS,\n    include_3d_2d_ratio: bool = False,\n    data_dict: dict = {},\n    faces_mask: typing.Union[np.ndarray, None] = None,\n    cache_data: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n    Args:\n        crs (pyproj.CRS):\n            Coordinate reference system of the dataframe\n        include_3d_2d_ratio (bool, optional):\n            Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n            slope of the face relative to horizontal. The computed data will be stored in the\n            column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n        data_dict (dict, optional):\n            Additional information to add to the dataframe. It must be a dict where the keys\n            are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n            Defaults to {}.\n        faces_mask (typing.Union[np.ndarray, None], optional):\n            A binary mask corresponding to which faces to return. Used to improve runtime of\n            creating the dataframe or downstream steps. Defaults to None.\n        cache_data (bool):\n            Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n    Returns:\n        geopandas.GeoDataFrame: A dataframe for each triangular face\n    \"\"\"\n    # Computing this data can be slow, and we might call it multiple times. This is especially\n    # true for doing clustered polygon labeling\n    if cache_data:\n        mesh_hash = self.get_mesh_hash()\n        transform_hash = self.get_transform_hash()\n        faces_mask_hash = hash(\n            faces_mask.tobytes() if faces_mask is not None else 0\n        )\n        # Create a key that uniquely identifies the relavant inputs\n        cache_key = (mesh_hash, transform_hash, faces_mask_hash, crs)\n\n        # See if the face polygons were in the cache. If not, None will be returned\n        cached_values = self.face_polygons_cache.get(cache_key)\n    else:\n        cached_values = None\n\n    if cached_values is not None:\n        face_polygons, faces = cached_values\n        logging.info(\"Using cached face polygons\")\n    else:\n        self.logger.info(\"Computing faces in working CRS\")\n        # Get the mesh vertices in the desired export CRS\n        verts_in_crs = self.get_vertices_in_CRS(crs)\n        # Get a triangle in geospatial coords for each face\n        # (n_faces, 3 points, xyz)\n        faces = verts_in_crs[self.faces]\n\n        # Select only the requested faces\n        if faces_mask is not None:\n            faces = faces[faces_mask]\n\n        # Extract the first two columns and convert them to a list of tuples of tuples\n        faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n        face_polygons = [\n            Polygon(face_tuple)\n            for face_tuple in tqdm(\n                faces_2d_tuples, desc=f\"Converting faces to polygons\"\n            )\n        ]\n        self.logger.info(\"Creating dataframe of faces\")\n\n        if cache_data:\n            # Save computed data to the cache for the future\n            self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n    # Remove data corresponding to masked faces\n    if faces_mask is not None:\n        data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n    # Compute the ratio between the 3D area and the projected top-down 2D area\n    if include_3d_2d_ratio:\n        if cache_data:\n            # Check if ratios are cached\n            ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n        else:\n            ratios = None\n\n        # Ratios need to be computed\n        if ratios is None:\n            ratios = []\n            for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                area, area_2d = compute_3D_triangle_area(face)\n                ratios.append(area / area_2d)\n\n            if cache_data:\n                self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n        # Add the ratios to the data dict\n        data_dict[RATIO_3D_2D_KEY] = ratios\n\n    # Create the dataframe\n    faces_gdf = gpd.GeoDataFrame(\n        data=data_dict,\n        geometry=face_polygons,\n        crs=crs,\n    )\n\n    return faces_gdf\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_height_above_ground","title":"<code>get_height_above_ground(DTM_file, threshold=None)</code>","text":"<p>Return height above ground for a points in the mesh and a given DTM</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the digital terrain model raster</p> required <code>threshold</code> <code>float</code> <p>If not None, return a boolean mask for points under this height. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Either the height above ground or a boolean mask for ground points</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_height_above_ground(\n    self, DTM_file: PATH_TYPE, threshold: float = None\n) -&gt; np.ndarray:\n    \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the digital terrain model raster\n        threshold (float, optional):\n            If not None, return a boolean mask for points under this height. Defaults to None.\n\n    Returns:\n        np.ndarray: Either the height above ground or a boolean mask for ground points\n    \"\"\"\n    # Get the height from the DTM and the points in the same CRS\n    DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n        DTM_file, return_verts_in_CRS=True\n    )\n    # Extract the vertex height as the third channel\n    verts_height = verts_in_raster_CRS[:, 2]\n    # Subtract the two to get the height above ground\n    height_above_ground = verts_height - DTM_heights\n\n    # If the threshold is not None, return a boolean mask that is true for ground points\n    if threshold is not None:\n        # Return boolean mask\n        # TODO see if this will break for nan values\n        return height_above_ground &lt; threshold\n    # Return height above ground\n    return height_above_ground\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_mesh_hash","title":"<code>get_mesh_hash()</code>","text":"<p>Generates a hash value for the mesh based on its points and faces Returns:     int: A hash value representing the current mesh.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_mesh_hash(self):\n    \"\"\"Generates a hash value for the mesh based on its points and faces\n    Returns:\n        int: A hash value representing the current mesh.\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(self.pyvista_mesh.points.tobytes())\n    hasher.update(self.pyvista_mesh.faces.tobytes())\n    return hasher.hexdigest()\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_transform_hash","title":"<code>get_transform_hash()</code>","text":"<p>Generates a hash value for the transform to geospatial coordinates Returns:     int: A hash value representing transformation.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_transform_hash(self):\n    \"\"\"Generates a hash value for the transform to geospatial coordinates\n    Returns:\n        int: A hash value representing transformation.\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(\n        self.local_to_epgs_4978_transform.tobytes()\n        if self.local_to_epgs_4978_transform is not None\n        else 0\n    )\n    return hasher.hexdigest()\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_values_for_verts_from_vector","title":"<code>get_values_for_verts_from_vector(vector_source, column_names)</code>","text":"<p>Get the value from a dataframe for each vertex</p> <p>Parameters:</p> Name Type Description Default <code>vector_source</code> <code>Union[GeoDataFrame, PATH_TYPE]</code> <p>geo data frame or path to data that can be loaded by geopandas</p> required <code>column_names</code> <code>Union[str, List[str]]</code> <p>Which columns to obtain data from</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of values for each vertex if there is one column name or</p> <code>ndarray</code> <p>dict[np.ndarray]: A dict mapping from column names to numpy arrays</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_values_for_verts_from_vector(\n    self,\n    vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n    column_names: typing.Union[str, typing.List[str]],\n) -&gt; np.ndarray:\n    \"\"\"Get the value from a dataframe for each vertex\n\n    Args:\n        vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n        column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n    Returns:\n        np.ndarray: Array of values for each vertex if there is one column name or\n        dict[np.ndarray]: A dict mapping from column names to numpy arrays\n    \"\"\"\n    # Lead the vector data if not already provided in memory\n    if isinstance(vector_source, gpd.GeoDataFrame):\n        gdf = vector_source\n    else:\n        # This will error if not readable\n        gdf = gpd.read_file(vector_source)\n\n    # Infer or standardize the column names\n    if column_names is None:\n        # Check if there is only one real column\n        if len(gdf.columns) == 2:\n            column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n        else:\n            # Log as well since this may be caught by an exception handler,\n            # and it's a user error that can be corrected\n            self.logger.error(\n                \"No column name provided and ambigious which column to use\"\n            )\n            raise ValueError(\n                \"No column name provided and ambigious which column to use\"\n            )\n    # If only one column is provided, make it a one-length list\n    elif isinstance(column_names, str):\n        column_names = [column_names]\n\n    # Get a dataframe of vertices\n    verts_df = self.get_verts_geodataframe(gdf.crs)\n\n    # See which vertices are in the geopolygons\n    points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n    # Get the index array\n    index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n    # This is one entry per vertex\n    labeled_verts_dict = {}\n    all_values_dict = {}\n    # Extract the data from each\n    for column_name in column_names:\n        # Create an array corresponding to all the points and initialize to NaN\n        column_values = points_in_polygons_gdf[column_name]\n        # TODO clean this up\n        if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n            # TODO be set to the default value for the type of the column\n            null_value = \"null\"\n        elif column_values.dtype == int:\n            null_value = 255\n        else:\n            null_value = np.nan\n        # Create an array, one per vertex, with the null value\n        values = np.full(\n            shape=verts_df.shape[0],\n            dtype=column_values.dtype,\n            fill_value=null_value,\n        )\n        # Assign the labeled values\n        values[index_array] = column_values\n\n        # Record the results\n        labeled_verts_dict[column_name] = values\n        all_values_dict[column_name] = gdf[column_name]\n\n    # If only one name was requested, just return that\n    if len(column_names) == 1:\n        labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n        all_values = np.array(list(all_values_dict.values())[0])\n\n        return labeled_verts, all_values\n    # Else return a dict of all requested values\n    return labeled_verts_dict, all_values_dict\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vert_values_from_raster_file","title":"<code>get_vert_values_from_raster_file(raster_file, return_verts_in_CRS=False, nodata_fill_value=np.nan)</code>","text":"<p>Compute the height above groun for each point on the mesh</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>The path to the geospatial raster file.</p> required <code>return_verts_in_CRS</code> <code>bool</code> <p>Return the vertices transformed into the raster CRS</p> <code>False</code> <code>nodata_fill_value</code> <code>float</code> <p>Set data defined by the opened file as NODATAVAL to this value</p> <code>nan</code> <p>Returns:</p> Type Description <p>np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)</p> <p>np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vert_values_from_raster_file(\n    self,\n    raster_file: PATH_TYPE,\n    return_verts_in_CRS: bool = False,\n    nodata_fill_value: float = np.nan,\n):\n    \"\"\"Compute the height above groun for each point on the mesh\n\n    Args:\n        raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n        return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n        nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n    Returns:\n        np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n        np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n    \"\"\"\n    # Open the DTM file\n    raster = rio.open(raster_file)\n    # Get the mesh points in the coordinate reference system of the DTM\n    verts_in_raster_CRS = self.get_vertices_in_CRS(\n        raster.crs, force_easting_northing=True\n    )\n\n    # Get the points as a list\n    easting_points = verts_in_raster_CRS[:, 0].tolist()\n    northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n    # Zip them together\n    zipped_locations = zip(easting_points, northing_points)\n    sampling_iter = tqdm(\n        zipped_locations,\n        desc=f\"Sampling values from raster {raster_file}\",\n        total=verts_in_raster_CRS.shape[0],\n    )\n    # Sample the raster file and squeeze if single channel\n    sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n    # Set nodata locations to nan\n    # TODO figure out if it will ever be a problem to take the first value\n    sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n        nodata_fill_value\n    )\n\n    if return_verts_in_CRS:\n        return sampled_raster_values, verts_in_raster_CRS\n\n    return sampled_raster_values\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vertices_in_CRS","title":"<code>get_vertices_in_CRS(output_CRS, force_easting_northing=True)</code>","text":"<p>Return the coordinates of the mesh vertices in a given CRS</p> <p>Parameters:</p> Name Type Description Default <code>output_CRS</code> <code>CRS</code> <p>The coordinate reference system to transform to</p> required <code>force_easting_northing</code> <code>bool</code> <p>Ensure that the returned points are east first, then north</p> <code>True</code> <p>Returns:</p> Type Description <p>np.ndarray: (n_points, 3)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vertices_in_CRS(\n    self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n):\n    \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n    Args:\n        output_CRS (pyproj.CRS): The coordinate reference system to transform to\n        force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n    Returns:\n        np.ndarray: (n_points, 3)\n    \"\"\"\n    # If no CRS is requested, just return the points\n    if output_CRS is None:\n        return self.pyvista_mesh.points\n\n    # The mesh points are defined in an arbitrary local coordinate system but we can transform them to EPGS:4978,\n    # the earth-centered, earth-fixed coordinate system, using an included transform\n    epgs4978_verts = self.transform_vertices(self.local_to_epgs_4978_transform)\n\n    # TODO figure out why this conversion was required. I think it was some typing issue\n    output_CRS = pyproj.CRS.from_epsg(output_CRS.to_epsg())\n    # Build a pyproj transfrormer from EPGS:4978 to the desired CRS\n    transformer = pyproj.Transformer.from_crs(\n        EARTH_CENTERED_EARTH_FIXED_CRS, output_CRS\n    )\n\n    # Transform the coordinates\n    verts_in_output_CRS = transformer.transform(\n        xx=epgs4978_verts[:, 0],\n        yy=epgs4978_verts[:, 1],\n        zz=epgs4978_verts[:, 2],\n    )\n    # Stack and transpose\n    verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n    # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n    # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n    # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n    if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n        # Swap first two columns\n        verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n    return verts_in_output_CRS\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_verts_geodataframe","title":"<code>get_verts_geodataframe(crs)</code>","text":"<p>Obtain the vertices as a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>The CRS to use</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A dataframe with all the vertices</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n    \"\"\"Obtain the vertices as a dataframe\n\n    Args:\n        crs (pyproj.CRS): The CRS to use\n\n    Returns:\n        gpd.GeoDataFrame: A dataframe with all the vertices\n    \"\"\"\n    # Get the vertices in the same CRS as the geofile\n    verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n    df = pd.DataFrame(\n        {\n            \"east\": verts_in_geopolygon_crs[:, 0],\n            \"north\": verts_in_geopolygon_crs[:, 1],\n        }\n    )\n    # Create a column of Point objects to use as the geometry\n    df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n    points = gpd.GeoDataFrame(df, crs=crs)\n\n    # Add an index column because the normal index will not be preserved in future operations\n    points[VERT_ID] = df.index\n\n    return points\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_ground_class","title":"<code>label_ground_class(DTM_file, height_above_ground_threshold, labels=None, only_label_existing_labels=True, ground_class_name='ground', ground_ID=None, set_mesh_texture=False)</code>","text":"<p>Set vertices to a potentially-new class with a thresholded height above the DTM. TODO, consider handling face textures as well</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the DTM file</p> required <code>height_above_ground_threshold</code> <code>float</code> <p>Height (meters) above that DTM that points below are considered ground</p> required <code>labels</code> <code>Union[None, ndarray]</code> <p>Vertex texture, otherwise will be queried from mesh. Defaults to None.</p> <code>None</code> <code>only_label_existing_labels</code> <code>bool</code> <p>Only label points that already have non-null labels. Defaults to True.</p> <code>True</code> <code>ground_class_name</code> <code>str</code> <p>The potentially-new ground class name. Defaults to \"ground\".</p> <code>'ground'</code> <code>ground_ID</code> <code>Union[None, int]</code> <p>What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The updated labels</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_ground_class(\n    self,\n    DTM_file: PATH_TYPE,\n    height_above_ground_threshold: float,\n    labels: typing.Union[None, np.ndarray] = None,\n    only_label_existing_labels: bool = True,\n    ground_class_name: str = \"ground\",\n    ground_ID: typing.Union[None, int] = None,\n    set_mesh_texture: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Set vertices to a potentially-new class with a thresholded height above the DTM.\n    TODO, consider handling face textures as well\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the DTM file\n        height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n        labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n        only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n        ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n        ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n    Returns:\n        np.ndarray: The updated labels\n    \"\"\"\n\n    if labels is None:\n        # Default to using vertex labels since it's the native way to check height above the DTM\n        use_vertex_labels = True\n    elif labels is not None:\n        # Check the size of the input labels and set what type they are. Note this could override existing value\n        if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n            use_vertex_labels = True\n        elif labels.shape[0] == self.faces.shape[0]:\n            use_vertex_labels = False\n        else:\n            raise ValueError(\n                \"Labels were provided but didn't match the shape of vertices or faces\"\n            )\n\n    # if a labels are not provided, get it from the mesh\n    if labels is None:\n        # Get the vertex textures from the mesh\n        labels = self.get_texture(\n            request_vertex_texture=use_vertex_labels,\n        )\n\n    # Compute which vertices are part of the ground by thresholding the height above the DTM\n    ground_mask = self.get_height_above_ground(\n        DTM_file=DTM_file, threshold=height_above_ground_threshold\n    )\n    # If we needed a mask for the faces, compute that instead\n    if not use_vertex_labels:\n        ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n            bool\n        )\n\n    # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n    if only_label_existing_labels:\n        # Find which vertices are labeled\n        is_labeled = np.isfinite(labels[:, 0])\n        # Find which points are ground that were previously labeled as something else\n        ground_mask = np.logical_and(is_labeled, ground_mask)\n\n    # Get the existing label names\n    IDs_to_labels = self.get_IDs_to_labels()\n\n    if IDs_to_labels is None and ground_ID is None:\n        # This means that the label is continous, so the concept of ID is meaningless\n        ground_ID = np.nan\n    elif IDs_to_labels is not None and ground_class_name in IDs_to_labels.values():\n        # If the ground class name is already in the list, set newly-predicted vertices to that class\n        # Get the dictionary mapping in the reverse direction\n        labels_to_IDs = {v: k for k, v in IDs_to_labels.items()}\n        # Determine the ID corresponding to the ground class name\n        ground_ID = labels_to_IDs.get(ground_class_name)\n    elif IDs_to_labels is not None:\n        # If the label names are present, and the class is not already included, add it as the last element\n        if ground_ID is None:\n            # Set it to the first unused ID\n            # TODO improve this since it should be the max plus one\n            ground_ID = len(IDs_to_labels)\n\n    self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n    # Replace mask for ground_vertices\n    labels[ground_mask, 0] = ground_ID\n\n    # Optionally apply the texture to the mesh\n    if set_mesh_texture:\n        self.set_texture(labels, use_derived_IDs_to_labels=False)\n\n    return labels\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2.0)</code>","text":"<p>Assign a class label to polygons using labels per face</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2.0,\n):\n    \"\"\"Assign a class label to polygons using labels per face\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Premptive error checking before expensive operations\n    face_labels = np.squeeze(face_labels)\n    if face_labels.ndim != 1:\n        raise ValueError(\n            f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n        )\n    if face_weighting is not None:\n        face_weighting = np.squeeze(face_weighting)\n        if face_weighting.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n            )\n\n    # Ensure that the input is a geopandas dataframe\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract just the geometry\n    polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n    # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n    faces_mask = np.isfinite(face_labels)\n\n    # Get the faces of the mesh as a geopandas dataframe\n    # Include the predicted face labels as a column in the dataframe\n    faces_2d_gdf = self.get_faces_2d_gdf(\n        polygons_gdf.crs,\n        include_3d_2d_ratio=True,\n        data_dict={CLASS_ID_KEY: face_labels},\n        faces_mask=faces_mask,\n        cache_data=True,\n    )\n\n    # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n    if face_weighting is not None:\n        face_weighting = face_weighting[faces_mask]\n        faces_2d_gdf[\"face_weighting\"] = (\n            faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n        )\n    # If not, just use the ratio\n    else:\n        faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n    # Set the precision to avoid approximate coliniearity errors\n    faces_2d_gdf.geometry = shapely.set_precision(\n        faces_2d_gdf.geometry.values, 1e-6\n    )\n    polygons_gdf.geometry = shapely.set_precision(\n        polygons_gdf.geometry.values, 1e-6\n    )\n\n    # Set the ID field so it's available after the overlay operation\n    # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n    # one and the index would not start from 0\n    polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n    # Since overlay is expensive, we first discard faces that are not near the polygons\n\n    # Dissolve the polygons to form one ROI\n    merged_polygons = polygons_gdf.dissolve()\n    # Try to decrease the number of elements in the polygon by expanding\n    # and then simplifying the number of elements in the polygon\n    merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n    merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n    # Determine which face IDs intersect the ROI. This is slow\n    start = time()\n    self.logger.info(\"Starting to subset to ROI\")\n\n    # Check which faces are fully within the buffered regions around the query polygons\n    # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n    # to compute more information than something like gpd.within\n    contained_faces = gpd.sjoin(\n        faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n    )[\"index_right\"].notna()\n    faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n    self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n    start = time()\n    self.logger.info(\"Starting `overlay`\")\n    if sjoin_overlay:\n        overlay = gpd.sjoin(\n            faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n        )\n        self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n    else:\n        # Drop faces not included\n        overlay = polygons_gdf.overlay(\n            faces_2d_gdf, how=\"identity\", keep_geom_type=False\n        )\n        self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n    # Drop nan, for geometries that don't intersect the polygons\n    overlay.dropna(inplace=True)\n    # Compute the weighted area for each face, which may have been broken up by the overlay\n    overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n    # Extract only the neccessary columns\n    overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n    aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n    # Compute the highest weighted class prediction\n    # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n    max_rows = aggregated_data.loc[\n        aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n            \"weighted_area\"\n        ].idxmax()\n    ].reset_index()\n\n    # Make the class predictions a list of IDs with nans where no information is available\n    pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n    pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n    predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n    predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n        pred_subset_IDs\n    )\n    predicted_class_IDs = predicted_class_IDs.tolist()\n\n    # Post-process to string label names if requested and IDs_to_labels exists\n    if return_class_labels and (\n        (IDs_to_labels := self.get_IDs_to_labels()) is not None\n    ):\n        # convert the IDs into labels\n        # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n        predicted_class_IDs = [\n            (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n            for pi in predicted_class_IDs\n        ]\n    return predicted_class_IDs\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_mesh","title":"<code>load_mesh(mesh, downsample_target=1.0, ROI=None, ROI_buffer_meters=0, ROI_simplify_tol_meters=2)</code>","text":"<p>Load the pyvista mesh and create the texture</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh or actual mesh</p> required <code>downsample_target</code> <code>float</code> <p>What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).</p> <code>1.0</code> <code>ROI</code> <p>See select_mesh_ROI. Defaults to None</p> <code>None</code> <code>ROI_buffer_meters</code> <p>See select_mesh_ROI. Defaults to 0.</p> <code>0</code> <code>ROI_simplify_tol_meters</code> <p>See select_mesh_ROI. Defaults to 2.</p> <code>2</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_mesh(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    downsample_target: float = 1.0,\n    ROI=None,\n    ROI_buffer_meters=0,\n    ROI_simplify_tol_meters=2,\n):\n    \"\"\"Load the pyvista mesh and create the texture\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n            Path to the mesh or actual mesh\n        downsample_target (float, optional):\n            What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n        ROI:\n            See select_mesh_ROI. Defaults to None\n        ROI_buffer_meters:\n            See select_mesh_ROI. Defaults to 0.\n        ROI_simplify_tol_meters:\n            See select_mesh_ROI. Defaults to 2.\n    \"\"\"\n    if isinstance(mesh, pv.PolyData):\n        self.pyvista_mesh = mesh\n    else:\n        # Load the mesh using pyvista\n        # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n        self.logger.info(\"Reading the mesh\")\n        self.pyvista_mesh = pv.read(mesh)\n\n    self.logger.info(\"Selecting an ROI from mesh\")\n    # Select a region of interest if needed\n    self.pyvista_mesh = self.select_mesh_ROI(\n        region_of_interest=ROI,\n        buffer_meters=ROI_buffer_meters,\n        simplify_tol_meters=ROI_simplify_tol_meters,\n    )\n\n    # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n    if downsample_target != 1.0:\n        # TODO try decimate_pro and compare quality and runtime\n        # TODO see if there's a way to preserve the mesh colors\n        # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n        self.logger.info(\"Downsampling the mesh\")\n        # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n        downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n            target_reduction=(1 - downsample_target)\n        )\n        self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n    self.logger.info(\"Extracting faces from mesh\")\n    # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n    self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_texture","title":"<code>load_texture(texture, texture_column_name=None, IDs_to_labels=None)</code>","text":"<p>Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other    one will be left as None</p> <p>Parameters:</p> Name Type Description Default <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>This is either a numpy array or a file to one of the following * A numpy array file in \".npy\" format * A vector file readable by geopandas and a label(s) specifying which column to use.   This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between   regions with different labels. These regions may be assigned based on the order of the rows. * A raster file readable by rasterio. We may want to support using a subset of bands</p> required <code>texture_column_name</code> <code>Union[None, PATH_TYPE]</code> <p>The column to use as the label for a vector data input</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Dictionary mapping from integer IDs to string class names</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_texture(\n    self,\n    texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n    texture_column_name: typing.Union[None, PATH_TYPE] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n):\n    \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n       one will be left as None\n\n    Args:\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n            * A numpy array file in \".npy\" format\n            * A vector file readable by geopandas and a label(s) specifying which column to use.\n              This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n              regions with different labels. These regions may be assigned based on the order of the rows.\n            * A raster file readable by rasterio. We may want to support using a subset of bands\n        texture_column_name: The column to use as the label for a vector data input\n        IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n    \"\"\"\n    # The easy case, a texture is passed in directly\n    if isinstance(texture, np.ndarray):\n        self.set_texture(\n            texture_array=texture,\n            IDs_to_labels=IDs_to_labels,\n            use_derived_IDs_to_labels=True,\n        )\n    # If the texture is None, try to load it from the mesh\n    # Note that this requires us to have not decimated yet\n    elif texture is None:\n        # See if the mesh has a texture, else this will be None\n        texture_array = self.pyvista_mesh.active_scalars\n\n        if texture_array is not None:\n            # Check if this was a really one channel that had to be tiled to\n            # three for saving\n            if len(texture_array.shape) == 2:\n                min_val_per_row = np.min(texture_array, axis=1)\n                max_val_per_row = np.max(texture_array, axis=1)\n                if np.array_equal(min_val_per_row, max_val_per_row):\n                    # This is supposted to be one channel\n                    texture_array = texture_array[:, 0].astype(float)\n                    # Set any values that are the ignore int value to nan\n            texture_array = texture_array.astype(float)\n            texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n            self.set_texture(\n                texture_array,\n                IDs_to_labels=IDs_to_labels,\n                use_derived_IDs_to_labels=True,\n            )\n        else:\n            if IDs_to_labels is not None:\n                self.IDs_to_labels = IDs_to_labels\n            # Assume that no texture will be needed, consider printing a warning\n            self.logger.warn(\"No texture provided\")\n    else:\n        # Try handling all the other supported filetypes\n        texture_array = None\n        all_values = None\n\n        # Name of scalar in the mesh\n        try:\n            self.logger.warn(\n                \"Trying to read texture as a scalar from the pyvista mesh:\"\n            )\n            texture_array = self.pyvista_mesh[texture]\n            self.logger.warn(\"- success\")\n        except (KeyError, ValueError):\n            self.logger.warn(\"- failed\")\n\n        # Numpy file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as a numpy file:\")\n                texture_array = np.load(texture, allow_pickle=True)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Vector file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as vector file:\")\n                # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                texture_array, all_values = self.get_values_for_verts_from_vector(\n                    column_names=texture_column_name,\n                    vector_source=texture,\n                )\n                self.logger.warn(\"- success\")\n            except (IndexError, fiona.errors.DriverError):\n                self.logger.warn(\"- failed\")\n\n        # Raster file\n        if texture_array is None:\n            try:\n                # TODO\n                self.logger.warn(\"Trying to read as texture as raster file: \")\n                texture_array = self.get_vert_values_from_raster_file(texture)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Error out if not set, since we assume the intent was to have a texture at this point\n        if texture_array is None:\n            raise ValueError(f\"Could not load texture for {texture}\")\n\n        # This will error if something is wrong with the texture that was loaded\n        self.set_texture(\n            texture_array,\n            all_discrete_texture_values=all_values,\n            use_derived_IDs_to_labels=True,\n            IDs_to_labels=IDs_to_labels,\n        )\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_transform_to_epsg_4326","title":"<code>load_transform_to_epsg_4326(transform_filename, require_transform=False)</code>","text":"<p>Load the 4x4 transform projects points from their local coordnate system into EPSG:4326, the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying it directly or extracted from a Metashape camera output</p> <p>Args     transform_filename (PATH_TYPE):     require_transform (bool): Does a local-to-global transform file need to be available\" Raises:     FileNotFoundError: Cannot find texture file     ValueError: Transform file doesn't have 4x4 matrix</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_transform_to_epsg_4326(\n    self, transform_filename: PATH_TYPE, require_transform: bool = False\n):\n    \"\"\"\n    Load the 4x4 transform projects points from their local coordnate system into EPSG:4326,\n    the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying\n    it directly or extracted from a Metashape camera output\n\n    Args\n        transform_filename (PATH_TYPE):\n        require_transform (bool): Does a local-to-global transform file need to be available\"\n    Raises:\n        FileNotFoundError: Cannot find texture file\n        ValueError: Transform file doesn't have 4x4 matrix\n    \"\"\"\n    if transform_filename is None:\n        if require_transform:\n            raise ValueError(\"Transform is required but not provided\")\n        # If not required, do nothing. TODO consider adding a warning\n        return\n\n    elif Path(transform_filename).suffix == \".xml\":\n        self.local_to_epgs_4978_transform = parse_transform_metashape(\n            transform_filename\n        )\n    elif Path(transform_filename).suffix == \".csv\":\n        self.local_to_epgs_4978_transform = np.loadtxt(\n            transform_filename, delimiter=\",\"\n        )\n        if self.local_to_epgs_4978_transform.shape != (4, 4):\n            raise ValueError(\n                f\"Transform should be (4,4) but is {self.local_to_epgs_4978_transform.shape}\"\n            )\n    else:\n        if require_transform:\n            raise ValueError(\n                f\"Transform could not be loaded from {transform_filename}\"\n            )\n        # Not set\n        return\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.pix2face","title":"<code>pix2face(cameras, render_img_scale=1, save_to_cache=False, cache_folder=CACHE_FOLDER)</code>","text":"<p>Compute the face that a ray from each pixel would intersect for each camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>A single camera or set of cameras. For each camera, the correspondences between pixels and the face IDs of the mesh will be computed. The images of all cameras are assumed to be the same size.</p> required <code>render_img_scale</code> <code>float</code> <p>Create a pix2face map that is this fraction of the original image scale. Defaults to 1.</p> <code>1</code> <code>save_to_cache</code> <code>bool</code> <p>Should newly-computed values be saved to the cache. This may speed up future operations but can take up 100s of GBs of space. Defaults to False.</p> <code>False</code> <code>cache_folder</code> <code>PATH_TYPE, None)</code> <p>Where to check for and save to cached data. Only applicable if use_cache=True. Defaults to CACHE_FOLDER</p> <code>CACHE_FOLDER</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: For each camera, there is an array that is the shape of an image and</p> <code>ndarray</code> <p>contains the integer face index for the ray originating at that pixel. Any pixel for</p> <code>ndarray</code> <p>which the given ray does not intersect a face is given a value of -1. If the input is</p> <code>ndarray</code> <p>a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is</p> <code>ndarray</code> <p>(n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def pix2face(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    render_img_scale: float = 1,\n    save_to_cache: bool = False,\n    cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n) -&gt; np.ndarray:\n    \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            A single camera or set of cameras. For each camera, the correspondences between\n            pixels and the face IDs of the mesh will be computed. The images of all cameras\n            are assumed to be the same size.\n        render_img_scale (float, optional):\n            Create a pix2face map that is this fraction of the original image scale. Defaults\n            to 1.\n        save_to_cache (bool, optional):\n            Should newly-computed values be saved to the cache. This may speed up future operations\n            but can take up 100s of GBs of space. Defaults to False.\n        cache_folder ((PATH_TYPE, None), optional):\n            Where to check for and save to cached data. Only applicable if use_cache=True.\n            Defaults to CACHE_FOLDER\n\n    Returns:\n        np.ndarray: For each camera, there is an array that is the shape of an image and\n        contains the integer face index for the ray originating at that pixel. Any pixel for\n        which the given ray does not intersect a face is given a value of -1. If the input is\n        a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n        (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n    \"\"\"\n    # If a set of cameras is passed in, call this method on each camera and concatenate\n    # Other derived methods might be able to compute a batch of renders and once, but pyvista\n    # cannot as far as I can tell\n    if isinstance(cameras, PhotogrammetryCameraSet):\n        pix2face_list = [\n            self.pix2face(camera, render_img_scale=render_img_scale)\n            for camera in cameras\n        ]\n        pix2face = np.stack(pix2face_list, axis=0)\n        return pix2face\n\n    ## Single camera case\n\n    # Check if the cache contains a valid pix2face for the camera based on the dependencies\n    # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n    # The cache will generate a unique key for each combination of the dependencies\n    # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n    # If no match is found, recompute pix2face\n    # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n    mesh_hash = self.get_mesh_hash()\n    camera_hash = cameras.get_camera_hash()\n    cacher = ub.Cacher(\n        \"pix2face\",\n        depends=[mesh_hash, camera_hash, render_img_scale],\n        dpath=cache_folder,\n        verbose=0,\n    )\n    pix2face = cacher.tryload(on_error=\"clear\")\n    ## Cache is valid\n    if pix2face is not None:\n        return pix2face\n\n    # This needs to be an attribute of the class because creating a large number of plotters\n    # results in an un-fixable memory leak.\n    # See https://github.com/pyvista/pyvista/issues/2252\n    # The first step is to clear it\n    self.pix2face_plotter.clear()\n    # This is important so there aren't intermediate values\n    self.pix2face_plotter.disable_anti_aliasing()\n    # Set the camera to the corresponding viewpoint\n    self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n    ## Compute the base 256 encoding of the face ID\n    n_faces = self.faces.shape[0]\n    ID_values = np.arange(n_faces)\n\n    # determine how many channels will be required to represent the number of faces\n    n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n    channel_multipliers = [256**i for i in range(n_channels)]\n\n    # Compute the encoding of each value, least significant value first\n    base_256_encoding = [\n        np.mod(np.floor(ID_values / m).astype(int), 256)\n        for m in channel_multipliers\n    ]\n\n    # ensure that there's a multiple of three channels\n    n_padding = int(np.ceil(n_channels / 3.0) * 3 - n_channels)\n    base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n    # Assume that all images are the same size\n    image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n    # Initialize pix2face\n    pix2face = np.zeros(image_size, dtype=int)\n    # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n    for chunk_ind in range(int(len(base_256_encoding) / 3)):\n        chunk_scalars = np.stack(\n            base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n        ).astype(np.uint8)\n        # Add the mesh with the associated scalars\n        self.pix2face_plotter.add_mesh(\n            self.pyvista_mesh,\n            scalars=chunk_scalars.copy(),\n            rgb=True,\n            diffuse=0.0,\n            ambient=1.0,\n        )\n\n        # Perform rendering, this is the slow step\n        rendered_img = self.pix2face_plotter.screenshot(\n            window_size=(image_size[1], image_size[0]),\n        )\n        # Take the rendered values and interpret them as the encoded value\n        # Make sure to not try to interpret channels that are not used in the encoding\n        channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n        for i in range(channels_to_decode):\n            channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n            channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n            pix2face += channel_value\n\n    # Mask out pixels for which the mesh was not visible\n    # This is because the background will render as white\n    # If there happen to be an exact power of (256^3) number of faces, the last one may get\n    # erronously masked. This seems like a minimal concern but it could be addressed by adding\n    # another channel or something like that\n    pix2face[pix2face &gt; n_faces] = -1\n\n    if save_to_cache:\n        # Save the most recently computed pix2face correspondance in the cache\n        cacher.save(pix2face)\n\n    return pix2face\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.project_images","title":"<code>project_images(cameras, batch_size=1, aggregate_img_scale=1, check_null_image=False, **pix2face_kwargs)</code>","text":"<p>Find the per-face projection for each of a set of images and associated camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to project images from. cam.get_image() will be called on each one</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>check_null_image</code> <code>bool</code> <p>Only do indexing if there are non-null image values. This adds additional overhead, but can save the expensive operation of indexing in cases where it would be a no-op.</p> <code>False</code> <p>Yields:</p> Type Description <p>np.ndarray: The per-face projection of an image in the camera set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def project_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    check_null_image: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to project images from. cam.get_image() will be called on each one\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        check_null_image (bool, optional):\n            Only do indexing if there are non-null image values. This adds additional overhead,\n            but can save the expensive operation of indexing in cases where it would be a no-op.\n\n    Yields:\n        np.ndarray: The per-face projection of an image in the camera set\n    \"\"\"\n    n_faces = self.faces.shape[0]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_inds = list(range(batch_start, batch_start + batch_size))\n        batch_cameras = cameras.get_subset_cameras(batch_inds)\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            render_img_scale=aggregate_img_scale,\n            **pix2face_kwargs,\n        )\n        for i, pix2face in enumerate(batch_pix2face):\n            img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n            n_channels = 1 if img.ndim == 2 else img.shape[-1]\n            textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n            # Only do the expensive indexing step if there are finite values in the image. This is most\n            # significant for sparse detection tasks where some images may have no real data\n            if not check_null_image or np.any(np.isfinite(img)):\n                flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                flat_pix2face = pix2face.flatten()\n                # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                # my guess is the later pixel in the flattened array will override the former\n                # TODO make sure that null pix2face values are handled properly\n                textured_faces[flat_pix2face] = flat_img\n            yield textured_faces\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, return_camera=False, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <code>return_camera</code> <code>bool</code> <p>Should the camera be yielded as the second value</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    return_camera: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n        return_camera (bool, optional):\n            Should the camera be yielded as the second value\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    if isinstance(cameras, PhotogrammetryCamera):\n        # Construct a camera set of length one\n        cameras = PhotogrammetryCameraSet([cameras])\n    elif not isinstance(cameras, PhotogrammetryCameraSet):\n        raise TypeError()\n\n    # Get the face texture from the mesh\n    # TODO consider whether the user should be able to pass a texture to this method. It could\n    # make the user's life easier but makes this method more complex\n    face_texture = self.get_texture(\n        request_vertex_texture=False, try_verts_faces_conversion=True\n    )\n    texture_dim = face_texture.shape[1]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_end = batch_start + batch_size\n        batch_cameras = cameras[batch_start:batch_end]\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n\n        # Iterate over the batch dimension\n        for i, pix2face in enumerate(batch_pix2face):\n            # Record the original shape of the image\n            img_shape = pix2face.shape[:2]\n            # Flatten for indexing\n            pix2face = pix2face.flatten()\n            # Compute which pixels intersected the mesh\n            mesh_pixel_inds = np.where(pix2face != -1)[0]\n            # Initialize and all-nan array\n            rendered_flattened = np.full(\n                (pix2face.shape[0], texture_dim), fill_value=np.nan\n            )\n            # Fill the values for which correspondences exist\n            rendered_flattened[mesh_pixel_inds] = face_texture[\n                pix2face[mesh_pixel_inds]\n            ]\n            # reshape to an image, where the last dimension is the texture dimension\n            rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n\n            if return_camera:\n                yield (rendered_img, batch_cameras[i])\n            else:\n                yield rendered_img\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_IDs_to_labels","title":"<code>save_IDs_to_labels(savepath)</code>","text":"<p>saves the contents of the IDs_to_labels to the file savepath provided</p> <p>Parameters:</p> Name Type Description Default <code>savepath</code> <code>PATH_TYPE</code> <p>path to the file where the data must be saved</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_IDs_to_labels(self, savepath: PATH_TYPE):\n    \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n    Args:\n        savepath (PATH_TYPE): path to the file where the data must be saved\n    \"\"\"\n\n    # Save the classes filename\n    ensure_containing_folder(savepath)\n    if self.is_discrete_texture():\n        self.logger.info(\"discrete texture, saving classes\")\n        self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n        with open(savepath, \"w\") as outfile_h:\n            json.dump(\n                self.get_IDs_to_labels(), outfile_h, ensure_ascii=False, indent=4\n            )\n    else:\n        self.logger.warn(\"non-discrete texture, not saving classes\")\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_renders","title":"<code>save_renders(camera_set, render_image_scale=1.0, output_folder=Path(VIS_FOLDER, 'renders'), make_composites=False, save_native_resolution=False, cast_to_uint8=True, uint8_value_for_null_texture=NULL_TEXTURE_INT_VALUE, **render_kwargs)</code>","text":"<p>Render an image from the viewpoint of each specified camera and save a composite</p> <p>Parameters:</p> Name Type Description Default <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Camera set to use for rendering</p> required <code>render_image_scale</code> <code>float</code> <p>Multiplier on the real image scale to obtain size for rendering. Lower values yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.</p> <code>1.0</code> <code>render_folder</code> <code>PATH_TYPE</code> <p>Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")</p> required <code>make_composites</code> <code>bool</code> <p>Should a triple pane composite with the original image be saved rather than the raw label</p> <code>False</code> <code>cast_to_uint8</code> <code>bool</code> <p>(bool, optional): cast the float valued data to unit8 for saving efficiency. May dramatically increase efficiency due to png compression</p> <code>True</code> <code>uint8_value_for_null_texture</code> <code>uint8</code> <p>What value to assign for values that can't be represented as unsigned 8-bit data. Defaults to NULL_TEXTURE_INT_VALUE</p> <code>NULL_TEXTURE_INT_VALUE</code> <code>render_kwargs</code> <p>keyword arguments passed to the render.</p> <code>{}</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_renders(\n    self,\n    camera_set: PhotogrammetryCameraSet,\n    render_image_scale=1.0,\n    output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n    make_composites: bool = False,\n    save_native_resolution: bool = False,\n    cast_to_uint8: bool = True,\n    uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n    **render_kwargs,\n):\n    \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n    Args:\n        camera_set (PhotogrammetryCameraSet):\n            Camera set to use for rendering\n        render_image_scale (float, optional):\n            Multiplier on the real image scale to obtain size for rendering. Lower values\n            yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n        render_folder (PATH_TYPE, optional):\n            Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n        make_composites (bool, optional):\n            Should a triple pane composite with the original image be saved rather than the\n            raw label\n        cast_to_uint8: (bool, optional):\n            cast the float valued data to unit8 for saving efficiency. May dramatically increase\n            efficiency due to png compression\n        uint8_value_for_null_texture (np.uint8, optional):\n            What value to assign for values that can't be represented as unsigned 8-bit data.\n            Defaults to NULL_TEXTURE_INT_VALUE\n        render_kwargs:\n            keyword arguments passed to the render.\n    \"\"\"\n\n    ensure_folder(output_folder)\n    self.logger.info(f\"Saving renders to {output_folder}\")\n\n    # Save the classes filename\n    self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n    # Create the generator object to render the images\n    # Since this is a generator, this will be fast\n    render_gen = self.render_flat(\n        camera_set,\n        render_img_scale=render_image_scale,\n        return_camera=True,\n        **render_kwargs,\n    )\n\n    # The computation only happens when items are requested from the generator\n    for rendered, camera in tqdm(\n        render_gen,\n        total=len(camera_set),\n        desc=\"Computing and saving renders\",\n    ):\n        ## All this is post-processing to visualize the rendered label.\n        # rendered could either be a one channel image of integer IDs,\n        # a one-channel image of scalars, or a three-channel image of\n        # RGB. It could also be multi-channel image corresponding to anything,\n        # but we don't expect that yet\n        if save_native_resolution and render_image_scale != 1:\n            native_size = camera.get_image_size()\n            # Upsample using nearest neighbor interpolation for discrete labels and\n            # bilinear for non-discrete\n            # TODO this will need to be fixed for multi-channel images since I don't think resize works\n            rendered = resize(\n                rendered,\n                native_size,\n                order=(0 if self.is_discrete_texture() else 1),\n            )\n\n        if cast_to_uint8:\n            # Deterimine values that cannot be represented as uint8\n            mask = np.logical_or.reduce(\n                [\n                    rendered &lt; 0,\n                    rendered &gt; 255,\n                    np.logical_not(np.isfinite(rendered)),\n                ]\n            )\n            rendered[mask] = uint8_value_for_null_texture\n            # Cast and squeeze since you can't save a one-channel image\n            rendered = np.squeeze(rendered.astype(np.uint8))\n\n        if make_composites:\n            RGB_image = camera.get_image(\n                image_scale=(1.0 if save_native_resolution else render_image_scale)\n            )\n            rendered = create_composite(\n                RGB_image=RGB_image,\n                label_image=rendered,\n                IDs_to_labels=self.get_IDs_to_labels(),\n            )\n        else:\n            # Clip channels if needed\n            if rendered.ndim == 3:\n                rendered = rendered[..., :3]\n\n        # Saving\n        camera_filename = camera.get_image_filename().relative_to(\n            camera_set.image_folder\n        )\n        output_filename = Path(output_folder, camera_filename)\n        # This may create nested folders in the output dir\n        ensure_containing_folder(output_filename)\n        if rendered.dtype == np.uint8:\n            output_filename = str(output_filename.with_suffix(\".png\"))\n\n            # Save the image\n            skimage.io.imsave(output_filename, rendered, check_contrast=False)\n        else:\n            output_filename = str(output_filename.with_suffix(\".npy\"))\n            # Save the image\n            np.save(output_filename, rendered)\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.select_mesh_ROI","title":"<code>select_mesh_ROI(region_of_interest, buffer_meters=0, simplify_tol_meters=0, default_CRS=pyproj.CRS.from_epsg(4326), return_original_IDs=False)</code>","text":"<p>Get a subset of the mesh based on geospatial data</p> <p>Parameters:</p> Name Type Description Default <code>region_of_interest</code> <code>Union[GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]</code> <p>Region of interest. Can be a * dataframe, where all columns will be colapsed * A shapely polygon/multipolygon * A file that can be loaded by geopandas</p> required <code>buffer_meters</code> <code>float</code> <p>Expand the geometry by this amount of meters. Defaults to 0.</p> <code>0</code> <code>simplify_tol_meters</code> <code>float</code> <p>Simplify the geometry using this as the tolerance. Defaults to 0.</p> <code>0</code> <code>default_CRS</code> <code>CRS</code> <p>The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).</p> <code>from_epsg(4326)</code> <code>return_original_IDs</code> <code>bool</code> <p>Return the indices into the original mesh. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pyvista.PolyData: The subset of the mesh</p> <p>np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)</p> <p>np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def select_mesh_ROI(\n    self,\n    region_of_interest: typing.Union[\n        gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n    ],\n    buffer_meters: float = 0,\n    simplify_tol_meters: int = 0,\n    default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n    return_original_IDs: bool = False,\n):\n    \"\"\"Get a subset of the mesh based on geospatial data\n\n    Args:\n        region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n            Region of interest. Can be a\n            * dataframe, where all columns will be colapsed\n            * A shapely polygon/multipolygon\n            * A file that can be loaded by geopandas\n        buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n        simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n        default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n        return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n    Returns:\n        pyvista.PolyData: The subset of the mesh\n        np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n        np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n    \"\"\"\n    if region_of_interest is None:\n        return self.pyvista_mesh\n\n    # Get the ROI into a geopandas GeoDataFrame\n    self.logger.info(\"Standardizing ROI\")\n    if isinstance(region_of_interest, gpd.GeoDataFrame):\n        ROI_gpd = region_of_interest\n    elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n        ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n    else:\n        ROI_gpd = gpd.read_file(region_of_interest)\n\n    self.logger.info(\"Dissolving ROI\")\n    # Disolve to ensure there is only one row\n    ROI_gpd = ROI_gpd.dissolve()\n    self.logger.info(\"Setting CRS and buffering ROI\")\n    # Make sure we're using a projected CRS so a buffer can be applied\n    ROI_gpd = ensure_projected_CRS(ROI_gpd)\n    # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n    ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n    # Simplify the geometry to reduce the computational load\n    ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n    self.logger.info(\"Dissolving buffered ROI\")\n    # Disolve again in case\n    ROI_gpd = ROI_gpd.dissolve()\n\n    self.logger.info(\"Extracting verts for dataframe\")\n    # Get the vertices as a dataframe in the same CRS\n    verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n    self.logger.info(\"Checking intersection of verts with ROI\")\n    # Determine which vertices are within the ROI polygon\n    verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n    # Extract the IDs of the set within the polygon\n    vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n    self.logger.info(\"Extracting points from pyvista mesh\")\n    # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n    subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n    self.logger.info(\"Extraction surface from subset mesh\")\n    # Convert the unstructured grid to a PolyData (mesh) again\n    subset_mesh = subset_unstructured_grid.extract_surface()\n\n    # If we need the indices into the original mesh, return those\n    if return_original_IDs:\n        try:\n            point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n            face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n        except KeyError:\n            point_IDs = np.array([])\n            face_IDs = np.array([])\n\n        return (\n            subset_mesh,\n            point_IDs,\n            face_IDs,\n        )\n    # Else return just the mesh\n    return subset_mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.set_texture","title":"<code>set_texture(texture_array, IDs_to_labels=None, all_discrete_texture_values=None, is_vertex_texture=None, use_derived_IDs_to_labels=False, delete_existing=True)</code>","text":"<p>Set the internal texture representation</p> <p>Parameters:</p> Name Type Description Default <code>texture_array</code> <code>ndarray</code> <p>The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.</p> required <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping from integer IDs to string names. Defaults to None.</p> <code>None</code> <code>all_discrete_texture_values</code> <code>Union[List, None]</code> <p>Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>is_vertex_texture</code> <code>Union[bool, None]</code> <p>Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>use_derived_IDs_to_labels</code> <code>bool</code> <p>Use IDs to labels derived from data if not explicitly provided. Defaults to False.</p> <code>False</code> <code>delete_existing</code> <code>bool</code> <p>Delete the existing texture when the other one (face, vertex) is set. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the size of the texture doesn't match the number of either faces or vertices</p> <code>ValueError</code> <p>If the number of faces and vertices are the same and is_vertex_texture isn't set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def set_texture(\n    self,\n    texture_array: np.ndarray,\n    IDs_to_labels: typing.Union[None, dict] = None,\n    all_discrete_texture_values: typing.Union[typing.List, None] = None,\n    is_vertex_texture: typing.Union[bool, None] = None,\n    use_derived_IDs_to_labels: bool = False,\n    delete_existing: bool = True,\n):\n    \"\"\"Set the internal texture representation\n\n    Args:\n        texture_array (np.ndarray):\n            The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.\n        IDs_to_labels (typing.Union[None, dict], optional): Mapping from integer IDs to string names. Defaults to None.\n        all_discrete_texture_values (typing.Union[typing.List, None], optional):\n            Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.\n        is_vertex_texture (typing.Union[bool, None], optional):\n            Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.\n        use_derived_IDs_to_labels (bool, optional): Use IDs to labels derived from data if not explicitly provided. Defaults to False.\n        delete_existing (bool, optional): Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n\n    Raises:\n        ValueError: If the size of the texture doesn't match the number of either faces or vertices\n        ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n    \"\"\"\n    texture_array = self.standardize_texture(texture_array)\n    # IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n\n    # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n    # TODO consider refactoring to check whether it matches the number of one of them,\n    # no matter whether is_vertex_texture is specified\n    if is_vertex_texture is None:\n        # Check that the number of matches face or verts\n        n_values = texture_array.shape[0]\n        n_faces = self.faces.shape[0]\n        n_verts = self.pyvista_mesh.points.shape[0]\n\n        if n_verts == n_faces:\n            raise ValueError(\n                \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n            )\n        elif n_values == n_verts:\n            is_vertex_texture = True\n        elif n_values == n_faces:\n            is_vertex_texture = False\n        else:\n            raise ValueError(\n                f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n            )\n\n    # Ensure that the actual data type is float, and record label names\n    if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n        # If it is more than one column, it's assumed to be a real-valued\n        # quantity and we try to cast it to a float\n        texture_array = texture_array.astype(float)\n        derived_IDs_to_labels = None\n    else:\n        texture_array, derived_IDs_to_labels = ensure_float_labels(\n            texture_array, full_array=all_discrete_texture_values\n        )\n\n    # If IDs to labels is explicitly provided, trust that\n    # TODO should do some type checking here\n    if isinstance(IDs_to_labels, dict):\n        self.IDs_to_labels = IDs_to_labels\n    # If not, but we can compute it, use that. Otherwise, we might want to force them to be set to None\n    elif use_derived_IDs_to_labels:\n        self.IDs_to_labels = derived_IDs_to_labels\n\n    # Set the appropriate texture and optionally delete the other one\n    if is_vertex_texture:\n        self.vertex_texture = texture_array\n        if delete_existing:\n            self.face_texture = None\n    else:\n        self.face_texture = texture_array\n        if delete_existing:\n            self.vertex_texture = None\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.transfer_texture","title":"<code>transfer_texture(downsampled_mesh)</code>","text":"<p>Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches</p> <p>Parameters:</p> Name Type Description Default <code>downsampled_mesh</code> <code>PolyData</code> <p>The downsampled version of the original mesh</p> required <p>Returns:</p> Type Description <p>pv.PolyData: The downsampled mesh with the transferred textures</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def transfer_texture(self, downsampled_mesh):\n    \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n    Args:\n        downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n    Returns:\n        pv.PolyData: The downsampled mesh with the transferred textures\n    \"\"\"\n    # Only transfer textures if there are point based scalars in the original mesh\n    if self.pyvista_mesh.point_data:\n        # Store original mesh points in KDTree for nearest neighbor search\n        kdtree = KDTree(self.pyvista_mesh.points)\n\n        # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n        _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n        # Iterate over all the point based scalars\n        for scalar_name in self.pyvista_mesh.point_data.keys():\n            # Retrieve scalar data of appropriate index using the nearest neighbor indices\n            transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                nearest_neighbor_indices\n            ]\n            # Set the corresponding scalar data in the downsampled mesh\n            downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n        # Set active mesh of downsampled mesh\n        if self.pyvista_mesh.active_scalars_name:\n            downsampled_mesh.active_scalars_name = (\n                self.pyvista_mesh.active_scalars_name\n            )\n    else:\n        self.logger.warning(\n            \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n        )\n    return downsampled_mesh\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.transform_vertices","title":"<code>transform_vertices(transform_4x4, in_place=False)</code>","text":"<p>Apply a transform to the vertex coordinates</p> <p>Parameters:</p> Name Type Description Default <code>transform_4x4</code> <code>ndarray</code> <p>Transform to be applied</p> required <code>in_place</code> <code>bool</code> <p>Should the vertices be updated for all member objects</p> <code>False</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def transform_vertices(self, transform_4x4: np.ndarray, in_place: bool = False):\n    \"\"\"Apply a transform to the vertex coordinates\n\n    Args:\n        transform_4x4 (np.ndarray): Transform to be applied\n        in_place (bool): Should the vertices be updated for all member objects\n    \"\"\"\n    homogenous_local_points = np.vstack(\n        (self.pyvista_mesh.points.T, np.ones(self.pyvista_mesh.points.shape[0]))\n    )\n    transformed_local_points = transform_4x4 @ homogenous_local_points\n    transformed_local_points = transformed_local_points[:3].T\n\n    # Overwrite existing vertices in both pytorch3d and pyvista mesh\n    if in_place:\n        self.pyvista_mesh.points = transformed_local_points.copy()\n    return transformed_local_points\n</code></pre>"},{"location":"API_reference/meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.vis","title":"<code>vis(plotter=None, interactive=True, camera_set=None, screenshot_filename=None, vis_scalars=None, mesh_kwargs=None, interactive_jupyter=False, plotter_kwargs={}, enable_ssao=True, force_xvfb=False, frustum_scale=2, IDs_to_labels=None)</code>","text":"<p>Show the mesh and cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to use, else one will be created</p> <code>None</code> <code>off_screen</code> <code>bool</code> <p>Show offscreen</p> required <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Cameras to visualize. Defaults to None.</p> <code>None</code> <code>screenshot_filename</code> <code>PATH_TYPE</code> <p>Filepath to save to, will show interactively if None. Defaults to None.</p> <code>None</code> <code>vis_scalars</code> <code>(None, ndarray)</code> <p>Scalars to show</p> <code>None</code> <code>mesh_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the mesh</p> <code>None</code> <code>interactive_jupyter</code> <code>bool</code> <p>Should jupyter windows be interactive. This doesn't always work, especially on VSCode.</p> <code>False</code> <code>plotter_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the plotter</p> <code>{}</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. Defaults to None.</p> <code>2</code> <code>IDs_to_labels</code> <code>[None, dict]</code> <p>Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh IDs_to_labels if unset.</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    interactive: bool = True,\n    camera_set: PhotogrammetryCameraSet = None,\n    screenshot_filename: PATH_TYPE = None,\n    vis_scalars: typing.Union[None, np.ndarray] = None,\n    mesh_kwargs: typing.Dict = None,\n    interactive_jupyter: bool = False,\n    plotter_kwargs: typing.Dict = {},\n    enable_ssao: bool = True,\n    force_xvfb: bool = False,\n    frustum_scale: float = 2,\n    IDs_to_labels: typing.Union[None, dict] = None,\n):\n    \"\"\"Show the mesh and cameras\n\n    Args:\n        plotter (pyvista.Plotter, optional):\n            Plotter to use, else one will be created\n        off_screen (bool, optional):\n            Show offscreen\n        camera_set (PhotogrammetryCameraSet, optional):\n            Cameras to visualize. Defaults to None.\n        screenshot_filename (PATH_TYPE, optional):\n            Filepath to save to, will show interactively if None. Defaults to None.\n        vis_scalars (None, np.ndarray):\n            Scalars to show\n        mesh_kwargs:\n            dict of keyword arguments for the mesh\n        interactive_jupyter (bool):\n            Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n        plotter_kwargs:\n            dict of keyword arguments for the plotter\n        frustum_scale (float, optional):\n            Size of cameras in world units. Defaults to None.\n        IDs_to_labels ([None, dict], optional):\n            Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n            IDs_to_labels if unset.\n    \"\"\"\n    off_screen = (not interactive) or (screenshot_filename is not None)\n\n    # If the IDs to labels is not set, use the default ones for this mesh\n    if IDs_to_labels is None:\n        IDs_to_labels = self.get_IDs_to_labels()\n\n    # Set the mesh kwargs if not set\n    if mesh_kwargs is None:\n        # This needs to be a dict, even if it's empty\n        mesh_kwargs = {}\n\n        # If there are discrete labels, set the colormap and limits inteligently\n        if IDs_to_labels is not None:\n            # Compute the largest ID\n            max_ID = max(IDs_to_labels.keys())\n            if max_ID &lt; 20:\n                colors = [\n                    matplotlib.colors.to_hex(c)\n                    for c in plt.get_cmap(\n                        (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                    ).colors\n                ]\n                mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n    # Create the plotter if it's None\n    plotter = create_pv_plotter(\n        off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n    )\n\n    # If the vis scalars are None, use the saved texture\n    if vis_scalars is None:\n        vis_scalars = self.get_texture(\n            # Request vertex texture if both are available\n            request_vertex_texture=(\n                True\n                if (\n                    self.vertex_texture is not None\n                    and self.face_texture is not None\n                )\n                else None\n            )\n        )\n\n    is_rgb = (\n        self.pyvista_mesh.active_scalars_name == \"RGB\"\n        if vis_scalars is None\n        else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n    )\n\n    # Data in the range [0, 255] must be uint8 type\n    if is_rgb and np.nanmax(vis_scalars) &gt; 1.0:\n        vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n    scalar_bar_args = {\"vertical\": True}\n    if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n        mesh_kwargs[\"annotations\"] = IDs_to_labels\n        scalar_bar_args[\"n_labels\"] = 0\n\n    # Add the mesh\n    plotter.add_mesh(\n        self.pyvista_mesh,\n        scalars=vis_scalars,\n        rgb=is_rgb,\n        scalar_bar_args=scalar_bar_args,\n        **mesh_kwargs,\n    )\n    # If the camera set is provided, show this too\n    if camera_set is not None:\n        # Adjust the frustum scale if the mesh came from metashape\n        # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n        if (\n            self.local_to_epgs_4978_transform is not None\n            and frustum_scale is not None\n        ):\n            transform_determinant = np.linalg.det(\n                self.local_to_epgs_4978_transform[:3, :3]\n            )\n            scale_factor = np.cbrt(transform_determinant)\n            frustum_scale = frustum_scale / scale_factor\n        camera_set.vis(\n            plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n        )\n\n    # Enable screen space shading\n    if enable_ssao:\n        plotter.enable_ssao()\n\n    # Create parent folder if none exists\n    if screenshot_filename is not None:\n        ensure_containing_folder(screenshot_filename)\n\n    if \"jupyter_backend\" not in plotter_kwargs:\n        if interactive_jupyter:\n            plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n        else:\n            plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n    if \"title\" not in plotter_kwargs:\n        plotter_kwargs[\"title\"] = \"Geograypher mesh viewer\"\n\n    # Show\n    return plotter.show(\n        screenshot=screenshot_filename,\n        **plotter_kwargs,\n    )\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/","title":"Derived Segmentors","text":""},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.BrightnessSegmentor","title":"<code>BrightnessSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class BrightnessSegmentor(Segmentor):\n    def __init__(self, brightness_threshold: float = np.sqrt(0.75)):\n        self.brightness_threshold = brightness_threshold\n        self.num_classes = 2\n\n    def segment_image(self, image: np.ndarray, **kwargs):\n        image_brightness = np.linalg.norm(image, axis=-1)\n        thresholded_image = image_brightness &gt; self.brightness_threshold\n        class_index_image = thresholded_image.astype(np.uint8)\n        one_hot_image = self.inds_to_one_hot(class_index_image)\n        return one_hot_image\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.LookUpSegmentor","title":"<code>LookUpSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class LookUpSegmentor(Segmentor):\n    def __init__(self, base_folder, lookup_folder, num_classes=10):\n        self.base_folder = Path(base_folder)\n        self.lookup_folder = lookup_folder\n        self.num_classes = num_classes\n\n    def segment_image(self, image: np.ndarray, filename: PATH_TYPE, image_scale: float):\n        relative_path = Path(filename).relative_to(self.base_folder)\n        lookup_path = Path(self.lookup_folder, relative_path)\n        lookup_path = lookup_path.with_suffix(\".png\")\n\n        image = imread(lookup_path)\n        if image_scale != 1:\n            image = resize(\n                image,\n                (int(image.shape[0] * image_scale), int(image.shape[1] * image_scale)),\n                order=0,  # Nearest neighbor interpolation\n            )\n        one_hot_image = self.inds_to_one_hot(image, num_classes=self.num_classes)\n        return one_hot_image\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor","title":"<code>TabularRectangleSegmentor</code>","text":"<p>               Bases: <code>Segmentor</code></p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>class TabularRectangleSegmentor(Segmentor):\n    def __init__(\n        self,\n        detection_file_or_folder: PATH_TYPE,\n        image_shape: tuple,\n        label_key: str = \"instance_ID\",\n        image_path_key: str = \"image_path\",\n        imin_key: str = \"ymin\",\n        imax_key: str = \"ymax\",\n        jmin_key: str = \"xmin\",\n        jmax_key: str = \"xmax\",\n        detection_file_extension: str = \"csv\",\n        strip_image_extension: bool = False,\n        use_absolute_filepaths: bool = False,\n        split_bbox: bool = True,\n        image_folder: typing.Union[PATH_TYPE, None] = None,\n    ):\n        \"\"\"Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.\n\n        Args:\n            detection_file_or_folder (PATH_TYPE):\n                Path to the CSV file with detections or a folder thereof\n            image_shape (tuple):\n                The (height, width) shape of the image in pixels.\n            label_key (str, optional):\n                The column that corresponds to the class. Defaults to \"label\".\n            image_path_key (str, optional):\n                The column that has the image filename. Defaults to \"image_path\".\n            imin_key (str, optional):\n                Column of the minimum i dimension. Defaults to \"ymin\".\n            imax_key (str, optional):\n                Column of the max i dimension. Defaults to \"ymax\".\n            jmin_key (str, optional):\n                Column of the min j dimension. Defaults to \"xmin\".\n            jmax_key (str, optional):\n                Column of the max j dimension. Defaults to \"xmax\".\n            detection_file_extension (str, optional):\n                File extension of the detection files. Defaults to \"csv\".\n            strip_image_extension (bool, optional):\n                Remove the extension from the image filenames. Defaults to True.\n            use_absolute_filepaths (bool, optional):\n                Add the absolute path from the image folder to the filenames. Defaults to False.\n            split_bbox (bool, optional):\n                Split the bounding box from one column rather than having seperate columns for imin,\n                imax, jmin, jmax. Defaults to True.\n            image_folder (PATH_TYPE, optional): Path to the image folder. Defaults to None.\n        \"\"\"\n        self.image_shape = image_shape\n\n        self.label_key = label_key\n        self.image_path_key = image_path_key\n        self.imin_key = imin_key\n        self.imax_key = imax_key\n        self.jmin_key = jmin_key\n        self.jmax_key = jmax_key\n        self.split_bbox = split_bbox\n\n        # Load the detections\n        self.labels_df = self.load_detection_files(\n            detection_file_or_folder=detection_file_or_folder,\n            detection_file_extension=detection_file_extension,\n            image_folder=image_folder,\n            use_absolute_filepaths=use_absolute_filepaths,\n            strip_image_extension=strip_image_extension,\n            image_path_key=image_path_key,\n        )\n\n        # Group the predictions\n        self.grouped_labels_df = self.labels_df.groupby(by=self.image_path_key)\n\n        # List the images\n        self.image_names = list(self.grouped_labels_df.groups.keys())\n        # Record the class names and number of classes\n        self.class_names = np.unique(self.labels_df[self.label_key]).tolist()\n        self.num_classes = len(self.class_names)\n\n    def load_detection_files(\n        self,\n        detection_file_or_folder: PATH_TYPE,\n        detection_file_extension: str,\n        image_folder: PATH_TYPE,\n        use_absolute_filepaths: bool,\n        strip_image_extension: bool,\n        image_path_key: str,\n    ):\n        # Determine whether the input is a file or folder\n        if Path(detection_file_or_folder).is_file():\n            # If it's a file, make a one-length list\n            files = [detection_file_or_folder]\n        else:\n            # List all the files in the folder with the requested extesion\n            files = sorted(\n                Path(detection_file_or_folder).glob(\"*\" + detection_file_extension)\n            )\n\n        # Read the individual files\n        dfs = [pd.read_csv(f) for f in files]\n\n        # Concatenate the dataframes into one\n        labels_df = pd.concat(dfs, ignore_index=True)\n\n        # Add an sequential instance ID column if not present\n        if \"instance_ID\" not in labels_df.columns:\n            labels_df[\"instance_ID\"] = labels_df.index\n\n        # Prepend the image folder to the image filenames if requested to make an absolute filepath\n        if image_folder is not None and use_absolute_filepaths:\n            absolute_filepaths = [\n                str(Path(image_folder, img_path))\n                for img_path in labels_df[image_path_key].tolist()\n            ]\n            labels_df[image_path_key] = absolute_filepaths\n\n        # Strip the extension from the image filenames if requested\n        if strip_image_extension:\n            image_path_without_ext = [\n                str(Path(img_path).with_suffix(\"\"))\n                for img_path in labels_df[image_path_key].tolist()\n            ]\n            labels_df[image_path_key] = image_path_without_ext\n\n        return labels_df\n\n    def get_all_detections(self) -&gt; pd.DataFrame:\n        \"\"\"Return the aggregated detections dataframe\"\"\"\n        return self.labels_df\n\n    def save_detection_data(self, output_csv_file: PATH_TYPE):\n        \"\"\"Save the aggregated detections to a file\n\n        Args:\n            output_csv_file (PATH_TYPE):\n                A path to a CSV file to save the detections to. The containing folder will be\n                created if needed.\n        \"\"\"\n        ensure_containing_folder(output_csv_file)\n        self.labels_df.to_csv(output_csv_file)\n\n    def get_corners(self, data, as_int=True):\n        if self.split_bbox:\n            # TODO split row\n            bbox = data[\"bbox\"]\n            bbox = bbox[1:-1]\n            splits = bbox.split(\", \")\n            jmin, imin, width, height = [float(s) for s in splits]\n\n            imax = imin + height\n            jmax = jmin + width\n\n            imin = imin\n            jmin = jmin\n        else:\n            imin = data[self.imin_key]\n            imax = data[self.imax_key]\n            jmin = data[self.jmin_key]\n            jmax = data[self.jmax_key]\n\n        corners = imin, jmin, imax, jmax\n        if as_int:\n            corners = list(map(int, corners))\n\n        return corners\n\n    def segment_image(self, image, filename, image_scale, vis=False):\n        output_shape = self.image_shape\n        label_image = np.full(output_shape, fill_value=np.nan, dtype=float)\n\n        name = filename.name\n        if name in self.image_names:\n            df = self.grouped_labels_df.get_group(name)\n        # Return an all-zero segmentation image\n        else:\n            return label_image\n\n        for _, row in df.iterrows():\n            label = row[self.label_key]\n            label_ind = self.class_names.index(label)\n\n            imin, jmin, imax, jmax = self.get_corners(\n                row,\n            )\n\n            label_image[imin:imax, jmin:jmax] = label_ind\n\n        if vis:\n            plt.imshow(label_image, vmin=0, vmax=10, cmap=\"tab10\")\n            plt.show()\n\n        if image_scale != 1.0:\n            output_size = (int(image_scale * x) for x in label_image.shape[:2])\n            label_image = resize(label_image, output_size, order=0)\n\n        return label_image\n\n    def get_detection_centers(self, filename):\n        \"\"\"_summary_\n\n        Args:\n            filename (_type_): _description_\n\n        Returns:\n            _type_: (n,2) array for (i,j) centers for each detection\n        \"\"\"\n        if filename not in self.image_names:\n            # Empty array of detection centers\n            return np.zeros((0, 2))\n\n        # Extract the corresponding dataframe\n        df = self.grouped_labels_df.get_group(filename)\n\n        all_corners = []\n        for _, row in df.iterrows():\n            corners = self.get_corners(row, as_int=False)\n            all_corners.append(corners)\n\n        all_corners = zip(*all_corners)\n        all_corners = [np.array(x) for x in all_corners]\n\n        imin, jmin, imax, jmax = all_corners\n\n        # Average the left-right, top-bottom pairs\n        centers = np.vstack([(imin + imax) / 2, (jmin + jmax) / 2]).T\n        return centers\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.__init__","title":"<code>__init__(detection_file_or_folder, image_shape, label_key='instance_ID', image_path_key='image_path', imin_key='ymin', imax_key='ymax', jmin_key='xmin', jmax_key='xmax', detection_file_extension='csv', strip_image_extension=False, use_absolute_filepaths=False, split_bbox=True, image_folder=None)</code>","text":"<p>Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.</p> <p>Parameters:</p> Name Type Description Default <code>detection_file_or_folder</code> <code>PATH_TYPE</code> <p>Path to the CSV file with detections or a folder thereof</p> required <code>image_shape</code> <code>tuple</code> <p>The (height, width) shape of the image in pixels.</p> required <code>label_key</code> <code>str</code> <p>The column that corresponds to the class. Defaults to \"label\".</p> <code>'instance_ID'</code> <code>image_path_key</code> <code>str</code> <p>The column that has the image filename. Defaults to \"image_path\".</p> <code>'image_path'</code> <code>imin_key</code> <code>str</code> <p>Column of the minimum i dimension. Defaults to \"ymin\".</p> <code>'ymin'</code> <code>imax_key</code> <code>str</code> <p>Column of the max i dimension. Defaults to \"ymax\".</p> <code>'ymax'</code> <code>jmin_key</code> <code>str</code> <p>Column of the min j dimension. Defaults to \"xmin\".</p> <code>'xmin'</code> <code>jmax_key</code> <code>str</code> <p>Column of the max j dimension. Defaults to \"xmax\".</p> <code>'xmax'</code> <code>detection_file_extension</code> <code>str</code> <p>File extension of the detection files. Defaults to \"csv\".</p> <code>'csv'</code> <code>strip_image_extension</code> <code>bool</code> <p>Remove the extension from the image filenames. Defaults to True.</p> <code>False</code> <code>use_absolute_filepaths</code> <code>bool</code> <p>Add the absolute path from the image folder to the filenames. Defaults to False.</p> <code>False</code> <code>split_bbox</code> <code>bool</code> <p>Split the bounding box from one column rather than having seperate columns for imin, imax, jmin, jmax. Defaults to True.</p> <code>True</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>Path to the image folder. Defaults to None.</p> <code>None</code> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def __init__(\n    self,\n    detection_file_or_folder: PATH_TYPE,\n    image_shape: tuple,\n    label_key: str = \"instance_ID\",\n    image_path_key: str = \"image_path\",\n    imin_key: str = \"ymin\",\n    imax_key: str = \"ymax\",\n    jmin_key: str = \"xmin\",\n    jmax_key: str = \"xmax\",\n    detection_file_extension: str = \"csv\",\n    strip_image_extension: bool = False,\n    use_absolute_filepaths: bool = False,\n    split_bbox: bool = True,\n    image_folder: typing.Union[PATH_TYPE, None] = None,\n):\n    \"\"\"Lookup rectangular bounding boxes corresponding to detections from a CSV or folder of them.\n\n    Args:\n        detection_file_or_folder (PATH_TYPE):\n            Path to the CSV file with detections or a folder thereof\n        image_shape (tuple):\n            The (height, width) shape of the image in pixels.\n        label_key (str, optional):\n            The column that corresponds to the class. Defaults to \"label\".\n        image_path_key (str, optional):\n            The column that has the image filename. Defaults to \"image_path\".\n        imin_key (str, optional):\n            Column of the minimum i dimension. Defaults to \"ymin\".\n        imax_key (str, optional):\n            Column of the max i dimension. Defaults to \"ymax\".\n        jmin_key (str, optional):\n            Column of the min j dimension. Defaults to \"xmin\".\n        jmax_key (str, optional):\n            Column of the max j dimension. Defaults to \"xmax\".\n        detection_file_extension (str, optional):\n            File extension of the detection files. Defaults to \"csv\".\n        strip_image_extension (bool, optional):\n            Remove the extension from the image filenames. Defaults to True.\n        use_absolute_filepaths (bool, optional):\n            Add the absolute path from the image folder to the filenames. Defaults to False.\n        split_bbox (bool, optional):\n            Split the bounding box from one column rather than having seperate columns for imin,\n            imax, jmin, jmax. Defaults to True.\n        image_folder (PATH_TYPE, optional): Path to the image folder. Defaults to None.\n    \"\"\"\n    self.image_shape = image_shape\n\n    self.label_key = label_key\n    self.image_path_key = image_path_key\n    self.imin_key = imin_key\n    self.imax_key = imax_key\n    self.jmin_key = jmin_key\n    self.jmax_key = jmax_key\n    self.split_bbox = split_bbox\n\n    # Load the detections\n    self.labels_df = self.load_detection_files(\n        detection_file_or_folder=detection_file_or_folder,\n        detection_file_extension=detection_file_extension,\n        image_folder=image_folder,\n        use_absolute_filepaths=use_absolute_filepaths,\n        strip_image_extension=strip_image_extension,\n        image_path_key=image_path_key,\n    )\n\n    # Group the predictions\n    self.grouped_labels_df = self.labels_df.groupby(by=self.image_path_key)\n\n    # List the images\n    self.image_names = list(self.grouped_labels_df.groups.keys())\n    # Record the class names and number of classes\n    self.class_names = np.unique(self.labels_df[self.label_key]).tolist()\n    self.num_classes = len(self.class_names)\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.get_all_detections","title":"<code>get_all_detections()</code>","text":"<p>Return the aggregated detections dataframe</p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def get_all_detections(self) -&gt; pd.DataFrame:\n    \"\"\"Return the aggregated detections dataframe\"\"\"\n    return self.labels_df\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.get_detection_centers","title":"<code>get_detection_centers(filename)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>_type_</code> <p>description</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>(n,2) array for (i,j) centers for each detection</p> Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def get_detection_centers(self, filename):\n    \"\"\"_summary_\n\n    Args:\n        filename (_type_): _description_\n\n    Returns:\n        _type_: (n,2) array for (i,j) centers for each detection\n    \"\"\"\n    if filename not in self.image_names:\n        # Empty array of detection centers\n        return np.zeros((0, 2))\n\n    # Extract the corresponding dataframe\n    df = self.grouped_labels_df.get_group(filename)\n\n    all_corners = []\n    for _, row in df.iterrows():\n        corners = self.get_corners(row, as_int=False)\n        all_corners.append(corners)\n\n    all_corners = zip(*all_corners)\n    all_corners = [np.array(x) for x in all_corners]\n\n    imin, jmin, imax, jmax = all_corners\n\n    # Average the left-right, top-bottom pairs\n    centers = np.vstack([(imin + imax) / 2, (jmin + jmax) / 2]).T\n    return centers\n</code></pre>"},{"location":"API_reference/predictors/derived_segmentors/#geograypher.predictors.derived_segmentors.TabularRectangleSegmentor.save_detection_data","title":"<code>save_detection_data(output_csv_file)</code>","text":"<p>Save the aggregated detections to a file</p> <p>Parameters:</p> Name Type Description Default <code>output_csv_file</code> <code>PATH_TYPE</code> <p>A path to a CSV file to save the detections to. The containing folder will be created if needed.</p> required Source code in <code>geograypher/predictors/derived_segmentors.py</code> <pre><code>def save_detection_data(self, output_csv_file: PATH_TYPE):\n    \"\"\"Save the aggregated detections to a file\n\n    Args:\n        output_csv_file (PATH_TYPE):\n            A path to a CSV file to save the detections to. The containing folder will be\n            created if needed.\n    \"\"\"\n    ensure_containing_folder(output_csv_file)\n    self.labels_df.to_csv(output_csv_file)\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/","title":"Ortho segmentor","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor","title":"<code>ortho_segmentor</code>","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.assemble_tiled_predictions","title":"<code>assemble_tiled_predictions(raster_file, pred_folder, class_savefile, num_classes, counts_savefile=None, downweight_edge_frac=0.25, nodataval=NULL_TEXTURE_INT_VALUE, count_dtype=np.uint8, max_overlapping_tiles=4)</code>","text":"<p>Take tiled predictions on disk and aggregate them into a raster</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>Path to the raster file used to generate chips. This is required only to understand the geospatial reference.</p> required <code>pred_folder</code> <code>PATH_TYPE</code> <p>A folder where every file is a prediction for a tile. The filename must encode the bounds of the windowed crop.</p> required <code>class_savefile</code> <code>PATH_TYPE</code> <p>Where to save the merged raster.</p> required <code>counts_savefile</code> <code>Union[PATH_TYPE, NoneType]</code> <p>Where to save the counts for the merged predictions raster. A tempfile will be created and then deleted if not specified. Defaults to None.</p> <code>None</code> <code>downweight_edge_frac</code> <code>float</code> <p>Downweight this fraction of predictions at the edge of each tile using a linear ramp. Defaults to 0.25.</p> <code>0.25</code> <code>nodataval</code> <code>Union[int, None]</code> <p>(typing.Union[int, None]): Value for unassigned pixels. If None, will be set to len(class_names), the first unused class. Defaults to 255</p> <code>NULL_TEXTURE_INT_VALUE</code> <code>count_dtype</code> <code>type</code> <p>What type to use for aggregation. Float uses more space but is more accurate. Defaults to np.uint8</p> <code>uint8</code> <code>max_overlapping_tiles</code> <code>int</code> <p>The max number of prediction tiles that may overlap at a given point. This is used to upper bound the valud in the count matrix, because we use scaled np.uint8 values rather than floats for efficiency. Setting a lower value enables slightly more accuracy in the aggregation process, but too low can lead to overflow. Defaults to 4</p> <code>4</code> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def assemble_tiled_predictions(\n    raster_file: PATH_TYPE,\n    pred_folder: PATH_TYPE,\n    class_savefile: PATH_TYPE,\n    num_classes: int,\n    counts_savefile: Union[PATH_TYPE, None] = None,\n    downweight_edge_frac: float = 0.25,\n    nodataval: Union[int, None] = NULL_TEXTURE_INT_VALUE,\n    count_dtype: type = np.uint8,\n    max_overlapping_tiles: int = 4,\n):\n    \"\"\"Take tiled predictions on disk and aggregate them into a raster\n\n    Args:\n        raster_file (PATH_TYPE):\n            Path to the raster file used to generate chips. This is required only to understand the\n            geospatial reference.\n        pred_folder (PATH_TYPE):\n            A folder where every file is a prediction for a tile. The filename must encode the\n            bounds of the windowed crop.\n        class_savefile (PATH_TYPE):\n            Where to save the merged raster.\n        counts_savefile (typing.Union[PATH_TYPE, NoneType], optional):\n            Where to save the counts for the merged predictions raster.\n            A tempfile will be created and then deleted if not specified. Defaults to None.\n        downweight_edge_frac (float, optional):\n            Downweight this fraction of predictions at the edge of each tile using a linear ramp. Defaults to 0.25.\n        nodataval: (typing.Union[int, None]):\n            Value for unassigned pixels. If None, will be set to len(class_names), the first unused class. Defaults to 255\n        count_dtype (type, optional):\n            What type to use for aggregation. Float uses more space but is more accurate. Defaults to np.uint8\n        max_overlapping_tiles (int):\n            The max number of prediction tiles that may overlap at a given point. This is used to upper bound the valud in the count matrix,\n            because we use scaled np.uint8 values rather than floats for efficiency. Setting a lower value enables slightly more accuracy in the\n            aggregation process, but too low can lead to overflow. Defaults to 4\n    \"\"\"\n    # Find the filenames of tiled predictions\n    pred_files = [f for f in pred_folder.glob(\"*\") if f.is_file()]\n\n    # Set nodataval to the first unused class ID\n    if nodataval is None:\n        nodataval = num_classes\n\n    # If the user didn't specify where to write the counts, create a tempfile that will be deleted\n    if counts_savefile is None:\n        # Create the containing folder if required\n        ensure_containing_folder(class_savefile)\n        counts_savefile_manager = tempfile.NamedTemporaryFile(\n            mode=\"w+\", suffix=\".tif\", dir=class_savefile.parent\n        )\n        counts_savefile = counts_savefile_manager.name\n\n    # Parse the filenames to get the windows\n    # TODO consider using the extent to only write a file for the minimum encolsing rectangle\n    windows, extent = parse_windows_from_files(pred_files, return_in_extent_coords=True)\n\n    # Aggregate predictions\n    with rio.open(raster_file) as src:\n        # Create file to store counts that is the same as the input raster except it has num_classes number of bands\n        # TODO make this only the size of the extent computed by parse_windows_from_files\n        extent_transform = src.window_transform(extent)\n\n        with rio.open(\n            counts_savefile,\n            \"w+\",\n            driver=\"GTiff\",\n            height=extent.height,\n            width=extent.width,\n            count=num_classes,\n            dtype=count_dtype,\n            crs=src.crs,\n            transform=extent_transform,\n        ) as dst:\n            # Create\n            pred_weighting_dict = {}\n            for pred_file, window in tqdm(\n                zip(pred_files, windows),\n                desc=\"Aggregating raster predictions\",\n                total=len(pred_files),\n            ):\n                # Read the prediction from disk\n                pred = read_image_or_numpy(pred_file)\n\n                if pred.shape != (window.height, window.width):\n                    raise ValueError(\"Size of pred does not match window\")\n\n                # We want to downweight portions at the edge so we create a ramped weighting mask\n                # but we don't want to duplicate this computation because it's the same for each same sized chip\n                if pred.shape not in pred_weighting_dict:\n                    # We want to keep this as a uint8\n                    pred_weighting = create_ramped_weighting(\n                        pred.shape, downweight_edge_frac\n                    )\n\n                    # Allow us to get as much granularity as possible given the datatype\n                    if count_dtype is not float:\n                        pred_weighting = pred_weighting * (\n                            np.iinfo(count_dtype).max / max_overlapping_tiles\n                        )\n                    # Convert weighting to desired type\n                    pred_weighting_dict[pred.shape] = pred_weighting.astype(count_dtype)\n\n                # Get weighting\n                pred_weighting = pred_weighting_dict[pred.shape]\n\n                # Update each band in the counts file within the window\n                for i in range(num_classes):\n                    # Bands in rasterio are 1-indexed\n                    band_ind = i + 1\n                    class_i_window_counts = dst.read(band_ind, window=window)\n                    class_i_preds = pred == i\n                    # If nothing matches this class, don't waste computation\n                    if not np.any(class_i_preds):\n                        continue\n                    # Weight the predictions to downweight the ones at the edge\n                    weighted_preds = (class_i_preds * pred_weighting).astype(\n                        count_dtype\n                    )\n                    # Add the new predictions to the previous counts\n                    class_i_window_counts += weighted_preds\n                    # Write out the updated results for this window\n                    dst.write(class_i_window_counts, band_ind, window=window)\n\n    ## Convert counts file to max-class file\n\n    with rio.open(counts_savefile, \"r\") as src:\n        # Create a one-band file to store the index of the most predicted class\n        with rio.open(\n            class_savefile,\n            \"w\",\n            driver=\"GTiff\",\n            height=src.shape[0],\n            width=src.shape[1],\n            count=1,\n            dtype=np.uint8,\n            crs=src.crs,\n            transform=src.transform,\n            nodata=nodataval,\n        ) as dst:\n            # Iterate over the blocks corresponding to the tiff driver in the dataset\n            # to compute the max class and write it out\n            for _, window in tqdm(\n                list(src.block_windows()), desc=\"Writing out max class\"\n            ):\n                # Read in the counts\n                counts_array = src.read(window=window)\n                # Compute which pixels have no recorded predictions and mask them out\n                nodata_mask = np.sum(counts_array, axis=0) == 0\n\n                # If it's all nodata, don't write it out\n                # TODO make sure this works as expected\n                if np.all(nodata_mask):\n                    continue\n\n                # Compute which class had the highest counts\n                max_class = np.argmax(counts_array, axis=0)\n                max_class[nodata_mask] = nodataval\n                # TODO, it would be good to check if it's all nodata and skip the write because that's unneeded\n                dst.write(max_class, 1, window=window)\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.parse_windows_from_files","title":"<code>parse_windows_from_files(files, sep=':', return_in_extent_coords=True)</code>","text":"<p>Return the boxes and extent from a list of filenames</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[Path]</code> <p>List of filenames</p> required <code>sep</code> <code>str</code> <p>Seperator between elements</p> <code>':'</code> <code>return_in_extent_coords</code> <code>bool</code> <p>Return in the coordinate frame of the extent</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[list[Window], Window]</code> <p>tuple[list[Window], Window]: List of windows for each file and extent</p> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def parse_windows_from_files(\n    files: list[Path], sep: str = \":\", return_in_extent_coords: bool = True\n) -&gt; tuple[list[Window], Window]:\n    \"\"\"Return the boxes and extent from a list of filenames\n\n    Args:\n        files (list[Path]): List of filenames\n        sep (str): Seperator between elements\n        return_in_extent_coords (bool): Return in the coordinate frame of the extent\n\n    Returns:\n        tuple[list[Window], Window]: List of windows for each file and extent\n    \"\"\"\n    # Split the coords out, currently ignorign the filename as the first element\n    coords = [file.stem.split(sep)[1:] for file in files]\n\n    # Compute the extents as the min/max of the boxes\n    coords_array = np.array(coords).astype(int)\n\n    xmin = np.min(coords_array[:, 0])\n    ymin = np.min(coords_array[:, 1])\n    xmax = np.max(coords_array[:, 2] + coords_array[:, 0])\n    ymax = np.max(coords_array[:, 3] + coords_array[:, 1])\n    extent = Window(row_off=ymin, col_off=xmin, width=xmax - xmin, height=ymax - ymin)\n\n    if return_in_extent_coords:\n        # Subtract out x and y min so it's w.r.t. the extent coordinates\n        coords_array[:, 0] = coords_array[:, 0] - xmin\n        coords_array[:, 1] = coords_array[:, 1] - ymin\n\n    # Create windows from coords\n    windows = [\n        Window(\n            col_off=coord[0],\n            row_off=coord[1],\n            width=coord[2],\n            height=coord[3],\n        )\n        for coord in coords_array.astype(int)\n    ]\n\n    return windows, extent\n</code></pre>"},{"location":"API_reference/predictors/ortho_segmentor/#geograypher.predictors.ortho_segmentor.write_chips","title":"<code>write_chips(raster_file, output_folder, chip_size, chip_stride, label_vector_file=None, label_column=None, label_remap=None, write_empty_tiles=False, drop_transparency=True, remove_old=True, output_suffix='.JPG', ROI_file=None, background_ind=NULL_TEXTURE_INT_VALUE)</code>","text":"<p>Take raster data and tile it for machine learning training or inference</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>Path to the raster file to tile.</p> required <code>output_folder</code> <code>PATH_TYPE</code> <p>Where to write the tiled outputs.</p> required <code>chip_size</code> <code>int</code> <p>Size of the square chip in pixels.</p> required <code>chip_stride</code> <code>int</code> <p>The stride in pixels between sliding window tiles.</p> required <code>label_vector_file</code> <code>Optional[PATH_TYPE]</code> <p>A path to a vector geofile for the same region as the raster file. If provided, a parellel folder structure will be written to the chipped images that contains the corresponding rasterized data from the vector file. This is primarily useful for generating training data for ML. Defaults to None.</p> <code>None</code> <code>label_column</code> <code>Optional[str]</code> <p>Which column to use within the provided file. If not provided, the index will be used. Defaults to None.</p> <code>None</code> <code>label_remap</code> <code>Optional[dict]</code> <p>A dictionary mapping from the values in the <code>label_column</code> to integers that will be used for rasterization. Defaults to None.</p> <code>None</code> <code>write_empty_tiles</code> <code>bool</code> <p>Should tiles with no vector data be written. Defaults to False.</p> <code>False</code> <code>drop_transparency</code> <code>bool</code> <p>Should the forth channel be dropped if present. Defaults to True.</p> <code>True</code> <code>remove_old</code> <code>bool</code> <p>Remove <code>output_folder</code> if present. Defaults to True.</p> <code>True</code> <code>output_suffix</code> <code>str</code> <p>Suffix for written imagery files. Defaults to \".JPG\".</p> <code>'.JPG'</code> <code>ROI_file</code> <code>Optional[PATH_TYPE]</code> <p>Path to a geospatial region of interest to restrict tile generation to. Defaults to None.</p> <code>None</code> <code>background_ind</code> <code>int</code> <p>If labels are written, any un-labeled region will have this value. Defaults to <code>NULL_TEXTURE_INT_VALUE</code>.</p> <code>NULL_TEXTURE_INT_VALUE</code> Source code in <code>geograypher/predictors/ortho_segmentor.py</code> <pre><code>def write_chips(\n    raster_file: PATH_TYPE,\n    output_folder: PATH_TYPE,\n    chip_size: int,\n    chip_stride: int,\n    label_vector_file: Optional[PATH_TYPE] = None,\n    label_column: Optional[str] = None,\n    label_remap: Optional[dict] = None,\n    write_empty_tiles: bool = False,\n    drop_transparency: bool = True,\n    remove_old: bool = True,\n    output_suffix: str = \".JPG\",\n    ROI_file: Optional[PATH_TYPE] = None,\n    background_ind: int = NULL_TEXTURE_INT_VALUE,\n):\n    \"\"\"Take raster data and tile it for machine learning training or inference\n\n    Args:\n        raster_file (PATH_TYPE):\n            Path to the raster file to tile.\n        output_folder (PATH_TYPE):\n            Where to write the tiled outputs.\n        chip_size (int):\n            Size of the square chip in pixels.\n        chip_stride (int):\n            The stride in pixels between sliding window tiles.\n        label_vector_file (Optional[PATH_TYPE], optional):\n            A path to a vector geofile for the same region as the raster file. If provided, a\n            parellel folder structure will be written to the chipped images that contains the\n            corresponding rasterized data from the vector file. This is primarily useful for\n            generating training data for ML. Defaults to None.\n        label_column (Optional[str], optional):\n            Which column to use within the provided file. If not provided, the index will be used.\n            Defaults to None.\n        label_remap (Optional[dict], optional):\n            A dictionary mapping from the values in the `label_column` to integers that will be used\n            for rasterization. Defaults to None.\n        write_empty_tiles (bool, optional):\n            Should tiles with no vector data be written. Defaults to False.\n        drop_transparency (bool, optional):\n            Should the forth channel be dropped if present. Defaults to True.\n        remove_old (bool, optional):\n            Remove `output_folder` if present. Defaults to True.\n        output_suffix (str, optional):\n            Suffix for written imagery files. Defaults to \".JPG\".\n        ROI_file (Optional[PATH_TYPE], optional):\n            Path to a geospatial region of interest to restrict tile generation to. Defaults to None.\n        background_ind (int, optional):\n            If labels are written, any un-labeled region will have this value.\n            Defaults to `NULL_TEXTURE_INT_VALUE`.\n    \"\"\"\n    # Remove the existing directory\n    if remove_old and os.path.isdir(output_folder):\n        shutil.rmtree(output_folder)\n\n    # Read the labels if provided\n    if label_vector_file is not None:\n        label_gdf = gpd.read_file(label_vector_file)\n    else:\n        label_gdf = None\n\n    # Open the raster file\n    with rio.open(raster_file, \"r\") as dataset:\n        working_CRS = dataset.crs\n        # Create a list of windows for reading\n        windows = create_windows(\n            dataset_h_w=(dataset.height, dataset.width),\n            window_size=chip_size,\n            window_stride=chip_stride,\n        )\n\n        desc = f\"Writing image chips to {output_folder}\"\n        if label_gdf is not None:\n            desc = f\"Writing image chips and labels to {output_folder}\"\n            label_gdf.to_crs(working_CRS, inplace=True)\n\n            if label_column is not None:\n                label_values = label_gdf[label_column].tolist()\n            else:\n                label_values = label_gdf.index.tolist()\n\n            if label_remap is not None:\n                label_values = [label_remap[old_label] for old_label in label_values]\n\n            label_shapes = list(zip(label_gdf.geometry.values, label_values))\n            labels_folder = Path(output_folder, \"anns\")\n            output_folder = Path(output_folder, \"imgs\")\n\n            ensure_folder(labels_folder)\n        ensure_folder(output_folder)\n\n        # Set up the ROI now that we have the working CRS\n        if ROI_file is not None:\n            ROI_gdf = gpd.read_file(ROI_file).to_crs(working_CRS)\n            ROI_geometry = ROI_gdf.dissolve().geometry.values[0]\n            if label_gdf is not None:\n                # Crop the labels dataframe to the ROI\n                label_gdf = label_gdf.intersection(ROI_geometry)\n        else:\n            ROI_geometry = None\n\n        for window in tqdm(windows, desc=desc):\n            if ROI_geometry is not None:\n                window_transformer = AffineTransformer(dataset.window_transform(window))\n                pixel_corners = (\n                    (0, 0),\n                    (0, chip_size),\n                    (chip_size, chip_size),\n                    (chip_size, 0),\n                )\n                geospatial_corners = [\n                    window_transformer.xy(pc[0], pc[1], offset=\"ul\")\n                    for pc in pixel_corners\n                ]\n                geospatial_corners.append(geospatial_corners[0])\n                window_polygon = Polygon(geospatial_corners)\n\n                if not ROI_geometry.intersects(window_polygon):\n                    # Skip writing this chip if it doesn't intersect the ROI\n                    continue\n\n            if label_gdf is not None:\n                window_transform = dataset.window_transform(window)\n                window_transformer = AffineTransformer(window_transform)\n                labels_raster = rasterize(\n                    label_shapes,\n                    out_shape=(chip_size, chip_size),\n                    transform=window_transform,\n                    fill=background_ind,\n                )\n                labels_raster = labels_raster.astype(np.uint8)\n                # See if we should skip this tile since it's only background data\n                if not write_empty_tiles and np.all(\n                    labels_raster == NULL_TEXTURE_INT_VALUE\n                ):\n                    continue\n\n                # Write out the label\n                output_file_name = Path(\n                    labels_folder,\n                    get_str_from_window(\n                        raster_file=raster_file, window=window, suffix=\".png\"\n                    ),\n                )\n                imwrite(\n                    output_file_name,\n                    pad_to_full_size(labels_raster, (chip_size, chip_size)),\n                )\n\n            windowed_raster = dataset.read(window=window)\n            windowed_img = reshape_as_image(windowed_raster)\n\n            if drop_transparency and windowed_img.shape[2] == 4:\n                transparency = windowed_img[..., 3]\n                windowed_img = windowed_img[..., :3]\n                # Set transperent regions to black\n                mask = transparency == 0\n                if np.all(mask):\n                    continue\n\n                windowed_img[mask, :] = 0\n\n            output_file_name = Path(\n                output_folder,\n                get_str_from_window(\n                    raster_file=raster_file, window=window, suffix=output_suffix\n                ),\n            )\n            imwrite(\n                output_file_name,\n                pad_to_full_size(\n                    windowed_img,\n                    (chip_size, chip_size),\n                ),\n            )\n</code></pre>"},{"location":"API_reference/predictors/segmentor/","title":"Segmentor","text":""},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor","title":"<code>Segmentor</code>","text":"Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>class Segmentor:\n    def __init__(self, num_classes=None):\n        self.num_classes = num_classes\n\n    def setup(self, **kwargs) -&gt; None:\n        \"\"\"This is for things like loading a model. It's fine to not override it if there's no setup\"\"\"\n        pass\n\n    def segment_image(self, image: np.ndarray, **kwargs):\n        \"\"\"Produce a segmentation mask for an image\n\n        Args:\n            image (np.ndarray): np\n        \"\"\"\n        raise NotImplementedError(\"Abstract base class\")\n\n    def segment_image_batch(self, images: typing.List[np.ndarray], **kwargs):\n        \"\"\"\n        Segment a batch of images, to potentially use full compute capacity. The current implementation\n        should be overriden when there is a way to get improvements\n\n        Args:\n            images (typing.List[np.ndarray]): The list of images\n        \"\"\"\n        segmentations = []\n\n        for image in images:\n            segmentation = self.segment_image(image, **kwargs)\n            segmentations.append(segmentation)\n        return segmentations\n\n    @staticmethod\n    def inds_to_one_hot(\n        inds_image: np.ndarray,\n        num_classes: typing.Union[int, None] = None,\n        ignore_ind: int = 255,\n    ) -&gt; np.ndarray:\n        \"\"\"Convert an image of indices to a one-hot, one-per-channel encoding\n\n        Args:\n            inds_image (np.ndarray): Image of integer indices. (m, n)\n            num_classes (int, None): The number of classes. If None, computed as the max index provided. Default None\n            ignore_ind (inte, optional): This index is an ignored class\n\n        Returns:\n            np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False\n        \"\"\"\n        if num_classes is None:\n            inds_image_copy = inds_image.copy()\n            # Mask out ignore ind so it's not used in computation\n            inds_image_copy[inds_image_copy == ignore_ind] == 0\n            num_classes = np.max(inds_image_copy) + 1\n\n        one_hot_array = np.zeros(\n            (inds_image.shape[0], inds_image.shape[1], num_classes), dtype=bool\n        )\n        # Iterate up to max ind, not num_classes to avoid wasted computation when there won't be matches\n        for i in range(num_classes):\n            # TODO determine if there are any more efficient ways to do this\n            # Maybe create all these slices and then concatenate\n            # Or test equality with an array that has all the values in it\n            one_hot_array[..., i] = inds_image == i\n\n        return one_hot_array\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor-functions","title":"Functions","text":""},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.inds_to_one_hot","title":"<code>inds_to_one_hot(inds_image, num_classes=None, ignore_ind=255)</code>  <code>staticmethod</code>","text":"<p>Convert an image of indices to a one-hot, one-per-channel encoding</p> <p>Parameters:</p> Name Type Description Default <code>inds_image</code> <code>ndarray</code> <p>Image of integer indices. (m, n)</p> required <code>num_classes</code> <code>(int, None)</code> <p>The number of classes. If None, computed as the max index provided. Default None</p> <code>None</code> <code>ignore_ind</code> <code>inte</code> <p>This index is an ignored class</p> <code>255</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False</p> Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>@staticmethod\ndef inds_to_one_hot(\n    inds_image: np.ndarray,\n    num_classes: typing.Union[int, None] = None,\n    ignore_ind: int = 255,\n) -&gt; np.ndarray:\n    \"\"\"Convert an image of indices to a one-hot, one-per-channel encoding\n\n    Args:\n        inds_image (np.ndarray): Image of integer indices. (m, n)\n        num_classes (int, None): The number of classes. If None, computed as the max index provided. Default None\n        ignore_ind (inte, optional): This index is an ignored class\n\n    Returns:\n        np.ndarray: (m, n, num_classes) boolean array with one channel filled with a True, all else False\n    \"\"\"\n    if num_classes is None:\n        inds_image_copy = inds_image.copy()\n        # Mask out ignore ind so it's not used in computation\n        inds_image_copy[inds_image_copy == ignore_ind] == 0\n        num_classes = np.max(inds_image_copy) + 1\n\n    one_hot_array = np.zeros(\n        (inds_image.shape[0], inds_image.shape[1], num_classes), dtype=bool\n    )\n    # Iterate up to max ind, not num_classes to avoid wasted computation when there won't be matches\n    for i in range(num_classes):\n        # TODO determine if there are any more efficient ways to do this\n        # Maybe create all these slices and then concatenate\n        # Or test equality with an array that has all the values in it\n        one_hot_array[..., i] = inds_image == i\n\n    return one_hot_array\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.segment_image","title":"<code>segment_image(image, **kwargs)</code>","text":"<p>Produce a segmentation mask for an image</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>np</p> required Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def segment_image(self, image: np.ndarray, **kwargs):\n    \"\"\"Produce a segmentation mask for an image\n\n    Args:\n        image (np.ndarray): np\n    \"\"\"\n    raise NotImplementedError(\"Abstract base class\")\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.segment_image_batch","title":"<code>segment_image_batch(images, **kwargs)</code>","text":"<p>Segment a batch of images, to potentially use full compute capacity. The current implementation should be overriden when there is a way to get improvements</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[ndarray]</code> <p>The list of images</p> required Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def segment_image_batch(self, images: typing.List[np.ndarray], **kwargs):\n    \"\"\"\n    Segment a batch of images, to potentially use full compute capacity. The current implementation\n    should be overriden when there is a way to get improvements\n\n    Args:\n        images (typing.List[np.ndarray]): The list of images\n    \"\"\"\n    segmentations = []\n\n    for image in images:\n        segmentation = self.segment_image(image, **kwargs)\n        segmentations.append(segmentation)\n    return segmentations\n</code></pre>"},{"location":"API_reference/predictors/segmentor/#geograypher.predictors.segmentor.Segmentor.setup","title":"<code>setup(**kwargs)</code>","text":"<p>This is for things like loading a model. It's fine to not override it if there's no setup</p> Source code in <code>geograypher/predictors/segmentor.py</code> <pre><code>def setup(self, **kwargs) -&gt; None:\n    \"\"\"This is for things like loading a model. It's fine to not override it if there's no setup\"\"\"\n    pass\n</code></pre>"},{"location":"API_reference/utils/example_data/","title":"Example Data","text":""},{"location":"API_reference/utils/example_data/#geograypher.utils.example_data","title":"<code>example_data</code>","text":""},{"location":"API_reference/utils/files/","title":"Files","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files","title":"<code>files</code>","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files-functions","title":"Functions","text":""},{"location":"API_reference/utils/files/#geograypher.utils.files.ensure_containing_folder","title":"<code>ensure_containing_folder(filename)</code>","text":"<p>Ensure the folder containing this file exists. Nothing happens if already present.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>PATH_TYPE</code> <p>The path to the file for which the containing folder should be created</p> required Source code in <code>geograypher/utils/files.py</code> <pre><code>def ensure_containing_folder(filename: PATH_TYPE):\n    \"\"\"Ensure the folder containing this file exists. Nothing happens if already present.\n\n    Args:\n        filename (PATH_TYPE): The path to the file for which the containing folder should be created\n    \"\"\"\n    # Cast the file to a pathlib Path\n    filename = Path(filename)\n    # Get the folder above it\n    containing_folder = filename.parent\n    # Create this folder and all parent folders if needed. Nothing happens if it already exists\n    ensure_folder(containing_folder)\n</code></pre>"},{"location":"API_reference/utils/files/#geograypher.utils.files.ensure_folder","title":"<code>ensure_folder(folder)</code>","text":"<p>Ensure this folder, and parent folders, exist. Nothing happens if already present</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PATH_TYPE</code> <p>Path to folder to ensure exists</p> required Source code in <code>geograypher/utils/files.py</code> <pre><code>def ensure_folder(folder: PATH_TYPE):\n    \"\"\"Ensure this folder, and parent folders, exist. Nothing happens if already present\n\n    Args:\n        folder (PATH_TYPE): Path to folder to ensure exists\n    \"\"\"\n    folder = Path(folder)\n    # Create this folder and all parent folders if needed. Nothing happens if it already exists\n    folder.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"API_reference/utils/geometric/","title":"Geometric","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric","title":"<code>geometric</code>","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric-functions","title":"Functions","text":""},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.angle_between","title":"<code>angle_between(v1, v2)</code>","text":"<p>Returns the angle in radians between vectors 'v1' and 'v2'::</p> <p>angle_between((1, 0, 0), (0, 1, 0)) 1.5707963267948966 angle_between((1, 0, 0), (1, 0, 0)) 0.0 angle_between((1, 0, 0), (-1, 0, 0)) 3.141592653589793</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def angle_between(v1, v2):\n    \"\"\"Returns the angle in radians between vectors 'v1' and 'v2'::\n\n    &gt;&gt;&gt; angle_between((1, 0, 0), (0, 1, 0))\n    1.5707963267948966\n    &gt;&gt;&gt; angle_between((1, 0, 0), (1, 0, 0))\n    0.0\n    &gt;&gt;&gt; angle_between((1, 0, 0), (-1, 0, 0))\n    3.141592653589793\n    \"\"\"\n    v1_u = unit_vector(v1)\n    v2_u = unit_vector(v2)\n    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n</code></pre>"},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.batched_unary_union","title":"<code>batched_unary_union(geometries, batch_size, grid_size=None, subsequent_batch_size=4, sort_by_loc=False, simplify_tol=0, verbose=False)</code>","text":"<p>Roughly replicate the functionality of shapely.unary_union using a batched implementation</p> <p>Parameters:</p> Name Type Description Default <code>geometries</code> <code>List[Geometry]</code> <p>Geometries to aggregate</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the first aggregation</p> required <code>grid_size</code> <code>Union[None, float]</code> <p>grid size passed to unary_union</p> <code>None</code> <code>subsequent_batch_size</code> <code>int</code> <p>The batch size for subsequent (recursive) batches. Defaults to 4.</p> <code>4</code> <code>sort_by_loc</code> <code>bool</code> <p>Should the polygons be sorted by location to have a higher likelihood of merging. Defaults to False.</p> <code>False</code> <code>simplify_tol</code> <code>float</code> <p>How much to simplify in intermediate steps</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>Should additional print outs be provided</p> <code>False</code> <p>Returns:</p> Type Description <code>MultiPolygon</code> <p>shapely.MultiPolygon: The merged multipolygon</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def batched_unary_union(\n    geometries: typing.List[shapely.Geometry],\n    batch_size: int,\n    grid_size: typing.Union[None, float] = None,\n    subsequent_batch_size: int = 4,\n    sort_by_loc: bool = False,\n    simplify_tol: float = 0,\n    verbose: bool = False,\n) -&gt; shapely.MultiPolygon:\n    \"\"\"Roughly replicate the functionality of shapely.unary_union using a batched implementation\n\n    Args:\n        geometries (typing.List[shapely.Geometry]): Geometries to aggregate\n        batch_size (int): The batch size for the first aggregation\n        grid_size (typing.Union[None, float]): grid size passed to unary_union\n        subsequent_batch_size (int, optional): The batch size for subsequent (recursive) batches. Defaults to 4.\n        sort_by_loc (bool, optional): Should the polygons be sorted by location to have a higher likelihood of merging. Defaults to False.\n        simplify_tol (float, optional): How much to simplify in intermediate steps\n        verbose (bool, optional): Should additional print outs be provided\n\n    Returns:\n        shapely.MultiPolygon: The merged multipolygon\n    \"\"\"\n\n    # If the geoemtry is already one entry or empty\n    if len(geometries) &lt;= 1:\n        # Run unary_union to ensure that edge cases such as empty geometries are handled in the expected way\n        return unary_union(geometries)\n\n    # Sort the polygons by their least x coordinates (any bound could be used)\n    # The goal of this is to have a higher likelihood of grouping objects together and removing interior coordinates\n    if sort_by_loc and batch_size &lt; len(geometries):\n        logger.error(f\"Sorting the geometries with {len(geometries)} entries\")\n        geometries = sorted(geometries, key=lambda x: x.bounds[0])\n        logger.error(\"Done sorting geometries\")\n\n    # TODO you could consider requesting a give number of points in the batch,\n    # rather than number of objects.\n    # TODO you could consider multiprocessing this since it's embarassingly parallel\n\n    # Wrap the iteration in tqdm if requested, else just return it\n    iteration_decorator = lambda x: (\n        tqdm(x, desc=f\"Computing batched unary union with batch size {batch_size}\")\n        if verbose\n        else x\n    )\n\n    # Compute batched version\n    batched_unions = []\n    for i in iteration_decorator(\n        range(0, len(geometries), batch_size),\n    ):\n        batch = geometries[i : i + batch_size]\n        batched_unions.append(unary_union(batch, grid_size=grid_size))\n\n    # Simplify the geometry to reduce the number of points\n    # Don't do this if it would otherwise be returned as-is\n    if simplify_tol &gt; 0.0 and len(batched_unions) &gt; 1:\n        # Simplify then buffer, to make sure we don't have holes\n        logger.info(\n            f\"Lengths before simplification {[len(bu.geoms) for bu in batched_unions]}\"\n        )\n        batched_unions = [\n            simplify(bu, simplify_tol).buffer(simplify_tol)\n            for bu in tqdm(batched_unions, desc=\"simplifying polygons\")\n        ]\n        logger.info(\n            f\"Lengths after simplification {[len(bu.geoms) for bu in batched_unions]}\"\n        )\n    # Recurse this process until there's only one merged geometry\n    # All future calls will use the subsequent_batch_size\n    # TODO this batch size could be computed more inteligently, or sidestepped by requesting a number of points\n    # Don't sort because this should already be sorted\n    # Don't simplify because we don't want to repeatedly degrade the geometry\n    return batched_unary_union(\n        batched_unions,\n        batch_size=subsequent_batch_size,\n        grid_size=grid_size,\n        subsequent_batch_size=subsequent_batch_size,\n        sort_by_loc=False,\n        simplify_tol=0.0,\n    )\n</code></pre>"},{"location":"API_reference/utils/geometric/#geograypher.utils.geometric.unit_vector","title":"<code>unit_vector(vector)</code>","text":"<p>Returns the unit vector of the vector.</p> Source code in <code>geograypher/utils/geometric.py</code> <pre><code>def unit_vector(vector):\n    \"\"\"Returns the unit vector of the vector.\"\"\"\n    return vector / np.linalg.norm(vector)\n</code></pre>"},{"location":"API_reference/utils/geospatial/","title":"Geospatial","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial","title":"<code>geospatial</code>","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial-functions","title":"Functions","text":""},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.ensure_projected_CRS","title":"<code>ensure_projected_CRS(geodata)</code>","text":"<p>Returns a projected geodataframe from the provided geodataframe by converting it to ESPG:4326 (if not already) and determining the projected CRS from the point coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geodata</code> <code>GeoDataGrame</code> <p>Original geodataframe that is potentially unprojected</p> required <p>Returns:     gpd.GeoDataGrame: projected geodataframe</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def ensure_projected_CRS(geodata: gpd.GeoDataFrame):\n    \"\"\"Returns a projected geodataframe from the provided geodataframe by converting it to\n    ESPG:4326 (if not already) and determining the projected CRS from the point\n    coordinates.\n\n    Args:\n        geodata (gpd.GeoDataGrame): Original geodataframe that is potentially unprojected\n    Returns:\n        gpd.GeoDataGrame: projected geodataframe\n    \"\"\"\n    # If CRS is projected return immediately\n    if geodata.crs.is_projected:\n        return geodata\n\n    # If CRS is geographic and not long-lat, convert it to long-lat\n    if geodata.crs.is_geographic and geodata.crs != LAT_LON_CRS:\n        geodata = geodata.to_crs(LAT_LON_CRS)\n\n    # Convert geographic long-lat CRS to projected CRS\n    point = geodata[\"geometry\"][0].centroid\n    geometric_crs = get_projected_CRS(lon=point.x, lat=point.y)\n    return geodata.to_crs(geometric_crs)\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.get_overlap_raster","title":"<code>get_overlap_raster(unlabeled_df, classes_raster, num_classes=None, normalize=False)</code>","text":"<p>Get the overlap for each polygon in the unlabeled DF with each class in the raster</p> <p>Parameters:</p> Name Type Description Default <code>unlabeled_df</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Dataframe or path to dataframe containing geometries per object</p> required <code>classes_raster</code> <code>PATH_TYPE</code> <p>Path to a categorical raster</p> required <code>num_classes</code> <code>Union[None, int]</code> <p>Number of classes, if None defaults to the highest overlapping class. Defaults to None.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>Normalize counts matrix from pixels to fraction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid, n_classes) counts per polygon per class</p> <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def get_overlap_raster(\n    unlabeled_df: typing.Union[PATH_TYPE, GeoDataFrame],\n    classes_raster: PATH_TYPE,\n    num_classes: typing.Union[None, int] = None,\n    normalize: bool = False,\n) -&gt; (np.ndarray, np.ndarray):\n    \"\"\"Get the overlap for each polygon in the unlabeled DF with each class in the raster\n\n    Args:\n        unlabeled_df (typing.Union[PATH_TYPE, GeoDataFrame]):\n            Dataframe or path to dataframe containing geometries per object\n        classes_raster (PATH_TYPE): Path to a categorical raster\n        num_classes (typing.Union[None, int], optional):\n            Number of classes, if None defaults to the highest overlapping class. Defaults to None.\n        normalize (bool, optional): Normalize counts matrix from pixels to fraction. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_valid, n_classes) counts per polygon per class\n        np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions\n    \"\"\"\n    unlabeled_df = coerce_to_geoframe(unlabeled_df)\n\n    with rio.open(classes_raster, \"r\") as src:\n        raster_crs = src.crs\n        # Note this is a shapely object with no CRS, but we ensure\n        # the vectors are converted to the same CRS\n        raster_bounds = box(*src.bounds)\n\n    # Ensure that the vector data is in the same CRS as the raster\n    if raster_crs != unlabeled_df.crs:\n        # Avoid doing this in place because we don't want to modify the input dataframe\n        # This should properly create a copy\n        unlabeled_df = unlabeled_df.to_crs(raster_crs)\n\n    # Compute which polygons intersect the raster region\n    within_bounds_IDs = intersects_union_of_polygons(unlabeled_df, raster_bounds)\n\n    # Compute the stats\n    stats = zonal_stats(\n        unlabeled_df.iloc[within_bounds_IDs], str(classes_raster), categorical=True\n    )\n\n    # Find which polygons have non-null class predictions\n    # Due to nondata regions, some polygons within the region may not have class information\n    valid_prediction_IDs = np.where([x != {} for x in stats])[0]\n\n    # Determine the number of classes if not set\n    if num_classes is None:\n        # Find the max value that show up in valid predictions\n        num_classes = 1 + np.max(\n            [np.max(list(stats[i].keys())) for i in valid_prediction_IDs]\n        )\n\n    # Build the counts matrix for non-null predictions\n    counts_matrix = np.zeros((len(valid_prediction_IDs), num_classes))\n\n    # Fill the counts matrix\n    for i in valid_prediction_IDs:\n        for j, count in stats[i].items():\n            counts_matrix[i, j] = count\n\n    # Bookkeeping to find the IDs that were both within the raster and non-null\n    valid_IDs_in_original = within_bounds_IDs[valid_prediction_IDs]\n\n    if normalize:\n        counts_matrix = counts_matrix / np.sum(counts_matrix, axis=1, keepdims=True)\n\n    return counts_matrix, valid_IDs_in_original\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.get_overlap_vector","title":"<code>get_overlap_vector(unlabeled_df, classes_df, class_column, normalize=False)</code>","text":"<p>For each element in unlabeled df, return the fractional overlap with each class in classes_df</p> <p>Parameters:</p> Name Type Description Default <code>unlabeled_df</code> <code>GeoDataFrame</code> <p>A dataframe of geometries</p> required <code>classes_df</code> <code>GeoDataFrame</code> <p>A dataframe of classes</p> required <code>class_column</code> <code>str</code> <p>Which column in the classes_df to use. Defaults to \"names\".</p> required <code>normalize</code> <code>bool</code> <p>Normalize counts matrix from area to fraction. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid, n_classes) counts per polygon per class</p> <code>(ndarray, ndarray)</code> <p>np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def get_overlap_vector(\n    unlabeled_df: GeoDataFrame,\n    classes_df: GeoDataFrame,\n    class_column: str,\n    normalize: bool = False,\n) -&gt; (np.ndarray, np.ndarray):\n    \"\"\"\n    For each element in unlabeled df, return the fractional overlap with each class in\n    classes_df\n\n\n    Args:\n        unlabeled_df (GeoDataFrame): A dataframe of geometries\n        classes_df (GeoDataFrame): A dataframe of classes\n        class_column (str, optional): Which column in the classes_df to use. Defaults to \"names\".\n        normalize (bool, optional): Normalize counts matrix from area to fraction. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_valid, n_classes) counts per polygon per class\n        np.ndarray: (n_valid,) indices into the original array for polygons with non-null predictions\n    \"\"\"\n\n    ## Preprocessing\n    # Ensure that both a geodataframes\n    unlabeled_df = coerce_to_geoframe(unlabeled_df)\n    classes_df = coerce_to_geoframe(classes_df)\n\n    unlabeled_df = ensure_projected_CRS(unlabeled_df)\n    if classes_df.crs != unlabeled_df.crs:\n        classes_df = classes_df.to_crs(unlabeled_df.crs)\n\n    unlabeled_df.geometry = unlabeled_df.geometry.simplify(0.01)\n    classes_df.geometry = classes_df.geometry.simplify(0.01)\n\n    if class_column not in classes_df.columns:\n        raise ValueError(f\"Class column `{class_column}` not in {classes_df.columns}\")\n\n    logging.info(\n        \"Computing the intersection of the unlabeled polygons with the labeled region\"\n    )\n    # Find which unlabeled polygons intersect with the labeled region\n    intersection_IDs = intersects_union_of_polygons(unlabeled_df, classes_df)\n    logging.info(\"Finished computing intersection\")\n    # Extract only these polygons\n    unlabeled_df_intersecting_classes = unlabeled_df.iloc[intersection_IDs]\n    unlabeled_df_intersecting_classes[\"index\"] = unlabeled_df_intersecting_classes.index\n\n    # Add area field to each\n    unlabeled_df_intersecting_classes[\"unlabeled_area\"] = (\n        unlabeled_df_intersecting_classes.area\n    )\n\n    # Find the intersecting geometries\n    # We want only the ones that have some overlap with the unlabeled geometry, but I don't think that can be specified\n    logging.info(\"computing overlay\")\n    overlay = gpd.overlay(\n        classes_df,\n        unlabeled_df_intersecting_classes,\n        how=\"union\",\n        keep_geom_type=False,\n    )\n    # Drop the rows that only contain information from the class_labels\n    overlay = overlay.dropna(subset=\"index\")\n\n    # TODO look more into this part, something seems wrong\n    overlay[\"overlapping_area\"] = overlay.area\n    # overlay[\"per_class_area_fraction\"] = (\n    #    overlay[\"overlapping_area\"] / overlay[\"unlabeled_area\"]\n    # )\n    # Aggregating the results\n    # WARNING Make sure that this is a list and not a tuple or it gets considered one key\n    logging.info(\"computing groupby\")\n\n    # If the two dataframes have a column with the same name, they will be renamed\n    if (\n        f\"{class_column}_1\" in overlay.columns\n        and f\"{class_column}_2\" in overlay.columns\n    ):\n        aggregatation_class_column = f\"{class_column}_1\"\n    else:\n        aggregatation_class_column = class_column\n\n    # Groupby and aggregate\n    grouped_by = overlay.groupby(by=[\"index\", aggregatation_class_column])\n    aggregated = grouped_by.agg({\"overlapping_area\": \"sum\"})\n\n    # Extract the original class names\n    unique_class_names = sorted(classes_df[class_column].unique().tolist())\n    # And the indices from the original dataframe. This is relavent if the input\n    # dataframe was a subset of an original one\n    unique_index_values = sorted(unlabeled_df_intersecting_classes.index.tolist())\n\n    counts_matrix = np.zeros(\n        (len(unlabeled_df_intersecting_classes), len(unique_class_names))\n    )\n\n    for r in aggregated.iterrows():\n        (index, class_name), area = r\n        # The index is the index from the original unlabeled dataset, but we need the index into the subset\n        unlabled_object_ind = unique_index_values.index(index)\n        # Transform the class name into a class index\n        class_ind = unique_class_names.index(class_name)\n        # Set the value to the area of the overlap between that unlabeled object and given class\n        counts_matrix[unlabled_object_ind, class_ind] = float(area.iloc[0])\n\n    if normalize:\n        counts_matrix = counts_matrix / np.sum(counts_matrix, axis=1, keepdims=True)\n\n    return counts_matrix, intersection_IDs, unique_class_names\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.load_downsampled_raster_data","title":"<code>load_downsampled_raster_data(dataset_filename, downsample_factor)</code>","text":"<p>Load a raster file spatially downsampled</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>PATH_TYPE</code> <p>Path to the raster</p> required <code>downsample_factor</code> <code>float</code> <p>Downsample factor of 10 means that pixels are 10 times larger</p> required <p>Returns:</p> Type Description <p>np.array: The downsampled array in the rasterio (c, h, w) convention</p> <p>rio.DatasetReader: The reader with the transform updated</p> <p>rio.Transform: The updated transform</p> Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def load_downsampled_raster_data(dataset_filename: PATH_TYPE, downsample_factor: float):\n    \"\"\"Load a raster file spatially downsampled\n\n    Args:\n        dataset (PATH_TYPE): Path to the raster\n        downsample_factor (float): Downsample factor of 10 means that pixels are 10 times larger\n\n    Returns:\n        np.array: The downsampled array in the rasterio (c, h, w) convention\n        rio.DatasetReader: The reader with the transform updated\n        rio.Transform: The updated transform\n    \"\"\"\n    # Open the dataset handler. Note that this doesn't read into memory.\n    dataset = rio.open(dataset_filename)\n\n    # resample data to target shape\n    data = dataset.read(\n        out_shape=(\n            dataset.count,\n            int(dataset.height / downsample_factor),\n            int(dataset.width / downsample_factor),\n        ),\n        resampling=rio.enums.Resampling.bilinear,\n    )\n\n    # scale image transform\n    updated_transform = dataset.transform * dataset.transform.scale(\n        (dataset.width / data.shape[-1]), (dataset.height / data.shape[-2])\n    )\n    # Return the data and the transform\n    return data, dataset, updated_transform\n</code></pre>"},{"location":"API_reference/utils/geospatial/#geograypher.utils.geospatial.reproject_raster","title":"<code>reproject_raster(in_path, out_path, out_crs=pyproj.CRS.from_epsg(4326))</code>","text":"Source code in <code>geograypher/utils/geospatial.py</code> <pre><code>def reproject_raster(in_path, out_path, out_crs=pyproj.CRS.from_epsg(4326)):\n    \"\"\" \"\"\"\n    logging.warn(\"Starting to reproject raster\")\n    # reproject raster to project crs\n    with rio.open(in_path) as src:\n        src_crs = src.crs\n        transform, width, height = rio.warp.calculate_default_transform(\n            src_crs, out_crs, src.width, src.height, *src.bounds\n        )\n        kwargs = src.meta.copy()\n\n        kwargs.update(\n            {\"crs\": out_crs, \"transform\": transform, \"width\": width, \"height\": height}\n        )\n\n        with rio.open(out_path, \"w\", **kwargs) as dst:\n            for i in tqdm(range(1, src.count + 1), desc=\"Reprojecting bands\"):\n                rio.warp.reproject(\n                    source=rio.band(src, i),\n                    destination=rio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=out_crs,\n                    resampling=rio.warp.Resampling.nearest,\n                )\n    logging.warn(\"Done reprojecting raster\")\n</code></pre>"},{"location":"API_reference/utils/image/","title":"Image","text":""},{"location":"API_reference/utils/image/#geograypher.utils.image","title":"<code>image</code>","text":""},{"location":"API_reference/utils/indexing/","title":"Indexing","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing","title":"<code>indexing</code>","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing-functions","title":"Functions","text":""},{"location":"API_reference/utils/indexing/#geograypher.utils.indexing.find_argmax_nonzero_value","title":"<code>find_argmax_nonzero_value(array, keepdims=False, axis=1)</code>","text":"<p>Find the argmax of an array, setting entires with zero sum or finite values to nan</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The input array</p> required <code>keepdims</code> <code>bool</code> <p>Should the dimensions be kept. Defaults to False.</p> <code>False</code> <code>axis</code> <code>int</code> <p>Which axis to perform the argmax along. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>array</code> <p>np.array: The argmax, with nans for invalid or all-zero entries</p> Source code in <code>geograypher/utils/indexing.py</code> <pre><code>def find_argmax_nonzero_value(\n    array: np.ndarray, keepdims: bool = False, axis: int = 1\n) -&gt; np.array:\n    \"\"\"Find the argmax of an array, setting entires with zero sum or finite values to nan\n\n    Args:\n        array (np.ndarray): The input array\n        keepdims (bool, optional): Should the dimensions be kept. Defaults to False.\n        axis (int, optional): Which axis to perform the argmax along. Defaults to 1.\n\n    Returns:\n        np.array: The argmax, with nans for invalid or all-zero entries\n    \"\"\"\n    # Find the column with the highest value per row\n    argmax = np.argmax(array, axis=axis, keepdims=keepdims).astype(float)\n\n    # Find rows with zero sum or any infinite values\n    zero_sum_mask = np.sum(array, axis=axis) == 0\n    infinite_mask = np.any(~np.isfinite(array), axis=axis)\n\n    # Set these rows in the argmax to nan\n    argmax[np.logical_or(zero_sum_mask, infinite_mask)] = np.nan\n\n    return argmax\n</code></pre>"},{"location":"API_reference/utils/io/","title":"Io","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io","title":"<code>io</code>","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io-functions","title":"Functions","text":""},{"location":"API_reference/utils/io/#geograypher.utils.io.read_image_or_numpy","title":"<code>read_image_or_numpy(filename)</code>","text":"<p>Read in a file that's either an image or numpy array</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>PATH_TYPE</code> <p>Filename to be read</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If file cannot be read</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Image, in format present on disk</p> Source code in <code>geograypher/utils/io.py</code> <pre><code>def read_image_or_numpy(filename: PATH_TYPE) -&gt; np.ndarray:\n    \"\"\"Read in a file that's either an image or numpy array\n\n    Args:\n        filename (PATH_TYPE): Filename to be read\n\n    Raises:\n        ValueError: If file cannot be read\n\n    Returns:\n        np.ndarray: Image, in format present on disk\n    \"\"\"\n    img = None\n    if img is None:\n        try:\n            img = imread(filename)\n        except:\n            print(\"Couldn't read as image\")\n\n    if img is None:\n        try:\n            img = np.load(filename)\n        except:\n            print(\"couldn't read as numpy\")\n\n    if img is None:\n        raise ValueError(\"Could not read image\")\n\n    return img\n</code></pre>"},{"location":"API_reference/utils/numeric/","title":"Numeric","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric","title":"<code>numeric</code>","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric-functions","title":"Functions","text":""},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.compute_3D_triangle_area_vectorized","title":"<code>compute_3D_triangle_area_vectorized(corners, return_z_proj_area=True)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>corners</code> <code>ndarray</code> <p>(n_faces, n)</p> required <code>return_z_proj_area</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def compute_3D_triangle_area_vectorized(corners: np.ndarray, return_z_proj_area=True):\n    \"\"\"_summary_\n\n    Args:\n        corners (np.ndarray): (n_faces, n)\n        return_z_proj_area (bool, optional): _description_. Defaults to True.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    A, B, C = corners\n    # https://math.stackexchange.com/questions/2152754/calculate-3d-triangle-area-by-determinant\n    u = B - A\n    v = C - A\n\n    # Save for future computation\n    u0v1_min_u1v0 = u[0] * v[1] - u[1] * v[0]\n    area = (\n        1\n        / 2\n        * np.sqrt(\n            np.power(u[1] * v[2] - u[2] * v[1], 2)\n            + np.power(u[2] * v[0] - u[0] * v[2], 2)\n            + np.power(u0v1_min_u1v0, 2)\n        )\n    )\n\n    if return_z_proj_area:\n        area_z_proj = np.abs(u0v1_min_u1v0) / 2\n        return area, area_z_proj\n\n    return area\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.compute_approximate_ray_intersection","title":"<code>compute_approximate_ray_intersection(a0, a1, b0, b1, clamp=False, plotter=None)</code>","text":"<p>Given two line segments defined by 3D numpy.array pairs (a0, a1, b0, b1), return the closest points on each segment and their distance. If clamp is True, then respect the line segment ends. If clamp is False, then use the infinite rays.</p> <p>Based on https://stackoverflow.com/questions/2824478/shortest-distance-between-two-line-segments</p> <p>Parameters:</p> Name Type Description Default <code>a0</code> <code>ndarray</code> <p>Start point of the first segment (shape: (3,)).</p> required <code>a1</code> <code>ndarray</code> <p>End point of the first segment (shape: (3,)).</p> required <code>b0</code> <code>ndarray</code> <p>Start point of the second segment (shape: (3,)).</p> required <code>b1</code> <code>ndarray</code> <p>End point of the second segment (shape: (3,)).</p> required <code>clamp</code> <code>bool</code> <p>If True, the closest points are clamped to the segment endpoints. If False, the closest points may be anywhere along the infinite lines.</p> <code>False</code> <code>plotter</code> <code>Plotter</code> <p>If provided, visualizes the segments and closest points using pyvista.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pA</code> <code>ndarray</code> <p>Closest point on the first segment or line (shape: (3,)), or None if segments are parallel and overlap.</p> <code>pB</code> <code>ndarray</code> <p>Closest point on the second segment or line (shape: (3,)), or None if segments are parallel and overlap.</p> <code>dist</code> <code>float</code> <p>The minimum distance between the two segments or lines.</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def compute_approximate_ray_intersection(\n    a0: np.array,\n    a1: np.array,\n    b0: np.array,\n    b1: np.array,\n    clamp: bool = False,\n    plotter=None,\n):\n    \"\"\"\n    Given two line segments defined by 3D numpy.array pairs (a0, a1, b0, b1), return the\n    closest points on each segment and their distance. If clamp is True, then respect\n    the line segment ends. If clamp is False, then use the infinite rays.\n\n    Based on https://stackoverflow.com/questions/2824478/shortest-distance-between-two-line-segments\n\n    Args:\n        a0 (np.ndarray): Start point of the first segment (shape: (3,)).\n        a1 (np.ndarray): End point of the first segment (shape: (3,)).\n        b0 (np.ndarray): Start point of the second segment (shape: (3,)).\n        b1 (np.ndarray): End point of the second segment (shape: (3,)).\n        clamp (bool, optional): If True, the closest points are clamped to the segment\n            endpoints. If False, the closest points may be anywhere along the infinite\n            lines.\n        plotter (pyvista.Plotter, optional): If provided, visualizes the segments\n            and closest points using pyvista.\n\n    Returns:\n        pA (np.ndarray): Closest point on the first segment or line (shape: (3,)),\n            or None if segments are parallel and overlap.\n        pB (np.ndarray): Closest point on the second segment or line (shape: (3,)),\n            or None if segments are parallel and overlap.\n        dist (float): The minimum distance between the two segments or lines.\n    \"\"\"\n\n    # Calculate vectors, normalized vectors, and denominator\n    A = a1 - a0\n    B = b1 - b0\n    magA = np.linalg.norm(A)\n    magB = np.linalg.norm(B)\n    # Normalized vectors\n    _A = A / magA\n    _B = B / magB\n    cross = np.cross(_A, _B)\n    # Denom is the area of the parallelogram formed by the A and B unit vectors\n    # (norm of the cross product), squared. As that area goes to zero, it\n    # means that the unit vectors are aligned.\n    denom = np.linalg.norm(cross) ** 2\n\n    # If lines are parallel (denom=0) test if lines overlap. If they don't\n    # overlap then there is a closest point solution. If they do overlap,\n    # there are infinite closest positions, but there is a closest distance\n    if denom == 0:\n        d0 = np.dot(_A, (b0 - a0))\n\n        # Overlap only possible with clamping\n        if clamp:\n            d1 = np.dot(_A, (b1 - a0))\n\n            # Is segment B before A?\n            if (0 &gt;= d0) and (0 &gt;= d1):\n                if np.absolute(d0) &lt; np.absolute(d1):\n                    return a0, b0, np.linalg.norm(a0 - b0)\n                return a0, b1, np.linalg.norm(a0 - b1)\n\n            # Is segment B after A?\n            elif (magA &lt;= d0) and (magA &lt;= d1):\n                if np.absolute(d0) &lt; np.absolute(d1):\n                    return a1, b0, np.linalg.norm(a1 - b0)\n                return a1, b1, np.linalg.norm(a1 - b1)\n\n        # Segments overlap, return distance between parallel segments.\n        # Closest point is meaningless on parallel lines.\n        return None, None, np.linalg.norm((a0 + (d0 * _A)) - b0)\n\n    # Lines criss-cross: Calculate the projected closest points\n    t = b0 - a0\n    detA = np.linalg.det([t, _B, cross])\n    detB = np.linalg.det([t, _A, cross])\n\n    # Scale vectors that stretch along the A and B vectors. If\n    # the scale is between 0 and the magnitude of that vector\n    # then the projected point is within the line segment\n    t0 = detA / denom\n    t1 = detB / denom\n\n    # Projected closest point on rays A and B\n    pA = a0 + (_A * t0)\n    pB = b0 + (_B * t1)\n\n    # Clamp projections\n    if clamp:\n        if t0 &lt; 0:\n            pA = a0\n        elif t0 &gt; magA:\n            pA = a1\n\n        if t1 &lt; 0:\n            pB = b0\n        elif t1 &gt; magB:\n            pB = b1\n\n        # Clamp projection A\n        if (t0 &lt; 0) or (t0 &gt; magA):\n            dot = np.dot(_B, (pA - b0))\n            if dot &lt; 0:\n                dot = 0\n            elif dot &gt; magB:\n                dot = magB\n            pB = b0 + (_B * dot)\n\n        # Clamp projection B\n        if (t1 &lt; 0) or (t1 &gt; magB):\n            dot = np.dot(_A, (pB - a0))\n            if dot &lt; 0:\n                dot = 0\n            elif dot &gt; magA:\n                dot = magA\n            pA = a0 + (_A * dot)\n\n    if plotter is not None:\n        points = np.vstack([a0, pA, a1, b1, pB, b0])\n        plotter.add_lines(points)\n        plotter.add_points(points)\n        plotter.background_color = \"black\"\n        plotter.show()\n\n    return pA, pB, np.linalg.norm(pA - pB)\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.create_ramped_weighting","title":"<code>create_ramped_weighting(rectangle_shape, ramp_dist_frac)</code>","text":"<p>Create a ramped weighting that is higher toward the center with a max value of 1 at a fraction from the edge</p> <p>Parameters:</p> Name Type Description Default <code>rectangle_shape</code> <code>Tuple[int, int]</code> <p>Size of rectangle to create a mask for</p> required <code>ramp_dist_frac</code> <code>float</code> <p>Portions at least this far from an edge will have full weight</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array representing the weighting from 0-1</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def create_ramped_weighting(\n    rectangle_shape: typing.Tuple[int, int], ramp_dist_frac: float\n) -&gt; np.ndarray:\n    \"\"\"Create a ramped weighting that is higher toward the center with a max value of 1 at a fraction from the edge\n\n    Args:\n        rectangle_shape (typing.Tuple[int, int]): Size of rectangle to create a mask for\n        ramp_dist_frac (float): Portions at least this far from an edge will have full weight\n\n    Returns:\n        np.ndarray: An array representing the weighting from 0-1\n    \"\"\"\n    i_ramp = np.clip(np.linspace(0, 1 / ramp_dist_frac, num=rectangle_shape[0]), 0, 1)\n    j_ramp = np.clip(np.linspace(0, 1 / ramp_dist_frac, num=rectangle_shape[1]), 0, 1)\n\n    i_ramp = np.minimum(i_ramp, np.flip(i_ramp))\n    j_ramp = np.minimum(j_ramp, np.flip(j_ramp))\n\n    i_ramp = np.expand_dims(i_ramp, 1)\n    j_ramp = np.expand_dims(j_ramp, 0)\n\n    ramped_weighting = np.minimum(i_ramp, j_ramp)\n    return ramped_weighting\n</code></pre>"},{"location":"API_reference/utils/numeric/#geograypher.utils.numeric.intersection_average","title":"<code>intersection_average(starts, ends)</code>","text":"<p>Given arrays of line segment start and end points, compute the average of the closest intersection points between all pairs of segments.</p> <p>Parameters:</p> Name Type Description Default <code>starts</code> <code>ndarray</code> <p>(N, 3) array of segment start points</p> required <code>ends</code> <code>ndarray</code> <p>(N, 3) array of segment end points</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (3,) array, the average intersection point</p> Source code in <code>geograypher/utils/numeric.py</code> <pre><code>def intersection_average(starts: np.ndarray, ends: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Given arrays of line segment start and end points, compute the average of the closest\n    intersection points between all pairs of segments.\n\n    Args:\n        starts (np.ndarray): (N, 3) array of segment start points\n        ends (np.ndarray): (N, 3) array of segment end points\n\n    Returns:\n        np.ndarray: (3,) array, the average intersection point\n    \"\"\"\n    N = starts.shape[0]\n    closest_points = []\n    for i in range(N):\n        for j in range(i + 1, N):\n            a0, a1 = starts[i], ends[i]\n            b0, b1 = starts[j], ends[j]\n            pA, pB, _ = compute_approximate_ray_intersection(a0, a1, b0, b1, clamp=True)\n            if pA is not None and pB is not None:\n                closest_points.append(pA)\n                closest_points.append(pB)\n    if len(closest_points) &gt; 0:\n        return np.mean(np.stack(closest_points, axis=0), axis=0)\n    else:\n        # If all are None, return the average of all start and end points\n        all_points = np.concatenate([starts, ends], axis=0)\n        return np.mean(all_points, axis=0)\n</code></pre>"},{"location":"API_reference/utils/parsing/","title":"Parsing","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing","title":"<code>parsing</code>","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing-functions","title":"Functions","text":""},{"location":"API_reference/utils/parsing/#geograypher.utils.parsing.make_4x4_transform","title":"<code>make_4x4_transform(rotation_str, translation_str, scale_str='1')</code>","text":"<p>Convenience function to make a 4x4 matrix from the string format used by Metashape</p> <p>Parameters:</p> Name Type Description Default <code>rotation_str</code> <code>str</code> <p>Row major with 9 entries</p> required <code>translation_str</code> <code>str</code> <p>3 entries</p> required <code>scale_str</code> <code>str</code> <p>single value. Defaults to \"1\".</p> <code>'1'</code> <p>Returns:</p> Type Description <p>np.ndarray: (4, 4) A homogenous transform mapping from cam to world</p> Source code in <code>geograypher/utils/parsing.py</code> <pre><code>def make_4x4_transform(rotation_str: str, translation_str: str, scale_str: str = \"1\"):\n    \"\"\"Convenience function to make a 4x4 matrix from the string format used by Metashape\n\n    Args:\n        rotation_str (str): Row major with 9 entries\n        translation_str (str): 3 entries\n        scale_str (str, optional): single value. Defaults to \"1\".\n\n    Returns:\n        np.ndarray: (4, 4) A homogenous transform mapping from cam to world\n    \"\"\"\n    rotation_np = np.fromstring(rotation_str, sep=\" \")\n    rotation_np = np.reshape(rotation_np, (3, 3))\n\n    if not np.isclose(np.linalg.det(rotation_np), 1.0, atol=1e-8, rtol=0):\n        raise ValueError(\n            f\"Inproper rotation matrix with determinant {np.linalg.det(rotation_np)}\"\n        )\n\n    translation_np = np.fromstring(translation_str, sep=\" \")\n    scale = float(scale_str)\n    transform = np.eye(4)\n    transform[:3, :3] = rotation_np * scale\n    transform[:3, 3] = translation_np\n    return transform\n</code></pre>"},{"location":"API_reference/utils/prediction_metrics/","title":"Prediction metrics","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics","title":"<code>prediction_metrics</code>","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics-functions","title":"Functions","text":""},{"location":"API_reference/utils/prediction_metrics/#geograypher.utils.prediction_metrics.compute_and_show_cf","title":"<code>compute_and_show_cf(pred_labels, gt_labels, labels=None, use_labels_from='both', vis=True, cf_plot_savefile=None, cf_np_savefile=None)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>pred_labels</code> <code>list</code> <p>description</p> required <code>gt_labels</code> <code>list</code> <p>description</p> required <code>labels</code> <code>Union[None, List[str]]</code> <p>description. Defaults to None.</p> <code>None</code> <code>use_labels_from</code> <code>str</code> <p>description. Defaults to \"both\".</p> <code>'both'</code> <code>vis</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <code>cf_plot_savefile</code> <code>Union[None, PATH_TYPE]</code> <p>description. Defaults to None.</p> <code>None</code> <code>cf_np_savefile</code> <code>Union[None, PATH_TYPE]</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>geograypher/utils/prediction_metrics.py</code> <pre><code>def compute_and_show_cf(\n    pred_labels: list,\n    gt_labels: list,\n    labels: typing.Union[None, typing.List[str]] = None,\n    use_labels_from: str = \"both\",\n    vis: bool = True,\n    cf_plot_savefile: typing.Union[None, PATH_TYPE] = None,\n    cf_np_savefile: typing.Union[None, PATH_TYPE] = None,\n):\n    \"\"\"_summary_\n\n    Args:\n        pred_labels (list): _description_\n        gt_labels (list): _description_\n        labels (typing.Union[None, typing.List[str]], optional): _description_. Defaults to None.\n        use_labels_from (str, optional): _description_. Defaults to \"both\".\n        vis (bool, optional): _description_. Defaults to True.\n        cf_plot_savefile (typing.Union[None, PATH_TYPE], optional): _description_. Defaults to None.\n        cf_np_savefile (typing.Union[None, PATH_TYPE], optional): _description_. Defaults to None.\n\n    Raises:\n        ValueError: _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    if labels is None:\n        if use_labels_from == \"gt\":\n            labels = np.unique(list(gt_labels))\n        elif use_labels_from == \"pred\":\n            labels = np.unique(list(pred_labels))\n        elif use_labels_from == \"both\":\n            labels = np.unique(list(pred_labels) + list(gt_labels))\n        else:\n            raise ValueError(\n                f\"Must use labels from gt, pred, or both but instead was {use_labels_from}\"\n            )\n\n    cf_matrix = confusion_matrix(y_true=gt_labels, y_pred=pred_labels, labels=labels)\n\n    if vis:\n        cf_disp = ConfusionMatrixDisplay(\n            confusion_matrix=cf_matrix, display_labels=labels\n        )\n        cf_disp.plot()\n        if cf_plot_savefile is None:\n            plt.show()\n        else:\n            ensure_containing_folder(cf_plot_savefile)\n            plt.savefig(cf_plot_savefile)\n\n    if cf_np_savefile:\n        ensure_containing_folder(cf_np_savefile)\n        np.save(cf_np_savefile, cf_matrix)\n\n    # TODO compute more comprehensive metrics here\n    accuracy = np.sum(cf_matrix * np.eye(cf_matrix.shape[0])) / np.sum(cf_matrix)\n\n    return cf_matrix, labels, accuracy\n</code></pre>"},{"location":"API_reference/utils/visualization/","title":"Visualization","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization","title":"<code>visualization</code>","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization-functions","title":"Functions","text":""},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.create_composite","title":"<code>create_composite(RGB_image, label_image, label_blending_weight=0.5, IDs_to_labels=None, grayscale_RGB_overlay=True)</code>","text":"<p>Create a three-panel composite with an RGB image and a label</p> <p>Parameters:</p> Name Type Description Default <code>RGB_image</code> <code>ndarray</code> <p>(h, w, 3) rgb image to be used directly as one panel</p> required <code>label_image</code> <code>ndarray</code> <p>(h, w) image containing either integer labels or float scalars. Will be colormapped prior to display.</p> required <code>label_blending_weight</code> <code>float</code> <p>Opacity for the label in the blended composite. Defaults to 0.5.</p> <code>0.5</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping from integer IDs to string labels. Used to compute colormap. If None, a continous colormap is used. Defaults to None.</p> <code>None</code> <code>grayscale_RGB_overlay</code> <code>bool</code> <p>Convert the RGB image to grayscale in the overlay. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the RGB image cannot be interpreted as such</p> <p>Returns:</p> Type Description <p>np.ndarray: (h, 3*w, 3) horizontally composited image</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def create_composite(\n    RGB_image: np.ndarray,\n    label_image: np.ndarray,\n    label_blending_weight: float = 0.5,\n    IDs_to_labels: typing.Union[None, dict] = None,\n    grayscale_RGB_overlay: bool = True,\n):\n    \"\"\"Create a three-panel composite with an RGB image and a label\n\n    Args:\n        RGB_image (np.ndarray):\n            (h, w, 3) rgb image to be used directly as one panel\n        label_image (np.ndarray):\n            (h, w) image containing either integer labels or float scalars. Will be colormapped\n            prior to display.\n        label_blending_weight (float, optional):\n            Opacity for the label in the blended composite. Defaults to 0.5.\n        IDs_to_labels (typing.Union[None, dict], optional):\n            Mapping from integer IDs to string labels. Used to compute colormap. If None, a\n            continous colormap is used. Defaults to None.\n        grayscale_RGB_overlay (bool):\n            Convert the RGB image to grayscale in the overlay. Default is True.\n\n    Raises:\n        ValueError: If the RGB image cannot be interpreted as such\n\n    Returns:\n        np.ndarray: (h, 3*w, 3) horizontally composited image\n    \"\"\"\n    if RGB_image.ndim != 3 or RGB_image.shape[2] != 3:\n        raise ValueError(\"Invalid RGB error\")\n\n    if RGB_image.dtype == np.uint8:\n        # Rescale to float range and implicitly cast\n        RGB_image = RGB_image / 255\n\n    if label_image.dtype == np.uint8:\n        # Rescale to float range and implicitly cast\n        label_image = label_image / 255\n\n    if not (label_image.ndim == 3 and label_image.shape[2] == 3):\n        # If it's a one channel image make it not have a channel dim\n        label_image = np.squeeze(label_image)\n\n        vis_options = get_vis_options_from_IDs_to_labels(IDs_to_labels)\n        cmap = plt.get_cmap(vis_options[\"cmap\"])\n        null_mask = np.logical_or(\n            label_image == NULL_TEXTURE_INT_VALUE,\n            np.logical_not(np.isfinite(label_image)),\n        )\n        if not vis_options[\"discrete\"]:\n            # Shift\n            label_image = label_image - np.nanmin(label_image)\n            # Find the max value that's not the null vlaue\n            valid_pixels = label_image[np.logical_not(null_mask)]\n            if valid_pixels.size &gt; 0:\n                # TODO this might have to be changed to nanmax in the future\n                max_value = np.max(valid_pixels)\n                # Scale\n                label_image = label_image / max_value\n        else:\n            # Convert it to an int so it's used to directly index the colormap\n            label_image = label_image.astype(np.uint8)\n\n        # Perform the colormapping\n        label_image = cmap(label_image)[..., :3]\n        # Mask invalid values\n        label_image[null_mask] = 0\n\n    # Create a blended image\n    if grayscale_RGB_overlay:\n        RGB_for_composite = np.tile(\n            np.mean(RGB_image, axis=2, keepdims=True), (1, 1, 3)\n        )\n    else:\n        RGB_for_composite = RGB_image\n    overlay = ((1 - label_blending_weight) * RGB_for_composite) + (\n        label_blending_weight * label_image\n    )\n    # Concatenate the images horizonally\n    composite = np.concatenate((label_image, RGB_image, overlay), axis=1)\n    # Cast to np.uint8 for saving\n    composite = (composite * 255).astype(np.uint8)\n    return composite\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.create_pv_plotter","title":"<code>create_pv_plotter(off_screen, force_xvfb=False, plotter=None)</code>","text":"<p>Create a pyvista plotter while handling offscreen rendering</p> <p>Parameters:</p> Name Type Description Default <code>off_screen</code> <code>bool</code> <p>Whether the plotter should be offscreen</p> required <code>force_xvfb</code> <code>bool</code> <p>Should XVFB be used for rendering by default. Defaults to False.</p> <code>False</code> <code>plotter</code> <code>None, pv.Plotter)</code> <p>Existing plotter to use, will just return it if not None. Defaults to None</p> <code>None</code> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def create_pv_plotter(\n    off_screen: bool,\n    force_xvfb: bool = False,\n    plotter: typing.Union[None, pv.Plotter] = None,\n):\n    \"\"\"Create a pyvista plotter while handling offscreen rendering\n\n    Args:\n        off_screen (bool):\n            Whether the plotter should be offscreen\n        force_xvfb (bool, optional):\n            Should XVFB be used for rendering by default. Defaults to False.\n        plotter ((None, pv.Plotter), optional):\n            Existing plotter to use, will just return it if not None. Defaults to None\n    \"\"\"\n    # If a valid plotter has not been passed in create one\n    if not isinstance(plotter, pv.Plotter):\n        # Catch the warning that there is not xserver running\n        with warnings.catch_warnings(record=True) as w:\n            # Create the plotter which may be onscreen or off\n            plotter = pv.Plotter(off_screen=off_screen)\n\n        # Start xvfb if requested or the system is not running an xserver\n        if force_xvfb or (len(w) &gt; 0 and \"pyvista.start_xvfb()\" in str(w[0].message)):\n            # Start a headless renderer\n            safe_start_xvfb()\n    return plotter\n</code></pre>"},{"location":"API_reference/utils/visualization/#geograypher.utils.visualization.get_vis_options_from_IDs_to_labels","title":"<code>get_vis_options_from_IDs_to_labels(IDs_to_labels, cmap_continous='viridis', cmap_10_classes='tab10', cmap_20_classes='tab20', cmap_many_classes='viridis')</code>","text":"<p>Determine vis options based on a given IDs_to_labels object</p> <p>Parameters:</p> Name Type Description Default <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>description</p> required <code>cmap_continous</code> <code>str</code> <p>Colormap to use if the values are continous. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>cmap_10_classes</code> <code>str</code> <p>Colormap to use if the values are discrete and there are 10 or fewer classes. Defaults to \"tab10\".</p> <code>'tab10'</code> <code>cmap_20_classes</code> <code>str</code> <p>Colormap to use if the values are discrete and there are 11-20 classes. Defaults to \"tab20\".</p> <code>'tab20'</code> <code>cmap_many_classes</code> <code>str</code> <p>Colormap to use if there are more than 20 classes. Defaults to \"viridis\".</p> <code>'viridis'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Containing the cmap, vmin/vmax, and whether the colormap is discrete</p> Source code in <code>geograypher/utils/visualization.py</code> <pre><code>def get_vis_options_from_IDs_to_labels(\n    IDs_to_labels: typing.Union[None, dict],\n    cmap_continous: str = \"viridis\",\n    cmap_10_classes: str = \"tab10\",\n    cmap_20_classes: str = \"tab20\",\n    cmap_many_classes: str = \"viridis\",\n):\n    \"\"\"Determine vis options based on a given IDs_to_labels object\n\n    Args:\n        IDs_to_labels (typing.Union[None, dict]): _description_\n        cmap_continous (str, optional):\n            Colormap to use if the values are continous. Defaults to \"viridis\".\n        cmap_10_classes (str, optional):\n            Colormap to use if the values are discrete and there are 10 or fewer classes. Defaults to \"tab10\".\n        cmap_20_classes (str, optional):\n            Colormap to use if the values are discrete and there are 11-20 classes. Defaults to \"tab20\".\n        cmap_many_classes (str, optional):\n            Colormap to use if there are more than 20 classes. Defaults to \"viridis\".\n\n    Returns:\n        dict: Containing the cmap, vmin/vmax, and whether the colormap is discrete\n    \"\"\"\n    # This could be written in fewer lines of code but I kept it intentionally explicit\n\n    if IDs_to_labels is None:\n        # No IDs_to_labels means it's continous\n        cmap = cmap_continous\n        vmin = None\n        vmax = None\n        discrete = False\n    else:\n        # Otherwise, we can determine the max class ID\n        max_ID = np.max(list(IDs_to_labels.keys()))\n\n        if max_ID &lt; 10:\n            # 10 or fewer discrete classes\n            cmap = cmap_10_classes\n            vmin = -0.5\n            vmax = 9.5\n            discrete = True\n        elif max_ID &lt; 20:\n            # 11-20 discrete classes\n            cmap = cmap_20_classes\n            vmin = -0.5\n            vmax = 19.5\n            discrete = True\n        else:\n            # More than 20 classes. There are no good discrete colormaps for this, so we generally\n            # fall back on displaying it with a continous colormap\n            cmap = cmap_many_classes\n            vmin = None\n            vmax = None\n            discrete = False\n\n    return {\"cmap\": cmap, \"vmin\": vmin, \"vmax\": vmax, \"discrete\": discrete}\n</code></pre>"},{"location":"examples_and_applications/applications/","title":"Projects using geograypher","text":""},{"location":"examples_and_applications/applications/#cross-site-tree-species-classification-for-western-conifers","title":"Cross-site tree species classification for Western Conifers","text":"<p>The goal of this project is to identify the species of western conifers in regions which have been burned by severe fire in the past decade. Data was collected at four sites and consisted of both drone surveys and manual field surveys. This work showed geograypher's multiview workflow enabled 75% prediction accuracy on a leave-on-site-out prediction task. Details can be found in this ArXiv paper.</p>"},{"location":"examples_and_applications/examples/","title":"Examples","text":""},{"location":"examples_and_applications/examples/#projecting-image-based-segmentation-predictions-to-geospatial-coordinates","title":"Projecting image-based segmentation predictions to geospatial coordinates","text":"<p>For tasks like vegetation cover segmentation, it is common to use a semantic segmentation network that produces a class label for each pixel in an image. This workflow shows how to take these per-image predictions and project them to a 3D mesh representation. From there, the most commonly-predicted class is identified for each face of the mesh. Then, this information can be exported in a geospatial format showing the boundaries of each class. This information can also be post-processed to determine the most common class for a pre-determined region, such as a single tree or a management region. This workflow is fairly well-developed and works for a variety of applications.</p> <ul> <li>geograypher/examples/aggregate_predictions.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#projecting-object-detections-to-geospatial-coordinates","title":"Projecting object detections to geospatial coordinates","text":"<p>Other tasks consist of identifying individual objects such as trees or birds. Given object detections in individual images, this workflow can determine the corresponding geospatial boundaries. The quality of this approach is heavily dependent on the quality of the mesh model of the scene. In cases where the scale of the reconstruction errors is significant compared to the size of individual objects, the localization may be poor. An alternative workflow is to triangulate multiple detections from different images to localize the objects without using a mesh. This works in some cases, but often under-predicts the true number of unique objects. Both workflows are highly experimental.</p> <ul> <li>geograypher/examples/project_detections.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#generating-per-image-labels-from-geospatial-ground-truth","title":"Generating per-image labels from geospatial ground truth","text":"<p>The prior examples have assumed that per-image predictions are available. In many practical applications, the user must train their own machine learning model to generate these predictions. Since the predictions are generated on individual raw images, it is important that the labeled data to train the model also consists of individual images, and not another representation such as an orthomosaic. The user can hand-annotate individual images, but this process is laborious and ecological classes (e.g. plant species) cannot always be reliably determined without in-field observations. This workflow takes geospatial ground truth data collected in-field and projects it to the viewpoint of each individual image. These per-pixel labels corresponding to each real image can be used to train a machine learning model.</p> <ul> <li>geograypher/examples/render_labels.ipynb</li> </ul>"},{"location":"examples_and_applications/examples/#workflow-using-simulated-data","title":"Workflow using simulated data","text":"<p>All the other examples use real data. To support conceptual understanding and rapid debugging, we developed an end-to-end workflow using only simulated data. The scene consists of various geometric objects arranged on a flat plane. Users can configure the objects in the scene and the locations of the virtual cameras observing them.</p> <ul> <li>geograypher/examples/concept_figure.ipynb</li> </ul>"},{"location":"getting_started/conceptual_workflow/","title":"Conceptual workflow","text":"<p>Imagine you are trying to map the objects in a hypothetical region. Your world consists of three types of objects: cones, cubes, and cylinders. Cones are different shades of blue, cubes are difference shades of orange, and cylinders are different shades of green. Your landscape consists of a variety of these objects arranged randomly on a flat gray surface. You fly a drone survey and collect images of your scene, some of which are shown below.</p> <p> </p> <p>While you are there, you also do some field work and survey a small subset of your region. Field work is labor-intensive, so you can't survey the entire region your drone flew. You note down the class of the object and their location and shape in geospatial coordinates. This results in the following geospatial map.</p> <p> </p> <p>You use structure from motion to build a 3D model of your scene and also estimate the locations that each image was taken from.</p> <p> </p> <p>Up to this point, you have been following a fairly standard workflow. A common practice at this point would be to generate a top-down, 2D orthomosaic of the scene and do any prediction tasks, such as deep learning model training or inference, using this data. Instead, you decide it's important to maintain the high quality of the raw images and be able to see the sides of your objects when you are generating predictions. This is where geograypher comes in.</p> <p>Using your field reference map and the 3D model from photogrammetry, you determine which portions of your 3D scene correspond to each object. This is shown below, with the colors now representing the classification label.</p> <p> </p> <p>Your end goal is to generate predictions on the entire region. For this, you need a machine learning model that can generate automatic predictions on your data. No one else has developed a model for your cone-cube-cylinder classification task, so you need to train your own using labeled example data. Using the mesh that is textured with the classification information from the field survey, and the pose of the camera, you can \"render\" the labels onto the images. They are shown below, color-coded by class.</p> <p> </p> <p>These labels correspond to the images shown below.</p> <p> </p> <p>Now that you have pairs of real images and rendered labels, you can train a machine learning model to predict the class of the objects from the images. This model can be now used to generate predictions on un-labeled images. An example prediction is shown below.</p> <p> </p> <p>To make these predictions useful, you need the information in geospatial coordinates. We again use the mesh model as an intermediate step between the image coordinates and 2D geospatial coordinates. The predictions are projected or \"splatted\" onto the mesh from each viewpoint.</p> <p> </p> <p> </p> <p>As seen above, each prediction only captures a small region of the mesh, and cannot make any predictions about parts of the object that were occluded in the original viewpoint. Therefore, we need to aggregate the predictions from all viewpoints to have an understanding of the entire scene. This gives us added robustness, because we can tolerate some prediction errors for a single viewpoint, by choosing the most common prediction across all viewpoints of a single location. The aggregated prediction is shown below.</p> <p> </p> <p>Now, the final step is to transform these predictions back into geospatial coordinates.</p> <p> </p>"},{"location":"getting_started/data/","title":"Data","text":""},{"location":"getting_started/data/#example-data","title":"Example data","text":"<p>The public example data is in <code>data/example_Emerald_Point_data</code> . You can run notebooks in the <code>examples</code> folder to see how to interact with this data. You can download this data using Google Drive from this folder. Once you've downloaded it, extract it into the <code>data</code> folder.</p>"},{"location":"getting_started/data/#using-your-own-data","title":"Using your own data","text":"<p>If you have a Metashape scene with the location of cameras, a mesh, and geospatial information, you can likely use geograypher. If you are using the Metashape GUI, you must do an important step before exporting the mesh model. Metashape stores the mesh in an arbitrary coordinate system that's optimized for viewing and will export it as such. To fix this, in the Metashape GUI you need to do <code>Model-&gt;Transform Object-&gt;Reset Transform</code> , then save the mesh with the local coordinates option. The cameras can be exported without any special considerations.</p> <p>You can also use our scripted workflow for running Metashape, automate-metashape. The cameras and the <code>local</code> mesh export will be properly formatted for use with geograypher.</p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>If you only need to use the exisiting functionality of Geograypher and not make changes to the toolkit code or dependencies, follow the <code>Basic Installation</code> instructions. </p> <p>If you want to do development work, please see <code>Advanced/Developer Installation</code>.</p> <p>Internal collaborators please navigate here for more instructions. </p>"},{"location":"getting_started/installation/#basic-installation","title":"Basic Installation","text":"<p>Create and activate a conda environment:</p> <pre><code>conda create -n geograypher python=3.9 -y\nconda activate geograypher\n</code></pre> <p>Install Geograypher: <pre><code>pip install geograypher\n</code></pre></p>"},{"location":"getting_started/installation/#advanceddeveloper-installation","title":"Advanced/Developer Installation","text":"<p>Create and activate a conda environment:</p> <pre><code>conda create -n geograypher python=3.9 -y\nconda activate geograypher\n</code></pre> <p>Install poetry:</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Now use this to install the majority of dependencies. First cd to the directory containing the <code>geograypher</code> repo. Then run:</p> <pre><code>poetry install\n</code></pre> <p>You may get the following error when running <code>pyvista</code> visualization:</p> <pre><code>libGL error: MESA-LOADER: failed to open swrast: &lt;CONDA ENV LOCATION&gt;/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /lib/x86_64-linux-gnu/libLLVM-15.so.1) (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n</code></pre> <p>If this happens, you can fix it by symlinking to the system version. I don't know why this is required.</p> <pre><code>ln -sf /usr/lib/x86_64-linux-gnu/libstdc++.so.6 &lt;CONDA ENV LOCATION&gt;/lib/libstdc++.so.6\n</code></pre>"},{"location":"getting_started/installation/#working-on-headless-machine","title":"Working on Headless Machine","text":"<p>If you are working on a headless machine, such as a remote server, you will need the XVFB package to provide a virtual frame buffer. This can be installed at the system level using the package manager, for example: <pre><code>sudo apt install xvfb\n</code></pre> If you do not have root access on your machine, it may not be possible to install xvfb.</p>"},{"location":"getting_started/installation/#optional-install-pytorch3d","title":"Optional: Install <code>pytorch3d</code>","text":"<p>If you are working on a headless machine and are unable to install xvfb, <code>pytorch3d</code> can be a viable alternative since installing it does not require admin privileges.</p> <p>Install the pytorch3d dependencies:</p> <pre><code>conda install pytorch=1.13.0 torchvision pytorch-cuda=11.6 -c pytorch -c nvidia -y\nconda install -c fvcore -c iopath -c conda-forge fvcore iopath -y\nconda install -c bottler nvidiacub -y\nconda install pytorch3d -c pytorch3d -y\n</code></pre> <p>Validate the installation</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\npython -c \"import pytorch3d; print(pytorch3d.__version__)\"\n</code></pre>"}]}