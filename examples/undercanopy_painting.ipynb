{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyproj\n",
    "import pyvista as pv\n",
    "from matplotlib import pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "from geograypher.cameras.rig_cameras import create_rig_cameras_from_equirectangular\n",
    "from geograypher.cameras.segmentor import SegmentorPhotogrammetryCameraSet\n",
    "from geograypher.meshes import TexturedPhotogrammetryMesh\n",
    "from geograypher.predictors.derived_segmentors import LookUpSegmentor\n",
    "from geograypher.utils.indexing import find_argmax_nonzero_value\n",
    "from geograypher.utils.visualization import show_segmentation_labels\n",
    "from geograypher.utils.parsing import parse_metashape_mesh_metadata\n",
    "from geograypher.utils.geospatial import convert_CRS_3D_points\n",
    "\n",
    "from geograypher.constants import DATA_FOLDER, EARTH_CENTERED_EARTH_FIXED_CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip re-computing the aggregation and use a saved version\n",
    "USE_CACHED_AGGREGATION = False\n",
    "# Aggregate images at this scale resolution\n",
    "AGGREGATE_IMAGE_SCALE = 0.25\n",
    "\n",
    "IDS_TO_LABELS = {0: \"BO\", 1: \"CR\", 2: \"SH\", 3: \"HE\", 4: \"GR\", 5: \"LO\", 6: \"MA\", 7: \"SK\"}\n",
    "# Path to the camera data from metashape\n",
    "CAMERAS_FILENAME = Path(DATA_FOLDER, \"under-canopy\", \"under-canopy-cameras.xml\")\n",
    "# The path to the mesh data from metashape\n",
    "MESH_FILENAME = Path(DATA_FOLDER, \"under-canopy\", \"under-canopy-mesh.ply\")\n",
    "# The path to the mesh metadata from metashape\n",
    "MESH_METADATA_FILENAME = Path(DATA_FOLDER, \"under-canopy\", \"under-canopy-mesh-metadata.xml\")\n",
    "# Path to the images, resampled to frame camera representations\n",
    "IMAGE_FOLDER = Path(DATA_FOLDER, \"under-canopy\", \"under-canopy-images-reprojected\")\n",
    "# Path to the corresponding semantic segmentation predictions for the images\n",
    "PREDICTED_IMAGE_LABELS_FOLDER = Path(DATA_FOLDER, \"under-canopy\", \"under-canopy-images-reprojected-predictions\")\n",
    "# The location of the images when photogrammetry was run. Do not update this path unless you use completely new photogrammetry data.\n",
    "ORIGINAL_IMAGE_FOLDER = \"/ofo-share/repos/david/under-canopy-mapping/scratch/photogrammetry/st-0077-gopro-360-photos-timelapse\"\n",
    "\n",
    "# Outputs\n",
    "AGGREGATED_FACE_LABELS_FILE = Path(DATA_FOLDER, \"under-canopy\", \"aggregated_face_labels.npy\")\n",
    "VIS_MESH_PATH = Path(DATA_FOLDER, \"under-canopy\", \"labeled_mesh.ply\")\n",
    "VIS_CAMERAS_MESH_PATH = Path(DATA_FOLDER, \"under-canopy\", \"camera_rig_vis_mesh.ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the rig camera\n",
    "The imagery was originally an equirectangular representation of 360 degree data. To support both machine learning tasks and projections, it has been reprojected into six perspective views. These are analogous to how a traditional (non-spherical) camera views the world. The variables below describe these six orientations, which together fully capture the original 360 degree scene. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIG_CAMERA_DEF = {\n",
    "    \"f\": 1440 / 2,\n",
    "    \"cx\": 0.0,\n",
    "    \"cy\": 0.0,\n",
    "    \"image_width\": 1440,\n",
    "    \"image_height\": 1440,\n",
    "}\n",
    "RIG_ORIENTATIONS = [\n",
    "    {\"yaw_deg\": 0.0, \"pitch_deg\": 0.0, \"roll_deg\": 0.0},\n",
    "    {\"yaw_deg\": 90.0, \"pitch_deg\": 0.0, \"roll_deg\": 0.0},\n",
    "    {\"yaw_deg\": 180.0, \"pitch_deg\": 0.0, \"roll_deg\": 0.0},\n",
    "    {\"yaw_deg\": 270.0, \"pitch_deg\": 0.0, \"roll_deg\": 0.0},\n",
    "    {\"yaw_deg\": 0.0, \"pitch_deg\": -90.0, \"roll_deg\": 0.0},\n",
    "    {\"yaw_deg\": 0.0, \"pitch_deg\": 90.0, \"roll_deg\": 0.0},\n",
    "]\n",
    "RESAMPLED_FORMAT_STR = \"_yaw{yaw_deg:03.0f}_pitch{pitch_deg:03.0f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse metadata\n",
    "The mesh file format does not support metadata so it is saved in a sidecar file. Since the values of geospatial coordinates are large, quantization from the limits of float precision have a noticable impact. To counteract this, the mesh is saved with zero-centered coordinates, since float quantization is dependent on the scale of the value being represented. The metadata file provides a shift to apply to the data to produce the original values. Also, it provides a coordinate reference system (CRS) to interpret the values in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_CRS, mesh_shift = parse_metashape_mesh_metadata(MESH_METADATA_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the camera set\n",
    "Each original spherical camera is now represented by six perspective cameras all facing outward from the center location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create camera set\n",
    "camera_set = create_rig_cameras_from_equirectangular(\n",
    "    camera_file=CAMERAS_FILENAME,\n",
    "    original_images=ORIGINAL_IMAGE_FOLDER,\n",
    "    perspective_images=PREDICTED_IMAGE_LABELS_FOLDER,\n",
    "    rig_camera=RIG_CAMERA_DEF,\n",
    "    rig_orientations=RIG_ORIENTATIONS,\n",
    "    perspective_filename_format_str=RESAMPLED_FORMAT_STR\n",
    ")\n",
    "\n",
    "# And show\n",
    "camera_set.vis(force_xvfb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the ROI\n",
    "Create a region of interest from the camera locations so we don't texture areas of the mesh that are very far from cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the region of interest around the camera locations\n",
    "ROI = camera_set.get_camera_locations(as_CRS=mesh_CRS)\n",
    "# Convert the ROI to a GeoDataFrame\n",
    "xs, ys = zip(*ROI)\n",
    "ROI = gpd.GeoDataFrame(geometry=gpd.points_from_xy(xs, ys), crs=mesh_CRS)\n",
    "ROI.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the mesh\n",
    "Load the mesh, clipped to 25 meters from the camera locations. Once it's loaded, show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = TexturedPhotogrammetryMesh(\n",
    "    MESH_FILENAME,\n",
    "    input_CRS=mesh_CRS,\n",
    "    IDs_to_labels=IDS_TO_LABELS,\n",
    "    ROI=ROI,\n",
    "    ROI_buffer_meters=25,\n",
    "    shift=mesh_shift,\n",
    ")\n",
    "\n",
    "mesh.vis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the predictions\n",
    "Show a set of semantic segmentation predictions to ensure they look correct. The predictions are overlaid on the correponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_segmentation_labels(\n",
    "    label_folder=PREDICTED_IMAGE_LABELS_FOLDER,\n",
    "    image_folder=IMAGE_FOLDER,\n",
    "    IDs_to_labels=IDS_TO_LABELS,\n",
    "    image_suffix=\".png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return the segmented images\n",
    "This creates an object that behaves like a camera set but returns the segmented image instead of the original RGB one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets a raw image and returns the corresponding segmentation labels\n",
    "segmentor = LookUpSegmentor(\n",
    "    base_folder=PREDICTED_IMAGE_LABELS_FOLDER,\n",
    "    lookup_folder=PREDICTED_IMAGE_LABELS_FOLDER,\n",
    "    num_classes=len(IDS_TO_LABELS),\n",
    ")\n",
    "\n",
    "# This wraps a camera set so it returns the corresponding segmentation label\n",
    "segmentor_camera_set = SegmentorPhotogrammetryCameraSet(\n",
    "    camera_set, segmentor=segmentor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project labels to mesh\n",
    "Here the labels are finally projected to the mesh using techniques from computer graphics. Under the hood, this is performed with the OpenGL rendering stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can skip the expensive aggregation step by loading a cached version\n",
    "if USE_CACHED_AGGREGATION and AGGREGATED_FACE_LABELS_FILE.is_file():\n",
    "    aggregated_face_labels = np.load(AGGREGATED_FACE_LABELS_FILE)\n",
    "else:\n",
    "    # Otherwise, actually run agregation. This is slow, approximately 10-30 minutes depending on the system\n",
    "    aggregated_face_labels, _ = mesh.aggregate_projected_images(\n",
    "        segmentor_camera_set,\n",
    "        aggregate_img_scale=AGGREGATE_IMAGE_SCALE,\n",
    "        apply_distortion=False,\n",
    "    )\n",
    "    np.save(AGGREGATED_FACE_LABELS_FILE, aggregated_face_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "Compute the most commonly projected class per face and show it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_face_classes = find_argmax_nonzero_value(aggregated_face_labels, keepdims=True)\n",
    "mesh.vis(vis_scalars=predicted_face_classes, IDs_to_labels=IDS_TO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results\n",
    "Colormap the results and save them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mesh in the original CRS and shift\n",
    "pymesh = mesh.reproject_CRS(target_CRS=pyproj.CRS(mesh_CRS), inplace=False)\n",
    "pymesh.points -= np.array([mesh_shift[0], mesh_shift[1], 0])\n",
    "# Remove the previous coloring\n",
    "assert pymesh.n_cells == len(predicted_face_classes)\n",
    "del pymesh.point_data[\"RGB\"]\n",
    "\n",
    "# Colormap the values\n",
    "colors = plt.colormaps[\"tab10\"](predicted_face_classes.squeeze() / 10)\n",
    "# Rescale to 0-255 and convert to uint8\n",
    "colors = (colors * 255).astype(np.uint8)\n",
    "\n",
    "# Add the colors to the mesh and save\n",
    "pymesh.cell_data[\"RGB\"] = colors\n",
    "pymesh.save(VIS_MESH_PATH, texture=\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save out the camera locations\n",
    "Create a mesh representation of the camera locations for subsequent visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the camera mesh in the local coordinate system\n",
    "camera_mesh = camera_set.get_vis_mesh()\n",
    "# Get the transform from this frame to the earth-centered earth-fixed frame\n",
    "local_to_epsg_4978_transform = camera_set.get_local_to_epsg_4978_transform()\n",
    "# Apply the transform to get the mesh in ECEF\n",
    "camera_mesh.transform(local_to_epsg_4978_transform, inplace=True)\n",
    "\n",
    "# Project the points from ECEF to the CRS that the scene mesh was provided in\n",
    "reprojected_points = convert_CRS_3D_points(\n",
    "    camera_mesh.points,\n",
    "    input_CRS=EARTH_CENTERED_EARTH_FIXED_CRS,\n",
    "    output_CRS=mesh_CRS\n",
    ")\n",
    "camera_mesh.points = reprojected_points\n",
    "\n",
    "# Apply the same shift that was applied to the scene mesh to avoid quantization\n",
    "camera_mesh.points -= np.array([mesh_shift[0], mesh_shift[1], 0])\n",
    "# Save the camera mesh\n",
    "camera_mesh.save(VIS_CAMERAS_MESH_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geograypher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
